{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import spacy\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head of movie_overviews: \n",
      "                             title  \\\n",
      "id                                   \n",
      "862                      Toy Story   \n",
      "8844                       Jumanji   \n",
      "15602             Grumpier Old Men   \n",
      "31357            Waiting to Exhale   \n",
      "11862  Father of the Bride Part II   \n",
      "\n",
      "                                                overview  \\\n",
      "id                                                         \n",
      "862    Led by Woody, Andy's toys live happily in his ...   \n",
      "8844   When siblings Judy and Peter discover an encha...   \n",
      "15602  A family wedding reignites the ancient feud be...   \n",
      "31357  Cheated on, mistreated and stepped on, the wom...   \n",
      "11862  Just when George Banks has recovered from his ...   \n",
      "\n",
      "                                                 tagline  \n",
      "id                                                        \n",
      "862                                                  NaN  \n",
      "8844           Roll the dice and unleash the excitement!  \n",
      "15602  Still Yelling. Still Fighting. Still Ready for...  \n",
      "31357  Friends are the people who let you be yourself...  \n",
      "11862  Just When His World Is Back To Normal... He's ...  \n",
      "\n",
      "\n",
      "Head of ted_talk: \n",
      "                                                 url               event  \\\n",
      "0  https://www.ted.com/talks/9_11_healing_the_mot...       TEDWomen 2010   \n",
      "1  https://www.ted.com/talks/a_j_jacobs_year_of_l...             EG 2007   \n",
      "2  https://www.ted.com/talks/a_robot_that_flies_l...      TEDGlobal 2011   \n",
      "3  https://www.ted.com/talks/a_ted_speaker_s_wors...             TED2012   \n",
      "4  https://www.ted.com/talks/a_whistleblower_you_...  TEDxRotterdam 2010   \n",
      "\n",
      "   languages                                               name  \\\n",
      "0         32  Aicha el-Wafi + Phyllis Rodriguez: The mothers...   \n",
      "1         39            AJ Jacobs: My year of living biblically   \n",
      "2         45     Markus Fischer: A robot that flies like a bird   \n",
      "3         51  Improv Everywhere: A TED speaker's worst night...   \n",
      "4         31   Geert Chatrou: A whistleblower you haven't heard   \n",
      "\n",
      "   published_date                                          title  \\\n",
      "0      1304362260  The mothers who found forgiveness, friendship   \n",
      "1      1216256400                   My year of living biblically   \n",
      "2      1311344765                 A robot that flies like a bird   \n",
      "3      1331309010                A TED speaker's worst nightmare   \n",
      "4      1297437420              A whistleblower you haven't heard   \n",
      "\n",
      "                                          transcript  \n",
      "0  Phyllis Rodriguez: We are here today because o...  \n",
      "1  I thought I'd tell you a little about what I l...  \n",
      "2  It is a dream of mankind to fly like a bird. B...  \n",
      "3  Today I'm going to talk about unexpected disco...  \n",
      "4  (Whistling)(Whistling ends)(Applause)Thank you...  \n"
     ]
    }
   ],
   "source": [
    "# Read data\n",
    "#ted_talk = pd.read_csv('data/ted.csv')\n",
    "#print(f'Head of ted_talk: \\n{ted_talk.head()}')\n",
    "\n",
    "movie_overviews = pd.read_csv('data/movie_overviews.csv', index_col=0)\n",
    "print(f'Head of movie_overviews: \\n{movie_overviews.head()}')\n",
    "\n",
    "ted_main = pd.read_csv('data/ted_main.csv.zip', compression='zip', index_col='url',\n",
    "                       usecols=['title', 'name', 'languages', 'published_date', 'event', 'url'])\n",
    "ted_transcripts = pd.read_csv('data/ted_transcripts.csv.zip', compression='zip', index_col='url',\n",
    "                              usecols=['transcript', 'url'])\n",
    "\n",
    "ted_talk = ted_main.join(ted_transcripts, how='right').reset_index()\n",
    "print(f'\\n\\nHead of ted_talk: \\n{ted_talk.head()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                           transcript\n",
      "url                                                                                                  \n",
      "https://www.ted.com/talks/shah_rukh_khan_though...  Namaskar.I'm a movie star, I'm 51 years of age...\n",
      "https://www.ted.com/talks/stuart_russell_how_ai...  This is Lee Sedol. Lee Sedol is one of the wor...\n",
      "https://www.ted.com/talks/lucy_kalanithi_what_m...  A few days after my husband Paul was diagnosed...\n",
      "https://www.ted.com/talks/ted_halstead_a_climat...  I have a two-year-old daughter named Naya who ...\n",
      "https://www.ted.com/talks/wendy_troxel_why_scho...  It's six o'clock in the morning, pitch black o...\n",
      "...                                                                                               ...\n",
      "https://www.ted.com/talks/duarte_geraldino_what...  So, Ma was trying to explain something to me a...\n",
      "https://www.ted.com/talks/armando_azua_bustos_t...  This is a picture of a sunset on Mars taken by...\n",
      "https://www.ted.com/talks/radhika_nagpal_what_i...  In my early days as a graduate student, I went...\n",
      "https://www.ted.com/talks/theo_e_j_wilson_a_bla...  I took a cell phone and accidentally made myse...\n",
      "https://www.ted.com/talks/karoliina_korppoo_how...  We humans are becoming an urban species, so ci...\n",
      "\n",
      "[100 rows x 1 columns]\n",
      "                                                                 event  \\\n",
      "url                                                                      \n",
      "https://www.ted.com/talks/shah_rukh_khan_though...             TED2017   \n",
      "https://www.ted.com/talks/stuart_russell_how_ai...             TED2017   \n",
      "https://www.ted.com/talks/lucy_kalanithi_what_m...         TEDMED 2016   \n",
      "https://www.ted.com/talks/ted_halstead_a_climat...             TED2017   \n",
      "https://www.ted.com/talks/wendy_troxel_why_scho...  TEDxManhattanBeach   \n",
      "...                                                                ...   \n",
      "https://www.ted.com/talks/duarte_geraldino_what...             TED2017   \n",
      "https://www.ted.com/talks/armando_azua_bustos_t...             TED2017   \n",
      "https://www.ted.com/talks/radhika_nagpal_what_i...             TED2017   \n",
      "https://www.ted.com/talks/theo_e_j_wilson_a_bla...        TEDxMileHigh   \n",
      "https://www.ted.com/talks/karoliina_korppoo_how...             TED2017   \n",
      "\n",
      "                                                    languages  \\\n",
      "url                                                             \n",
      "https://www.ted.com/talks/shah_rukh_khan_though...         26   \n",
      "https://www.ted.com/talks/stuart_russell_how_ai...         10   \n",
      "https://www.ted.com/talks/lucy_kalanithi_what_m...         14   \n",
      "https://www.ted.com/talks/ted_halstead_a_climat...         13   \n",
      "https://www.ted.com/talks/wendy_troxel_why_scho...         20   \n",
      "...                                                       ...   \n",
      "https://www.ted.com/talks/duarte_geraldino_what...          4   \n",
      "https://www.ted.com/talks/armando_azua_bustos_t...          3   \n",
      "https://www.ted.com/talks/radhika_nagpal_what_i...          1   \n",
      "https://www.ted.com/talks/theo_e_j_wilson_a_bla...          1   \n",
      "https://www.ted.com/talks/karoliina_korppoo_how...          1   \n",
      "\n",
      "                                                                                                 name  \\\n",
      "url                                                                                                     \n",
      "https://www.ted.com/talks/shah_rukh_khan_though...  Shah Rukh Khan: Thoughts on humanity, fame and...   \n",
      "https://www.ted.com/talks/stuart_russell_how_ai...  Stuart Russell: 3 principles for creating safe...   \n",
      "https://www.ted.com/talks/lucy_kalanithi_what_m...  Lucy Kalanithi: What makes life worth living i...   \n",
      "https://www.ted.com/talks/ted_halstead_a_climat...  Ted Halstead: A climate solution where all sid...   \n",
      "https://www.ted.com/talks/wendy_troxel_why_scho...  Wendy Troxel: Why school should start later fo...   \n",
      "...                                                                                               ...   \n",
      "https://www.ted.com/talks/duarte_geraldino_what...  Duarte Geraldino: What we're missing in the de...   \n",
      "https://www.ted.com/talks/armando_azua_bustos_t...  Armando Azua-Bustos: The most Martian place on...   \n",
      "https://www.ted.com/talks/radhika_nagpal_what_i...  Radhika Nagpal: What intelligent machines can ...   \n",
      "https://www.ted.com/talks/theo_e_j_wilson_a_bla...  Theo E.J. Wilson: A black man goes undercover ...   \n",
      "https://www.ted.com/talks/karoliina_korppoo_how...  Karoliina Korppoo: How a video game might help...   \n",
      "\n",
      "                                                    published_date  \\\n",
      "url                                                                  \n",
      "https://www.ted.com/talks/shah_rukh_khan_though...      1494618280   \n",
      "https://www.ted.com/talks/stuart_russell_how_ai...      1494858586   \n",
      "https://www.ted.com/talks/lucy_kalanithi_what_m...      1494946863   \n",
      "https://www.ted.com/talks/ted_halstead_a_climat...      1495031900   \n",
      "https://www.ted.com/talks/wendy_troxel_why_scho...      1495119518   \n",
      "...                                                            ...   \n",
      "https://www.ted.com/talks/duarte_geraldino_what...      1505851216   \n",
      "https://www.ted.com/talks/armando_azua_bustos_t...      1505919737   \n",
      "https://www.ted.com/talks/radhika_nagpal_what_i...      1506006095   \n",
      "https://www.ted.com/talks/theo_e_j_wilson_a_bla...      1506024042   \n",
      "https://www.ted.com/talks/karoliina_korppoo_how...      1506092422   \n",
      "\n",
      "                                                                                                title  \n",
      "url                                                                                                    \n",
      "https://www.ted.com/talks/shah_rukh_khan_though...                Thoughts on humanity, fame and love  \n",
      "https://www.ted.com/talks/stuart_russell_how_ai...                 3 principles for creating safer AI  \n",
      "https://www.ted.com/talks/lucy_kalanithi_what_m...  What makes life worth living in the face of death  \n",
      "https://www.ted.com/talks/ted_halstead_a_climat...         A climate solution where all sides can win  \n",
      "https://www.ted.com/talks/wendy_troxel_why_scho...            Why school should start later for teens  \n",
      "...                                                                                               ...  \n",
      "https://www.ted.com/talks/duarte_geraldino_what...  What we're missing in the debate about immigra...  \n",
      "https://www.ted.com/talks/armando_azua_bustos_t...                    The most Martian place on Earth  \n",
      "https://www.ted.com/talks/radhika_nagpal_what_i...  What intelligent machines can learn from a sch...  \n",
      "https://www.ted.com/talks/theo_e_j_wilson_a_bla...       A black man goes undercover in the alt-right  \n",
      "https://www.ted.com/talks/karoliina_korppoo_how...  How a video game might help us build better ci...  \n",
      "\n",
      "[100 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "print(ted_transcripts.tail(100))\n",
    "print(ted_main.tail(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. TF-IDF and similarity scores\n",
    "Learn how to compute tf-idf weights and the cosine similarity score between two vectors. You will use these concepts to build a movie and a TED Talk recommender. Finally, you will also learn about word embeddings and using word vector representations, you will compute similarities between various Pink Floyd songs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Building tf-idf document vectors\n",
    "\n",
    "1. Building tf-idf document vectors\n",
    ">In the last chapter, we learned about n-gram modeling.\n",
    "\n",
    "2. n-gram modeling\n",
    ">In n-gram modeling, the weight of a dimension for the vector representation of a document is dependent on the number of times the word corresponding to the dimension occurs in the document. Let's say we have a document that has the word 'human' occurring 5 times. Then, the dimension of its vector representation corresponding to 'human' would have the value 5.\n",
    "\n",
    "3. Motivation\n",
    ">However, some words occur very commonly across all the documents in the corpus. As a result, the vector representations get more characterized by these dimensions. Consider a corpus of documents on the Universe. Let's say there is a particular document on Jupiter where the word 'jupiter' and 'universe' both occur about 20 times. However, 'jupiter' rarely figures in the other documents whereas 'universe' is just as common. We could argue that although both *jupiter* and *universe* occur 20 times, *jupiter* should be given a larger weight on account of its exclusivity. In other words, the word 'jupiter' characterizes the document more than 'universe'.\n",
    "\n",
    "4. Applications\n",
    ">Weighting words this way has a huge number of applications. They can be used to automatically detect stopwords for the corpus instead of relying on a generic list. They're used in search algorithms to determine the ranking of pages containing the search query and in recommender systems as we will soon find out. In a lot of cases, this kind of weighting also generates better performance during predictive modeling.\n",
    "\n",
    "5. Term frequency-inverse document frequency\n",
    ">The weighting mechanism we've described is known as term frequency-inverse document frequency or tf-idf for short. It is based on the idea that the weight of a term in a document should be proportional to its frequency and an inverse function of the number of documents in which it occurs.\n",
    "\n",
    "6. Mathematical formula\n",
    ">Mathematically, the weight of a term i in document j is computed as\n",
    "\n",
    "7. Mathematical formula\n",
    ">term frequency of the term i in document j\n",
    "\n",
    "8. Mathematical formula\n",
    ">multiplied by the log of the ratio of the number of documents in the corpus and the number of documents in which the term i occurs or dfi.\n",
    "\n",
    "9. Mathematical formula\n",
    ">Therefore, let's say the word 'library' occurs in a document 5 times. There are 20 documents in the corpus and 'library' occurs in 8 of them. Then, the tf-idf weight of 'library' in the vector representation of this document will be 5 times log of 20 by 8 which is approximately 2. In general, higher the tf-idf weight, more important is the word in characterizing the document. A high tf-idf weight for a word in a document may imply that the word is relatively exclusive to that particular document or that the word occurs extremely commonly in the document, or both.\n",
    "\n",
    "10. tf-idf using scikit-learn\n",
    ">Generating vectors that use tf-idf weighting is almost identical to what we've already done so far. Instead of using CountVectorizer, we use the TfidfVectorizer class of scikit-learn. The parameters and methods it has is almost identical to CountVectorizer. The only difference is that TfidfVectorizer assigns weights using the tf-idf formula from before and has extra parameters related to inverse document frequency which we will not cover in this course. Here, we can see how using TfidfVectorizer is almost identical to using CountVectorizer for a corpus. However, notice that the weights are non-integer and reflect values calculated by the tf-idf formula.\n",
    "\n",
    "11. Let's practice!\n",
    ">That's enough theory for now. Let's practice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus to analize: \n",
      "0    The lion is the king of the jungle\n",
      "1      Lions have lifespans of a decade\n",
      "2     The lion is an endangered species\n",
      "dtype: object\n",
      "\n",
      "Features: \n",
      "['an', 'decade', 'endangered', 'have', 'is', 'jungle', 'king', 'lifespans', 'lion', 'lions', 'of', 'species', 'the']\n",
      "\n",
      "Vectorized data: \n",
      "[[0.         0.         0.         0.         0.25434658 0.33443519\n",
      "  0.33443519 0.         0.25434658 0.         0.25434658 0.\n",
      "  0.76303975]\n",
      " [0.         0.46735098 0.         0.46735098 0.         0.\n",
      "  0.         0.46735098 0.         0.46735098 0.35543247 0.\n",
      "  0.        ]\n",
      " [0.45954803 0.         0.45954803 0.         0.34949812 0.\n",
      "  0.         0.         0.34949812 0.         0.         0.45954803\n",
      "  0.34949812]]\n"
     ]
    }
   ],
   "source": [
    "# Bag of words model using sklearn\n",
    "lcorpus = pd.Series([\n",
    "'The lion is the king of the jungle',\n",
    "'Lions have lifespans of a decade',\n",
    "'The lion is an endangered species'\n",
    "])\n",
    "print(f'Corpus to analize: \\n{lcorpus}')\n",
    "\n",
    "# Create TfidfVectorizer object\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Generate matrix of word vectors\n",
    "tfidf_matrix = vectorizer.fit_transform(lcorpus)\n",
    "features = vectorizer.get_feature_names()\n",
    "matrix = tfidf_matrix.toarray()\n",
    "\n",
    "print(f'\\nFeatures: \\n{features}')\n",
    "print(f'\\nVectorized data: \\n{matrix}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 tf-idf weight of commonly occurring words\n",
    "\n",
    "**Instruction**\n",
    "\n",
    "The word bottle occurs 5 times in a particular document D and also occurs in every document of the corpus. What is the tf-idf weight of bottle in D?\n",
    "\n",
    "**Possible Answers**\n",
    "\n",
    "1. <font color=blue>__0__</font> Correct!\n",
    "2. 1\n",
    "3. Not defined\n",
    "4. 5\n",
    "\n",
    "**Results**\n",
    "\n",
    "<font color=darkgreen>Correct! In fact, the tf-idf weight for bottle in every document will be 0. This is because the inverse document frequency is constant across documents in a corpus and since bottle occurs in every document, its value is log(1), which is 0.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 tf-idf vectors for TED talks\n",
    "\n",
    "In this exercise, you have been given a corpus **ted** which contains the transcripts of 500 TED Talks. Your task is to generate the tf-idf vectors for these talks.\n",
    "\n",
    "In a later lesson, we will use these vectors to generate recommendations of similar talks based on the transcript.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "1. Import TfidfVectorizer from sklearn.\n",
    "2. Create a TfidfVectorizer object. Name it vectorizer.\n",
    "3. Generate tfidf_matrix for ted using the fit_transform() method.\n",
    "\n",
    "**Results**\n",
    "\n",
    "<font color=darkgreen>Good job! You now know how to generate tf-idf vectors for a given corpus of text. You can use these vectors to perform predictive modeling just like we did with CountVectorizer. In the next few lessons, we will see another extremely useful application of the vectorized form of documents: generating recommendations.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2467, 58795)\n"
     ]
    }
   ],
   "source": [
    "# Read data\n",
    "ted = ted_talk.transcript.copy(deep = True)\n",
    "\n",
    "# Create TfidfVectorizer object\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Generate matrix of word vectors\n",
    "tfidf_matrix = vectorizer.fit_transform(ted)\n",
    "\n",
    "# Print the shape of tfidf_matrix\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Cosine similarity\n",
    "\n",
    "1. Cosine similarity\n",
    ">We now know how to compute vectors out of text documents. With this representation in mind, let us now explore techniques that will allow us to determine how similar two vectors and consequentially two documents, are to each other. More specifically, we will learn about the cosine similarity score which is one of the most popularly used similarity metrics in NLP.\n",
    "\n",
    "2. Mathematical formula\n",
    ">Very simply put, the cosine similarity score of two vectors is the cosine of the angle between the vectors. Mathematically, it is the ratio of the dot product of the vectors and the product of the magnitude of the two vectors. Let's walk through what this formula really means.\n",
    "\n",
    "1 Image courtesy techninpink.com\n",
    "3. The dot product\n",
    ">The dot product is computed by summing the product of values across corresponding dimensions of the vectors. Let's say we have two n-dimensional vectors V and W as shown. Then, the dot product here would be v1 times w1 plus v2 times w2 and so on until vn times wn. As an example, consider two vectors A and B. By applying the formula above, we see that the dot product comes to 37.\n",
    "\n",
    "4. Magnitude of a vector\n",
    ">The magnitude of a vector is essentially the length of the vector. Mathematically, it is defined as the square root of the sum of the squares of values across all the dimensions of a vector. Therefore, for an n-dimensional vector V, the magnitude,mod V, is computed as the square root of v1 square plus v2 square and so on until vn square. Consider the vector A from before. Using the above formula, we compute its magnitude to be root 66.\n",
    "\n",
    "5. The cosine score\n",
    ">We are now in a position to compute the cosine similarity score of A and B. It is the dot product, which is 37, divided by the product of the magnitudes of A and B, which are root 66 and root 38 respectively. The value comes out to be approximately 0.738, which is the value of the cosine of the angle theta between the two vectors.\n",
    "\n",
    "6. Cosine Score: points to remember\n",
    ">Since the cosine score is simply the cosine of the angle between two vectors, its value is bounded between -1 and 1. However, in NLP, document vectors almost always use non-negative weights. Therefore, cosine scores vary between 0 and 1 where 0 indicates no similarity and 1 indicates that the documents are identical. Finally, since the cosine score ignores the magnitude of the vectors, it is fairly robust to document length. This may be an advantage or a disadvantage depending on the use case.\n",
    "\n",
    "7. Implementation using scikit-learn\n",
    ">Scikit-learn offers a cosine_similarity function that outputs a similarity matrix containing the pairwise cosine scores for a set of vectors. You can import cosine_similarity from sklearn dot metrics dot pairwise. However, remember that cosine_similarity takes in 2-D arrays as arguments. Passing in 1-D arrays will throw an error. Let us compute the cosine similarity scores of vectors A and B from before. We see that we get the same answer of 0.738 from before.\n",
    "\n",
    "8. Let's practice!\n",
    ">That's enough theory for now. Let's practice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.73881883]]\n"
     ]
    }
   ],
   "source": [
    "# Define two 3-dimensional vectors A and B\n",
    "A = (4,7,1)\n",
    "B = (5,2,3)\n",
    "\n",
    "# Compute the cosine score of A and B\n",
    "score = cosine_similarity([A], [B])\n",
    "\n",
    "# Print the cosine score\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Range of cosine scores\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "Which of the following is a possible cosine score for a pair of document vectors?\n",
    "\n",
    "**Possible Answers**\n",
    "\n",
    "1. <font color=blue>__0.86__</font> Correct!\n",
    "2. -0.52\n",
    "3. 2.36\n",
    "4. -1.32\n",
    "\n",
    "**Results**\n",
    "\n",
    "<font color=darkgreen>Great job! Since document vectors use only non-negative weights, the cosine score lies between 0 and 1.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 Computing dot product\n",
    "\n",
    "In this exercise, we will learn to compute the dot product between two vectors, A = (1, 3) and B = (-2, 2), using the numpy library. More specifically, we will use the np.dot() function to compute the dot product of two numpy arrays.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "1. Initialize A (1,3) and B (-2,2) as numpy arrays using np.array().\n",
    "2. Compute the dot product using np.dot() and passing A and B as arguments.\n",
    "\n",
    "**Results**\n",
    "\n",
    "<font color=darkgreen>Good job! The dot product of the two vectors is 1 * -2 + 3 * 2 = 4, which is indeed the output produced. We will not be using np.dot() too much in this course but it can prove to be a helpful function while computing dot products between two standalone vectors.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# Initialize numpy vectors\n",
    "A = np.array([1, 3])\n",
    "B = np.array([-2, 2])\n",
    "\n",
    "# Compute dot product\n",
    "dot_prod = np.dot(A, B)\n",
    "\n",
    "# Print dot product\n",
    "print(dot_prod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7 Cosine similarity matrix of a corpus\n",
    "\n",
    "In this exercise, you have been given a __corpus__, which is a list containing five sentences. The __corpus__ is printed in the console. You have to compute the cosine similarity matrix which contains the pairwise cosine similarity score for every pair of sentences (vectorized using tf-idf).\n",
    "\n",
    "Remember, the value corresponding to the ith row and jth column of a similarity matrix denotes the similarity score for the ith and jth vector.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "1. Initialize an instance of TfidfVectorizer. Name it tfidf_vectorizer.\n",
    "2. Using fit_transform(), generate the tf-idf vectors for corpus. Name it tfidf_matrix.\n",
    "3. Use cosine_similarity() and pass tfidf_matrix to compute the cosine similarity matrix cosine_sim.\n",
    "\n",
    "**Results**\n",
    "\n",
    "<font color=darkgreen>Great work! As you will see in a subsequent lesson, computing the cosine similarity matrix lies at the heart of many practical systems such as recommenders. From our similarity matrix, we see that the first and the second sentence are the most similar. Also the fifth sentence has, on average, the lowest pairwise cosine scores. This is intuitive as it contains entities that are not present in the other sentences.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.36413198 0.18314713 0.18435251 0.16336438]\n",
      " [0.36413198 1.         0.15054075 0.21704584 0.11203887]\n",
      " [0.18314713 0.15054075 1.         0.21318602 0.07763512]\n",
      " [0.18435251 0.21704584 0.21318602 1.         0.12960089]\n",
      " [0.16336438 0.11203887 0.07763512 0.12960089 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Read data\n",
    "corpus = ['The sun is the largest celestial body in the solar system', \n",
    "          'The solar system consists of the sun and eight revolving planets', \n",
    "          'Ra was the Egyptian Sun God', \n",
    "          'The Pyramids were the pinnacle of Egyptian architecture', \n",
    "          'The quick brown fox jumps over the lazy dog']\n",
    "\n",
    "# Initialize an instance of tf-idf Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Generate the tf-idf vectors for the corpus\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Compute and print the cosine similarity matrix\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "print(cosine_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.8 Building a plot line based recommender\n",
    "\n",
    "1. Building a plot line based recommender\n",
    ">In this lesson, we will use tf-idf vectors and cosine scores to build a recommender system that suggests movies based on overviews.\n",
    "\n",
    "2. Movie recommender\n",
    ">We've a dataset containing movie overviews. Here, we can see two movies, Shanghai Triad and Cry, the Beloved Country and their overviews.\n",
    "\n",
    "3. Movie recommender\n",
    ">Our task is to build a system that takes in a movie title and outputs a list of movies that has similar plot lines. For instance, if we passed in 'The Godfather', we could expect output like this. Notice how a lot of the movies listed here have to do with crime and gangsters, just like The Godfather.\n",
    "\n",
    "4. Steps\n",
    ">Following are the steps involved. The first step, as always, is to preprocess movie overviews. The next step is to generate the tf-idf vectors for our overviews. Finally, we generate a cosine similarity matrix which contains the pairwise similarity scores of every movie with every other movie. Once the cosine similarity matrix is computed, we can proceed to build the recommender function.\n",
    "\n",
    "5. The recommender function\n",
    ">We will build a recommender function as part of this course. Let's take a look at how it works. The recommender function takes a movie title, the cosine similarity matrix and an indices series as arguments. The indices series is a reverse mapping of movie titles with their indices in the original dataframe. The function extracts the pairwise cosine similarity scores of the movie passed in with every other movie. Next, it sorts these scores in descending order. Finally, it outputs the titles of movies corresponding to the highest similarity scores. Note that the function ignores the highest similarity score of 1. This is because the movie most similar to a given movie is the movie itself!\n",
    "\n",
    "6. Generating tf-idf vectors\n",
    ">Let's say we already have the preprocessed movie overviews as 'movie_plots'. We already know how to generate the tf-idf vectors.\n",
    "\n",
    "7. Generating cosine similarity matrix\n",
    ">Generating the cosine similarity matrix is also extremely simple. We simply pass in tfidf_matrix as both the first and second argument of cosine_similarity. This generates a matrix that contains the pairwise similarity score of every movie with every other movie. The value corresponding to the ith row and the jth column is the cosine similarity score of movie i with movie j. Notice that the diagonal elements of this matrix is 1. This is because, as stated earlier, the cosine similarity score of movie k with itself is 1.\n",
    "\n",
    "8. The linear_kernel function\n",
    ">The magnitude of a tf-idf vector is always 1. Recall from the previous lesson that the cosine score is computed as the ratio of the dot product and the product of the magnitude of the vectors. Since the magnitude is 1, the cosine score of two tf-idf vectors is equal to their dot product! This fact can help us greatly improve the speed of computation of our cosine similarity matrix as we do not need to compute the magnitudes while working with tf-idf vectors. Therefore, while working with tf-idf vectors, we can use the linear_kernel function which computes the pairwise dot product of every vector with every other vector.\n",
    "\n",
    "9. Generating cosine similarity matrix\n",
    ">Let us replace the cosine_similarity function with linear_kernel. As you can see, the output remains the same but it takes significantly lesser time to compute.\n",
    "\n",
    "10. The get_recommendations function\n",
    ">The recommender function and the indices series described earlier will be built in the exercises. You can use this function to generate recommendations using the cosine similarity matrix.\n",
    "\n",
    "11. Let's practice!\n",
    ">In the exercises, you will build recommendation systems of your own and see them in action. Let's practice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.36413198 0.18314713 0.18435251 0.16336438]\n",
      " [0.36413198 1.         0.15054075 0.21704584 0.11203887]\n",
      " [0.18314713 0.15054075 1.         0.21318602 0.07763512]\n",
      " [0.18435251 0.21704584 0.21318602 1.         0.12960089]\n",
      " [0.16336438 0.11203887 0.07763512 0.12960089 1.        ]]\n",
      "\n",
      "With TfidfVectorizer, the program took 0.00498533 seconds to complete.\n",
      "\n",
      "\n",
      "[[1.         0.36413198 0.18314713 0.18435251 0.16336438]\n",
      " [0.36413198 1.         0.15054075 0.21704584 0.11203887]\n",
      " [0.18314713 0.15054075 1.         0.21318602 0.07763512]\n",
      " [0.18435251 0.21704584 0.21318602 1.         0.12960089]\n",
      " [0.16336438 0.11203887 0.07763512 0.12960089 1.        ]]\n",
      "\n",
      "With cosine_similarity, the program took 0.00399542 seconds to complete.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute and print the cosine similarity matrix\n",
    "start_time = time.time()\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "print(cosine_sim)\n",
    "print(\"\\nWith TfidfVectorizer, the program took %.8f seconds to complete.\\n\\n\" % (time.time() - start_time))\n",
    "\n",
    "# Compute and print the cosine similarity matrix\n",
    "start_time = time.time()\n",
    "linear_ker = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "print(linear_ker)\n",
    "print(\"\\nWith cosine_similarity, the program took %.8f seconds to complete.\\n\\n\" % (time.time() - start_time))\n",
    "\n",
    "assert (cosine_sim == linear_ker).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.9 Comparing linear_kernel and cosine_similarity\n",
    "\n",
    "In this exercise, you have been given __tfidf_matrix__ which contains the tf-idf vectors of a thousand documents. Your task is to generate the cosine similarity matrix for these vectors first using __cosine_similarity__ and then, using __linear_kernel__.\n",
    "\n",
    "We will then compare the computation times for both functions.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "1. Compute the cosine similarity matrix for tfidf_matrix using cosine_similarity.\n",
    "2. Compute the cosine similarity matrix for tfidf_matrix using linear_kernel.\n",
    "\n",
    "**Results**\n",
    "\n",
    "<font color=darkgreen>Good job! Notice how both linear_kernel and cosine_similarity produced the same result. However, linear_kernel took a smaller amount of time to execute. When you're working with a very large amount of data and your vectors are in the tf-idf representation, it is good practice to default to linear_kernel to improve performance. (NOTE: In case, you see linear_kernel taking more time, it's because the dataset we're dealing with is extremely small and Python's time module is incapable of capture such minute time differences accurately)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id\n",
      "862      Led by Woody, Andy's toys live happily in his ...\n",
      "8844     When siblings Judy and Peter discover an encha...\n",
      "15602    A family wedding reignites the ancient feud be...\n",
      "31357    Cheated on, mistreated and stepped on, the wom...\n",
      "11862    Just when George Banks has recovered from his ...\n",
      "Name: overview, dtype: object\n",
      "\n",
      "Shape of tfidf_matrix: (9087, 29727)\n",
      "\n",
      "\n",
      "Time taken in method cosine_similarity: 1.4352073669433594 seconds\n",
      "Time taken in method linear_kernel: 2.30485200881958 seconds\n"
     ]
    }
   ],
   "source": [
    "# Read data\n",
    "corpus = movie_overviews[movie_overviews.overview.notnull()].overview\n",
    "print(corpus.head())\n",
    "\n",
    "# Create TfidfVectorizer object\n",
    "vectorizer = TfidfVectorizer(lowercase=True, stop_words='english')\n",
    "\n",
    "# Generate matrix of word vectors\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Print the shape of tfidf_matrix\n",
    "print(f'\\nShape of tfidf_matrix: {tfidf_matrix.shape}\\n\\n')\n",
    "\n",
    "methods = {cosine_similarity: 'cosine_similarity', linear_kernel: 'linear_kernel'}\n",
    "for method in methods:\n",
    "    # Record start time\n",
    "    start = time.time()\n",
    "    \n",
    "    # Compute cosine similarity matrix\n",
    "    cosine_sim = method(tfidf_matrix, tfidf_matrix)\n",
    "    \n",
    "    # Print cosine similarity matrix\n",
    "    #print(cosine_sim)\n",
    "    \n",
    "    # Print time taken\n",
    "    print(\"Time taken in method %s: %s seconds\" %  (methods[method], (time.time() - start)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.10 Plot recommendation engine\n",
    "\n",
    "In this exercise, we will build a recommendation engine that suggests movies based on similarity of plot lines. You have been given a __get_recommendations()__ function that takes in the title of a movie, a similarity matrix and an __indices__ series as its arguments and outputs a list of most similar movies. __indices__ has already been provided to you.\n",
    "\n",
    "You have also been given a __movie_plots__ Series that contains the plot lines of several movies. Your task is to generate a cosine similarity matrix for the tf-idf vectors of these plots.\n",
    "\n",
    "Consequently, we will check the potency of our engine by generating recommendations for one of my favorite movies, The Dark Knight Rises.\n",
    "\n",
    "Instructions\n",
    "\n",
    "1. Initialize a TfidfVectorizer with English stop_words. Name it tfidf.\n",
    "2. Construct tfidf_matrix by fitting and transforming the movie plot data using fit_transform().\n",
    "3. Generate the cosine similarity matrix cosine_sim using tfidf_matrix. Don't use cosine_similarity()!\n",
    "4. Use get_recommendations() to generate recommendations for 'The Dark Knight Rises'.\n",
    "\n",
    "**Results**\n",
    "\n",
    "<font color=darkgreen>Congratulations! You've just built your very first recommendation system. Notice how the recommender correctly identifies 'The Dark Knight Rises' as a Batman movie and recommends other Batman movies as a result. This sytem is, of course, very primitive and there are a host of ways in which it could be improved. One method would be to look at the cast, crew and genre in addition to the plot to generate recommendations. We will not be covering this in this course but you have all the tools necessary to accomplish this. Do give it a try!</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to retrieve movie recomendation\n",
    "def get_recommendations(title, cosine_sim, indices, metadata):\n",
    "    \"\"\"Retrieve movies recomendation.\"\"\"\n",
    "    # Get the index of the movie that matches the title\n",
    "    idx = indices[title]\n",
    "    #print('\\n\\nidx:', idx)\n",
    "    \n",
    "    # Get the pairwsie similarity scores\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    #print('\\nsim_scores:', cosine_sim)\n",
    "    \n",
    "    # Sort the movies based on the similarity scores\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Get the scores for 10 most similar movies\n",
    "    sim_scores = sim_scores[1:11]\n",
    "    \n",
    "    # Get the movie indices\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "    \n",
    "    # Return the top 10 most similar movies\n",
    "    return metadata['title'].iloc[movie_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata: \n",
      "      id                        title  \\\n",
      "0    862                    Toy Story   \n",
      "1   8844                      Jumanji   \n",
      "2  15602             Grumpier Old Men   \n",
      "3  31357            Waiting to Exhale   \n",
      "4  11862  Father of the Bride Part II   \n",
      "\n",
      "                                            overview  \\\n",
      "0  Led by Woody, Andy's toys live happily in his ...   \n",
      "1  When siblings Judy and Peter discover an encha...   \n",
      "2  A family wedding reignites the ancient feud be...   \n",
      "3  Cheated on, mistreated and stepped on, the wom...   \n",
      "4  Just when George Banks has recovered from his ...   \n",
      "\n",
      "                                             tagline  \n",
      "0                                                NaN  \n",
      "1          Roll the dice and unleash the excitement!  \n",
      "2  Still Yelling. Still Fighting. Still Ready for...  \n",
      "3  Friends are the people who let you be yourself...  \n",
      "4  Just When His World Is Back To Normal... He's ...  \n",
      "\n",
      "\n",
      "Movie_plots: \n",
      "0    Led by Woody, Andy's toys live happily in his ...\n",
      "1    When siblings Judy and Peter discover an encha...\n",
      "2    A family wedding reignites the ancient feud be...\n",
      "3    Cheated on, mistreated and stepped on, the wom...\n",
      "4    Just when George Banks has recovered from his ...\n",
      "Name: overview, dtype: object\n",
      "\n",
      "\n",
      "Indices: \n",
      "title\n",
      "Toy Story                      0\n",
      "Jumanji                        1\n",
      "Grumpier Old Men               2\n",
      "Waiting to Exhale              3\n",
      "Father of the Bride Part II    4\n",
      "Name: index, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Read data\n",
    "metadata = movie_overviews[movie_overviews.overview.notnull()].reset_index()\n",
    "movie_plots = metadata.overview\n",
    "indices = metadata.reset_index().set_index('title')['index']\n",
    "print(f'Metadata: \\n{metadata.head()}')\n",
    "print(f'\\n\\nMovie_plots: \\n{movie_plots.head()}')\n",
    "print(f'\\n\\nIndices: \\n{indices.head()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Base on movie: \"Toy Story\", our recomendations are:\n",
      "2499               Toy Story 2\n",
      "7537               Toy Story 3\n",
      "6194    The 40 Year Old Virgin\n",
      "889      Rebel Without a Cause\n",
      "2544           Man on the Moon\n",
      "6629              Factory Girl\n",
      "1597                 Condorman\n",
      "6556    For Your Consideration\n",
      "4988          Rivers and Tides\n",
      "436                     Malice\n",
      "Name: title, dtype: object\n",
      "\n",
      "\n",
      "Base on movie: \"The Dark Knight Rises\", our recomendations are:\n",
      "132                              Batman Forever\n",
      "6902                            The Dark Knight\n",
      "523                                      Batman\n",
      "1113                             Batman Returns\n",
      "7567                 Batman: Under the Red Hood\n",
      "7901                           Batman: Year One\n",
      "8164    Batman: The Dark Knight Returns, Part 1\n",
      "7245                  The File on Thelma Jordon\n",
      "6145                              Batman Begins\n",
      "4489                                      Q & A\n",
      "Name: title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Initialize the TfidfVectorizer \n",
    "tfidf = TfidfVectorizer(lowercase=True, stop_words='english', ngram_range=(1, 2))\n",
    "\n",
    "# Construct the TF-IDF matrix\n",
    "tfidf_matrix = tfidf.fit_transform(movie_plots)\n",
    "#print(list(tfidf_matrix.toarray()))\n",
    "\n",
    "# Generate the cosine similarity matrix\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Generate recommendations\n",
    "movies = ['Toy Story', 'The Dark Knight Rises']\n",
    "for movie in movies:\n",
    "    print(f'\\n\\nBase on movie: \"{movie}\", our recomendations are:')\n",
    "    print(get_recommendations(movie, cosine_sim, indices, metadata))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.11 The recommender function\n",
    "\n",
    "In this exercise, we will build a recommender function __get_recommendations()__, as discussed in the lesson and the previous exercise. As we know, it takes in a title, a cosine similarity matrix, and a movie title and index mapping as arguments and outputs a list of 10 titles most similar to the original title (excluding the title itself).\n",
    "\n",
    "You have been given a dataset __metadata__ that consists of the movie titles and overviews. The head of this dataset has been printed to console.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "1. Get index of the movie that matches the title by using the title key of indices.\n",
    "2. Extract the ten most similar movies from sim_scores and store it back in sim_scores.\n",
    "\n",
    "\n",
    "**Results**\n",
    "\n",
    "<font color=darkgreen>Good job! With this recommender function in our toolkit, we are now in a very good place to build the rest of the components of our recommendation engine.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata: \n",
      "                         title  \\\n",
      "0                      Jumanji   \n",
      "1             Grumpier Old Men   \n",
      "2            Waiting to Exhale   \n",
      "3  Father of the Bride Part II   \n",
      "4                         Heat   \n",
      "\n",
      "                                             tagline  \n",
      "0          Roll the dice and unleash the excitement!  \n",
      "1  Still Yelling. Still Fighting. Still Ready for...  \n",
      "2  Friends are the people who let you be yourself...  \n",
      "3  Just When His World Is Back To Normal... He's ...  \n",
      "4                           A Los Angeles Crime Saga  \n",
      "\n",
      "\n",
      "Indices: \n",
      "title\n",
      "Jumanji                        0\n",
      "Grumpier Old Men               1\n",
      "Waiting to Exhale              2\n",
      "Father of the Bride Part II    3\n",
      "Heat                           4\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Read data\n",
    "metadata = movie_overviews[movie_overviews.tagline.notnull()].reset_index(drop=True)[['title', 'tagline']]\n",
    "\n",
    "# Generate mapping between titles and index\n",
    "indices = pd.Series(metadata.index, index=metadata['title']).drop_duplicates()\n",
    "\n",
    "print(f'Metadata: \\n{metadata.head()}')\n",
    "print(f'\\n\\nIndices: \\n{indices.head()}')\n",
    "\n",
    "\n",
    "# Function to retrieve movie recomendation\n",
    "def get_new_recommendations(title, cosine_sim, indices, metadata, num_recomendation=10):\n",
    "    \"\"\"Retrieve movies recomendation.\"\"\"    # Get index of movie that matches title\n",
    "    idx = indices[title]\n",
    "\n",
    "    # Sort the movies based on the similarity scores\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Get the scores for 10 most similar movies\n",
    "    sim_scores = sim_scores[1:num_recomendation+1]\n",
    "    \n",
    "    # Get the movie indices\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "    \n",
    "    # Return the top 10 most similar movies\n",
    "    return metadata['title'].iloc[movie_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.12 TED talk recommender\n",
    "\n",
    "In this exercise, we will build a recommendation system that suggests TED Talks based on their transcripts. You have been given a __get_recommendations()__ function that takes in the title of a talk, a similarity matrix and an __indices__ series as its arguments, and outputs a list of most similar talks. __indices__ has already been provided to you.\n",
    "\n",
    "You have also been given a __transcripts__ series that contains the transcripts of around 500 TED talks. Your task is to generate a cosine similarity matrix for the tf-idf vectors of the talk transcripts.\n",
    "\n",
    "Consequently, we will generate recommendations for a talk titled '5 ways to kill your dreams' by Brazilian entrepreneur Bel Pesce.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "1. Initialize a TfidfVectorizer with English stopwords. Name it tfidf.\n",
    "2. Construct tfidf_matrix by fitting and transforming transcripts.\n",
    "3. Generate the cosine similarity matrix cosine_sim using tfidf_matrix.\n",
    "4. Use get_recommendations() to generate recommendations for '5 ways to kill your dreams'.\n",
    "\n",
    "**Results**\n",
    "\n",
    "<font color=darkgreen>Excellent work! You have successfully built a TED talk recommender. This recommender works surprisingly well despite being trained only on a small subset of TED talks. In fact, three of the talks recommended by our system is also recommended by the official TED website as talks to watch next after '5 ways to kill your dreams'!</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata: \n",
      "                                           title  \\\n",
      "0  The mothers who found forgiveness, friendship   \n",
      "1                   My year of living biblically   \n",
      "2                 A robot that flies like a bird   \n",
      "3                A TED speaker's worst nightmare   \n",
      "4              A whistleblower you haven't heard   \n",
      "\n",
      "                                          transcript  \\\n",
      "0  Phyllis Rodriguez: We are here today because o...   \n",
      "1  I thought I'd tell you a little about what I l...   \n",
      "2  It is a dream of mankind to fly like a bird. B...   \n",
      "3  Today I'm going to talk about unexpected disco...   \n",
      "4  (Whistling)(Whistling ends)(Applause)Thank you...   \n",
      "\n",
      "                                                 url  \n",
      "0  https://www.ted.com/talks/9_11_healing_the_mot...  \n",
      "1  https://www.ted.com/talks/a_j_jacobs_year_of_l...  \n",
      "2  https://www.ted.com/talks/a_robot_that_flies_l...  \n",
      "3  https://www.ted.com/talks/a_ted_speaker_s_wors...  \n",
      "4  https://www.ted.com/talks/a_whistleblower_you_...  \n",
      "\n",
      "\n",
      "Indices: \n",
      "title\n",
      "The mothers who found forgiveness, friendship    0\n",
      "My year of living biblically                     1\n",
      "A robot that flies like a bird                   2\n",
      "A TED speaker's worst nightmare                  3\n",
      "A whistleblower you haven't heard                4\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Read data\n",
    "metadata = ted_talk[ted_talk.transcript.notnull()].reset_index(drop=True)[['title', 'transcript', 'url']]\n",
    "\n",
    "# Generate mapping between titles and index\n",
    "indices = pd.Series(metadata.index, index=metadata['title']).drop_duplicates()\n",
    "\n",
    "print(f'Metadata: \\n{metadata.head()}')\n",
    "print(f'\\n\\nIndices: \\n{indices.head()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Base on ted talk: \"5 ways to kill your dreams\", our recomendations are:\n",
      "519                  The dream we haven't dared to dream\n",
      "2159                   Bring on the learning revolution!\n",
      "1921                     Success is a continuous journey\n",
      "1012                 Let's crowdsource the world's goals\n",
      "2359                                Why we do what we do\n",
      "2070                           How to find work you love\n",
      "2156                    How great leaders inspire action\n",
      "1568    How we can make the world a better place by 2030\n",
      "1905         How to run a company with (almost) no rules\n",
      "1927                                   A life of purpose\n",
      "Name: title, dtype: object\n",
      "\n",
      "\n",
      "Base on ted talk: \"The mothers who found forgiveness, friendship\", our recomendations are:\n",
      "2129                    Why we have too few women leaders\n",
      "2055                           A one-woman global village\n",
      "631         What we don't know about Europe's Muslim kids\n",
      "2023    How I stopped the Taliban from shutting down m...\n",
      "2235     My son was a Columbine shooter. This is my story\n",
      "764     How we turned the tide on domestic violence (H...\n",
      "1551           Let's put birth control back on the agenda\n",
      "1285                   Radical women, embracing tradition\n",
      "2039               A political party for women's equality\n",
      "888                         New data on the rise of women\n",
      "Name: title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Initialize the TfidfVectorizer \n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# Construct the TF-IDF matrix\n",
    "tfidf_matrix = tfidf.fit_transform(metadata.transcript)\n",
    "\n",
    "# Generate the cosine similarity matrix\n",
    "cosine_sim = linear_kernel(tfidf_matrix)\n",
    " \n",
    "# Generate recommendations \n",
    "talks = ['5 ways to kill your dreams', 'The mothers who found forgiveness, friendship']\n",
    "for talk in talks:\n",
    "    print(f'\\n\\nBase on ted talk: \"{talk}\", our recomendations are:')\n",
    "    print(get_new_recommendations(talk, cosine_sim, indices, metadata))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.13 Beyond n-grams: word embeddings\n",
    "\n",
    "1. Beyond n-grams: word embeddings\n",
    ">We have covered a lot of ground in the last 4 chapters. However, before we bid adieu, we will cover one advanced topic that has a large number of applications in NLP.\n",
    "\n",
    "2. The problem with BoW and tf-idf\n",
    ">Consider the three sentences, I am happy, I am joyous and I am sad. Now if we were to compute the similarities, I am happy and I am joyous would have the same score as I am happy and I am sad, regardless of how we vectorize it. This is because 'happy', 'joyous' and 'sad' are considered to be completely different words. However, we know that happy and joyous are more similar to each other than sad. This is something that the vectorization techniques we've covered so far simply cannot capture.\n",
    "\n",
    "3. Word embeddings\n",
    ">Word embedding is the process of mapping words into an n-dimensional vector space. These vectors are usually produced using deep learning models and huge amounts of data. The techniques used are beyond the scope of this course. However, once generated, these vectors can be used to discern how similar two words are to each other. Consequently, they can also be used to detect synonyms and antonyms. Word embeddings are also capable of capturing complex relationships. For instance, it can be used to detect that the words king and queen relate to each other the same way as man and woman. Or that France and Paris are related in the same way as Russia and Moscow. One last thing to note is that word embeddings are not trained on user data; they are dependent on the pre-trained spacy model you're using and are independent of the size of your dataset.\n",
    "\n",
    "4. Word embeddings using spaCy\n",
    ">Generating word embeddings is easy using spaCy's pre-trained models. As usual, we load the spacy model and create the doc object for our string. Note that it is advisable to load larger spacy models while working with word vectors. This is because the en_core_web_sm model does not technically ship with word vectors but context specific tensors, which tend to give relatively poorer results. We generate word vectors for each word by looping through the tokens and accessing the vector attribute. The truncated output is as shown.\n",
    "\n",
    "5. Word similarities\n",
    ">We can compute how similar two words are to each other by using the similarity method of a spacy token. Let's say we want to compute how similar happy, joyous and sad are to each other. We define a doc containing the three words. We then use a nested loop to calculate the similarity scores between each pair of words. As expected, happy and joyous are more similar to each other than they are to sad.\n",
    "\n",
    "6. Document similarities\n",
    ">Spacy also allows us to directly compute the similarity between two documents by using the average of the word vectors of all the words in a particular document. Let's consider the three sentences from before. We create doc objects for the sentences. Like spacy tokens, docs also have a similarity method. Therefore, we can compute the similarity between two docs as follows. As expected, I am happy is more similar to I am joyous than it is to I am sad. Note that the similarity scores are high in both cases because all sentences share 2 out of their three words, I and am.\n",
    "\n",
    "7. Let's practice!\n",
    ">With this, we come to an end of this lesson. Let's now practice our new found skills in the last set of exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 1.8733e-01,  4.0595e-01, -5.1174e-01, -5.5482e-01,  3.9716e-02,\n",
      "        1.2887e-01,  4.5137e-01, -5.9149e-01,  1.5591e-01,  1.5137e+00,\n",
      "       -8.7020e-01,  5.0672e-02,  1.5211e-01, -1.9183e-01,  1.1181e-01,\n",
      "        1.2131e-01, -2.7212e-01,  1.6203e+00, -2.4884e-01,  1.4060e-01,\n",
      "        3.3099e-01, -1.8061e-02,  1.5244e-01, -2.6943e-01, -2.7833e-01,\n",
      "       -5.2123e-02, -4.8149e-01, -5.1839e-01,  8.6262e-02,  3.0818e-02,\n",
      "       -2.1253e-01, -1.1378e-01, -2.2384e-01,  1.8262e-01, -3.4541e-01,\n",
      "        8.2611e-02,  1.0024e-01, -7.9550e-02, -8.1721e-01,  6.5621e-03,\n",
      "        8.0134e-02, -3.9976e-01, -6.3131e-02,  3.2260e-01, -3.1625e-02,\n",
      "        4.3056e-01, -2.7270e-01, -7.6020e-02,  1.0293e-01, -8.8653e-02,\n",
      "       -2.9087e-01, -4.7214e-02,  4.6036e-02, -1.7788e-02,  6.4990e-02,\n",
      "        8.8451e-02, -3.1574e-01, -5.8522e-01,  2.2295e-01, -5.2785e-02,\n",
      "       -5.5981e-01, -3.9580e-01, -7.9849e-02, -1.0933e-02, -4.1722e-02,\n",
      "       -5.5576e-01,  8.8707e-02,  1.3710e-01, -2.9873e-03, -2.6256e-02,\n",
      "        7.7330e-02,  3.9199e-01,  3.4507e-01, -8.0130e-02,  3.3451e-01,\n",
      "        2.7063e-01, -2.4544e-02,  7.2576e-02, -1.8120e-01,  2.3693e-01,\n",
      "        3.9977e-01,  4.5012e-01,  2.7179e-02,  2.7400e-01,  1.4791e-01,\n",
      "       -5.8324e-03,  9.5910e-01, -1.0129e+00,  2.0699e-01,  1.8237e-01,\n",
      "       -2.5234e-01, -2.6261e-01, -3.4799e-01, -2.4051e-02,  4.4470e-01,\n",
      "        5.9226e-02,  4.5561e-01,  1.9700e-01, -4.8327e-01,  8.9523e-02,\n",
      "       -2.2373e-01, -1.5654e-01,  2.1578e-01,  1.1673e-01,  8.2006e-02,\n",
      "       -8.0735e-01,  2.3903e-01, -5.1304e-01, -3.3888e-01, -3.1499e-01,\n",
      "       -1.7272e-01, -6.7020e-01,  2.7096e-01, -4.3241e-01,  4.3103e-02,\n",
      "        2.1233e-02,  1.3350e-02, -6.3938e-02, -2.4957e-01, -2.4938e-01,\n",
      "        3.4812e-01, -7.1321e-02,  2.3375e-01, -9.5384e-02,  5.2488e-01,\n",
      "        6.8175e-01, -1.0214e-01, -1.4914e-01, -7.5697e-02,  1.7248e-01,\n",
      "        2.5440e-01,  1.5760e-01, -5.9125e-01,  2.4300e-01,  6.3962e-01,\n",
      "       -9.3280e-02, -2.7914e-01, -6.6262e-02, -6.7170e-02, -4.0929e-01,\n",
      "       -3.0300e+00,  1.8250e-01,  2.0113e-01,  6.0628e-02, -2.4769e-01,\n",
      "        5.5324e-02, -4.9106e-01,  3.1544e-01, -3.4231e-01, -6.3766e-01,\n",
      "       -3.6129e-01, -5.9029e-02,  1.5510e-01,  4.4577e-02,  2.3572e-01,\n",
      "       -1.7095e-01, -2.2749e-01, -2.3184e-02,  2.3868e-01,  2.8170e-02,\n",
      "        4.2965e-01, -1.2458e-01, -3.6972e-02,  2.0061e-01, -3.1405e-01,\n",
      "       -8.5287e-02, -3.3496e-01, -9.7047e-02, -1.4388e-01,  1.1147e-01,\n",
      "       -4.5232e-01, -2.4217e-01, -1.8245e-01, -6.7292e-01,  2.1933e-02,\n",
      "       -5.4816e-02, -4.6508e-01,  4.7767e-01, -2.4752e-01, -1.5790e-01,\n",
      "        1.1817e-01,  5.6851e-02, -4.9151e-01,  1.5496e-01,  1.6425e-02,\n",
      "        4.1650e-02, -3.4990e-01, -1.5979e-01,  3.9705e-01,  2.2963e-01,\n",
      "        2.4688e-01,  1.9567e-02, -2.8802e-01, -6.9983e-01,  3.2744e-01,\n",
      "        1.0833e-01,  2.4945e-01, -7.8653e-01, -6.1379e-02, -3.7359e-01,\n",
      "       -1.1603e-01, -2.4950e-01,  1.0161e-01,  3.3994e-02,  1.5650e-01,\n",
      "        2.1344e-01, -1.1094e-01, -5.7687e-03,  1.7869e-01, -1.0127e-01,\n",
      "       -1.6891e-02,  3.0001e-01, -3.4116e-01, -3.2390e-02,  4.2514e-02,\n",
      "        1.1850e-01, -1.8337e-01, -6.2865e-01, -2.8021e-01,  4.2351e-01,\n",
      "        1.1277e-01,  1.2121e-03,  1.5710e-01, -3.6321e-01, -4.9251e-01,\n",
      "        1.1653e-01,  2.4024e-01,  1.7712e-01,  6.8700e-02, -4.4137e-01,\n",
      "       -2.9877e-01, -1.2071e-02,  2.8325e-01,  1.0668e-01, -1.8859e-01,\n",
      "       -4.1345e-01, -3.4090e-01,  4.7236e-02, -3.8309e-01,  4.3572e-01,\n",
      "        2.4505e-01,  2.7337e-01, -7.3038e-02,  4.2514e-01, -3.2455e-02,\n",
      "       -3.5211e-01,  4.5691e-01,  1.9433e-01, -1.5230e-01,  4.2675e-01,\n",
      "        2.8795e-01, -5.5969e-01, -1.3031e-01,  8.9844e-02,  4.2605e-01,\n",
      "       -1.9632e-01, -7.1989e-02, -8.0189e-02, -3.0425e-01, -4.6190e-01,\n",
      "        2.8178e-01, -9.9872e-02,  3.5097e-01,  1.6123e-01, -3.6548e-02,\n",
      "       -3.6739e-01, -1.9819e-02,  3.2130e-01,  1.7479e-01,  2.5175e-01,\n",
      "       -7.6439e-03, -9.3786e-02, -3.7852e-01,  4.3725e-01,  2.1288e-01,\n",
      "        2.5096e-01, -1.9613e-01, -2.8865e-01, -5.6726e-03,  4.2795e-01,\n",
      "        2.0625e-01, -3.7701e-02, -1.2200e-01, -7.9253e-02, -1.0290e-01,\n",
      "        1.0558e-02,  4.9880e-01,  2.5382e-01,  1.5526e-01,  1.7951e-03,\n",
      "        1.1633e-01,  7.9300e-02, -3.9142e-01, -3.2483e-01,  6.3451e-01,\n",
      "       -1.8910e-01,  5.4050e-02,  1.6495e-01,  1.8757e-01,  5.3874e-01],\n",
      "      dtype=float32),\n",
      " array([-8.1760e-02,  9.5050e-01, -1.9627e-01,  3.2322e-01, -1.1475e-01,\n",
      "        7.2491e-01,  1.9991e-01, -4.1873e-01, -2.1523e-01,  2.2864e+00,\n",
      "       -3.7111e-01,  2.0624e-01, -3.4831e-01, -2.4620e-01,  1.9677e-01,\n",
      "       -7.9295e-02, -3.6580e-01,  1.1856e+00, -3.5559e-03,  5.7556e-01,\n",
      "       -2.3356e-01, -4.5721e-01,  1.8607e-01,  5.3613e-01, -5.5108e-02,\n",
      "        2.6244e-01, -1.9040e-02, -5.3399e-02,  9.9329e-01, -4.6326e-02,\n",
      "       -5.8343e-01,  2.4831e-01,  2.3628e-01,  3.6735e-01, -4.1087e-01,\n",
      "       -2.4192e-02, -3.8454e-01,  3.3113e-01, -2.5281e-01, -2.1088e-02,\n",
      "       -2.9030e-01, -3.1195e-01, -3.7847e-01,  7.9862e-01,  3.3756e-01,\n",
      "        6.0462e-01,  2.1106e-01,  1.7161e-02,  1.6436e-01, -3.4147e-01,\n",
      "        3.2015e-01,  3.4738e-01, -6.0944e-01,  2.8853e-01,  4.7932e-01,\n",
      "        5.8479e-01, -7.5082e-01, -4.4402e-01,  6.5224e-03, -1.0748e-01,\n",
      "        9.1903e-02, -1.1566e+00, -7.5945e-02, -1.7315e-01, -6.5896e-01,\n",
      "       -7.6838e-01,  6.9863e-01, -3.9328e-02,  3.5764e-01,  5.6520e-01,\n",
      "        2.0184e-02, -4.9523e-01, -1.9349e-01, -1.6587e-01,  8.3815e-01,\n",
      "        5.0432e-01,  5.3014e-01,  2.2458e-01, -3.8416e-01,  6.2961e-01,\n",
      "        1.9621e-01,  5.1145e-02,  8.5893e-02, -3.1473e-01,  3.4123e-01,\n",
      "       -1.9530e-01,  5.7403e-01, -8.9242e-01,  6.0832e-01, -2.1274e-02,\n",
      "        2.0518e-01, -3.1363e-01, -8.1625e-01,  2.9493e-01,  3.4002e-01,\n",
      "        2.3381e-01,  4.7663e-01,  6.4454e-02, -1.7138e-01, -1.0685e-01,\n",
      "       -1.1225e-02, -5.8588e-02, -2.0039e-01, -2.4616e-01,  3.6297e-01,\n",
      "        5.6525e-01, -5.1624e-01, -8.0988e-01,  2.7651e-02,  1.0138e-01,\n",
      "       -2.4744e-01, -3.0343e-01,  3.0367e-01, -4.2305e-01,  5.6549e-01,\n",
      "       -1.1670e-01, -3.1203e-01, -5.6972e-01, -8.0784e-02, -9.2206e-02,\n",
      "        1.1807e-01, -3.7990e-01,  4.1985e-01, -2.6395e-01,  5.8506e-01,\n",
      "       -3.6453e-02, -3.0516e-02, -1.4047e-01, -4.9372e-01,  5.1860e-02,\n",
      "        1.0483e-01, -4.4887e-01, -6.6936e-01, -8.9778e-02,  1.3845e-01,\n",
      "       -6.9786e-01, -4.3077e-01,  2.3854e-01, -1.3252e-01,  2.1398e-01,\n",
      "       -1.3762e+00,  1.1814e-01,  1.6111e-01,  1.2531e-01,  5.4347e-02,\n",
      "        5.4878e-01, -3.7092e-01, -1.0099e-01, -7.2403e-02, -6.6552e-01,\n",
      "       -3.4027e-01,  3.2232e-02, -7.6320e-02,  3.7552e-01, -4.1588e-01,\n",
      "        2.2676e-01,  3.4596e-01, -3.3434e-01, -5.3742e-01,  9.4397e-02,\n",
      "       -1.7104e-02, -2.6160e-01,  2.7804e-01, -6.5511e-02,  7.3646e-02,\n",
      "        2.5776e-01, -3.5709e-02, -1.4276e-01, -2.0539e-01, -1.8760e-01,\n",
      "       -5.5609e-01, -3.2273e-01,  2.9594e-01, -3.9765e-01, -2.5016e-02,\n",
      "       -4.8199e-02, -3.4339e-01, -1.7692e-01,  2.5867e-01,  6.8487e-01,\n",
      "       -3.3053e-01, -5.7565e-02, -3.3737e-01, -1.2563e-01, -2.9366e-01,\n",
      "        1.9139e-01,  5.3887e-02, -2.7826e-02,  1.9512e-01,  1.1829e-01,\n",
      "       -5.8045e-01,  3.5679e-01, -6.1477e-01, -5.1816e-01,  1.4871e-01,\n",
      "        8.7451e-01, -2.3132e-01,  4.0470e-01, -7.8567e-02, -2.0145e-01,\n",
      "        4.3885e-01, -1.0504e-01,  6.6496e-02,  1.5818e-01,  3.2686e-01,\n",
      "        7.7705e-02, -1.3774e-01, -2.8313e-01,  5.6313e-02,  3.0749e-02,\n",
      "       -1.6830e-01, -2.4820e-01,  1.6009e-01, -2.7654e-01,  2.6247e-01,\n",
      "       -7.1188e-03, -3.4712e-01,  2.9753e-01, -3.5213e-01, -3.3462e-02,\n",
      "        3.2864e-01, -4.1862e-02,  3.7404e-01,  2.5576e-01,  1.6339e-01,\n",
      "       -7.5334e-02,  3.4678e-01,  1.8152e-01, -3.8499e-01, -3.7584e-01,\n",
      "       -9.0580e-02,  1.9120e-01, -3.0927e-01,  1.3177e-01, -2.7881e-01,\n",
      "       -5.6982e-01, -6.6286e-01, -3.0949e-01, -2.0002e-01,  4.9368e-01,\n",
      "        3.4719e-01,  2.3746e-02,  1.0352e-01,  2.8529e-01, -4.0391e-01,\n",
      "       -4.0647e-01, -3.9268e-02, -1.4621e-01,  3.4379e-01,  8.5500e-01,\n",
      "       -7.4400e-02,  1.5851e-01, -7.3204e-02, -7.2655e-02,  6.4226e-01,\n",
      "       -2.3160e-01,  7.3683e-03, -9.2140e-02,  6.3533e-02, -7.4703e-02,\n",
      "        3.4700e-01,  4.2235e-01,  4.8242e-01,  1.7781e-01, -1.4147e-01,\n",
      "       -1.8437e-01, -2.4470e-01,  8.4425e-02, -1.0549e-03,  1.0658e-01,\n",
      "       -2.5153e-01, -1.7086e-01, -6.8509e-01, -1.2899e-01,  8.3986e-02,\n",
      "        1.5304e-01,  1.1152e-01, -4.2578e-01, -3.4582e-01,  7.9028e-01,\n",
      "        2.5903e-01, -5.1056e-01,  7.9888e-02, -2.5027e-02,  1.8446e-01,\n",
      "        2.6668e-02,  2.2356e-01, -1.5973e-01,  1.0135e-01,  8.0687e-01,\n",
      "        6.3786e-02,  2.2335e-01, -2.6171e-01, -4.6166e-01,  3.7709e-02,\n",
      "       -7.7545e-02, -1.7156e-01, -1.5184e-01, -2.3864e-01,  4.1293e-01],\n",
      "      dtype=float32),\n",
      " array([ 0.036775 ,  0.40917  , -0.52141  , -0.067184 ,  0.087702 ,\n",
      "       -0.048564 ,  0.40947  , -0.42818  ,  0.19304  ,  2.3925   ,\n",
      "       -0.11441  , -0.22952  , -0.16061  ,  0.035533 , -0.53179  ,\n",
      "        0.19764  , -0.48827  ,  0.57439  , -0.064301 ,  0.47053  ,\n",
      "       -0.29647  , -0.15927  , -0.052798 ,  0.10121  , -0.054461 ,\n",
      "        0.036129 , -0.16118  , -0.34139  ,  0.45834  , -0.20144  ,\n",
      "       -0.29067  , -0.51888  , -0.062106 ,  0.14084  ,  0.016413 ,\n",
      "        0.050826 ,  0.13243  , -0.033663 , -0.42228  , -0.30086  ,\n",
      "        0.06202  ,  0.26338  ,  0.077223 ,  0.27307  ,  0.13392  ,\n",
      "        0.30183  , -0.16546  ,  0.057011 , -0.0034585, -0.071113 ,\n",
      "       -0.27287  , -0.10297  ,  0.07457  , -0.32104  ,  0.36696  ,\n",
      "        0.27051  , -0.15776  ,  0.2978   , -0.18988  ,  0.097477 ,\n",
      "        0.035665 , -0.49749  , -0.52759  , -0.046148 ,  0.021715 ,\n",
      "       -0.11047  , -0.18007  ,  0.20295  ,  0.15254  , -0.045976 ,\n",
      "       -0.21846  , -0.066865 , -0.21355  ,  0.017509 ,  0.66474  ,\n",
      "        0.25527  ,  0.24864  , -0.094851 , -0.012857 ,  0.46896  ,\n",
      "        0.052031 ,  0.62488  , -0.12662  ,  0.063972 , -0.15719  ,\n",
      "       -0.45907  ,  0.32286  , -0.17502  ,  0.64181  ,  0.091587 ,\n",
      "       -0.075871 ,  0.11718  , -0.13864  ,  0.24951  , -0.40664  ,\n",
      "        0.08845  , -0.29196  , -0.51624  , -0.074847 , -0.012822 ,\n",
      "       -0.088844 , -0.19935  ,  0.052734 , -0.13588  ,  0.231    ,\n",
      "       -0.34368  ,  0.30607  , -0.21223  ,  0.08178  ,  0.10097  ,\n",
      "        0.33585  , -0.17491  ,  0.019115 ,  0.15998  ,  0.38803  ,\n",
      "       -0.35932  ,  0.31682  , -0.18614  ,  0.11732  , -0.068517 ,\n",
      "        0.50785  , -0.0035486,  0.20069  ,  0.25218  ,  0.38309  ,\n",
      "        0.19359  ,  0.43857  , -0.29954  , -0.14219  ,  0.087962 ,\n",
      "       -0.14229  ,  0.10075  , -0.58986  , -0.12672  ,  0.036944 ,\n",
      "       -0.050421 , -0.19875  , -0.051368 , -0.023402 ,  0.08744  ,\n",
      "       -2.4938   ,  0.15427  ,  0.12373  , -0.0086429, -0.17007  ,\n",
      "       -0.519    , -0.29962  ,  0.24369  , -0.20535  , -0.24942  ,\n",
      "       -0.079362 ,  0.40986  , -0.10753  ,  0.098907 , -0.063449 ,\n",
      "        0.05373  ,  0.26206  ,  0.13207  , -0.067694 , -0.56168  ,\n",
      "       -0.18867  ,  0.14453  , -0.22469  , -0.28404  ,  0.20909  ,\n",
      "       -0.46989  ,  0.30992  , -0.13283  ,  0.041392 ,  0.11146  ,\n",
      "        0.17015  , -0.059407 , -0.16098  , -0.2211   , -0.0035877,\n",
      "       -0.22357  , -0.01852  , -0.23026  , -0.18824  ,  0.32997  ,\n",
      "        0.16287  , -0.52067  ,  0.17308  , -0.024264 , -0.041321 ,\n",
      "       -0.3241   , -0.44122  , -0.11114  ,  0.22684  , -0.10883  ,\n",
      "       -0.1278   , -0.16696  ,  0.051048 , -0.12131  ,  0.18038  ,\n",
      "        0.19793  ,  0.134    , -0.37113  ,  0.36008  ,  0.092685 ,\n",
      "       -0.30263  ,  0.16565  , -0.10863  , -0.29565  ,  0.26143  ,\n",
      "        0.13369  , -0.090181 ,  0.021989 , -0.093353 , -0.20325  ,\n",
      "       -0.2008   ,  0.20721  ,  0.17208  , -0.20199  ,  0.043315 ,\n",
      "        0.17768  ,  0.57448  , -0.45917  , -0.077197 ,  0.12051  ,\n",
      "        0.07209  , -0.095313 ,  0.10973  ,  0.22375  ,  0.045804 ,\n",
      "       -0.13573  ,  0.14041  , -0.11364  , -0.46605  , -0.43262  ,\n",
      "       -0.058678 ,  0.19043  , -0.40867  ,  0.30509  ,  0.18542  ,\n",
      "        0.095309 , -0.42329  , -0.15225  , -0.13827  ,  0.18119  ,\n",
      "        0.14755  , -0.053628 ,  0.031298 ,  0.65695  , -0.1717   ,\n",
      "        0.23649  , -0.34742  , -0.17438  , -0.085304 ,  0.37687  ,\n",
      "        0.21322  , -0.13184  , -0.35197  , -0.14072  ,  0.2332   ,\n",
      "        0.21014  , -0.14763  ,  0.047515 , -0.27979  ,  0.090331 ,\n",
      "       -0.15565  ,  0.42803  , -0.019297 ,  0.012198 ,  0.036031 ,\n",
      "       -0.10396  ,  0.11014  ,  0.13458  ,  0.2775   ,  0.36225  ,\n",
      "       -0.35591  , -0.16877  , -0.41201  ,  0.070133 , -0.27769  ,\n",
      "        0.13739  , -0.057831 ,  0.19277  ,  0.11131  ,  0.53696  ,\n",
      "        0.0093424, -0.26107  , -0.38663  ,  0.040653 ,  0.18617  ,\n",
      "        0.26312  ,  0.12212  , -0.030012 ,  0.096286 ,  0.47376  ,\n",
      "       -0.21633  ,  0.10798  , -0.17703  ,  0.22116  ,  0.6726   ,\n",
      "        0.065036 , -0.017414 , -0.048585 , -0.090863 ,  0.28591  ],\n",
      "      dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# Create Doc object\n",
    "doc = nlp('I am happy')\n",
    "\n",
    "# Generate word vectors for each token\n",
    "pprint([token.vector for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " happy - happy  : 1.0\n",
      " happy - joyous : 0.5333030223846436\n",
      " happy - sad    : 0.6438988447189331\n",
      "joyous - happy  : 0.5333030223846436\n",
      "joyous - joyous : 1.0\n",
      "joyous - sad    : 0.4383276700973511\n",
      "   sad - happy  : 0.6438988447189331\n",
      "   sad - joyous : 0.4383276700973511\n",
      "   sad - sad    : 1.0\n"
     ]
    }
   ],
   "source": [
    "# Word similarities\n",
    "doc = nlp(\"happy joyous sad\")\n",
    "\n",
    "for token1 in doc:\n",
    "    for token2 in doc:\n",
    "        print('{:>6} - {:<6} : {}'.format(token1.text, token2.text, token1.similarity(token2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am happy - I am sad    : 0.9492464724721577\n",
      "I am happy - I am joyous : 0.9239675481730458\n"
     ]
    }
   ],
   "source": [
    "# Document similarities\n",
    "sent1 = nlp(\"I am happy\")\n",
    "sent2 = nlp(\"I am sad\")\n",
    "sent3 = nlp(\"I am joyous\")\n",
    "\n",
    "# Compute similarity between sent1 and sent2\n",
    "print('{} - {:<11} : {}'.format(sent1, sent2.text, sent1.similarity(sent2)))\n",
    "\n",
    "# Compute similarity between sent1 and sent3\n",
    "print('{} - {:} : {:<11}'.format(sent1, sent3.text, sent1.similarity(sent3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.14 Generating word vectors\n",
    "\n",
    "In this exercise, we will generate the pairwise similarity scores of all the words in a sentence. The sentence is available as __sent__ and has been printed to the console for your convenience.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "1. Create a Doc object doc for sent.\n",
    "2. In the nested loop, compute the similarity between token1 and token2.\n",
    "\n",
    "**Results**\n",
    "\n",
    "<font color=darkgreen>Good job! Notice how the words 'apples' and 'oranges' have the highest pairwaise similarity score. This is expected as they are both fruits and are more related to each other than any other pair of words.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       I - I        : 1.0\n",
      "       I - like     : 0.5554912686347961\n",
      "       I - apples   : 0.20442721247673035\n",
      "       I - and      : 0.3160785734653473\n",
      "       I - oranges  : 0.18824081122875214\n",
      "    like - I        : 0.5554912686347961\n",
      "    like - like     : 1.0\n",
      "    like - apples   : 0.32987144589424133\n",
      "    like - and      : 0.5267484188079834\n",
      "    like - oranges  : 0.2771747410297394\n",
      "  apples - I        : 0.20442721247673035\n",
      "  apples - like     : 0.32987144589424133\n",
      "  apples - apples   : 1.0\n",
      "  apples - and      : 0.24097733199596405\n",
      "  apples - oranges  : 0.7780942320823669\n",
      "     and - I        : 0.3160785734653473\n",
      "     and - like     : 0.5267484188079834\n",
      "     and - apples   : 0.24097733199596405\n",
      "     and - and      : 1.0\n",
      "     and - oranges  : 0.19245947897434235\n",
      " oranges - I        : 0.18824081122875214\n",
      " oranges - like     : 0.2771747410297394\n",
      " oranges - apples   : 0.7780942320823669\n",
      " oranges - and      : 0.19245947897434235\n",
      " oranges - oranges  : 1.0\n"
     ]
    }
   ],
   "source": [
    "# Read data\n",
    "sent = 'I like apples and oranges'\n",
    "\n",
    "# Create the doc object\n",
    "doc = nlp(sent)\n",
    "\n",
    "# Compute pairwise similarity scores\n",
    "for token1 in doc:\n",
    "    for token2 in doc:\n",
    "        print('{:>8} - {:<8} : {}'.format(token1.text, token2.text, token1.similarity(token2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.15 Computing similarity of Pink Floyd songs\n",
    "\n",
    "In this final exercise, you have been given lyrics of three songs by the British band Pink Floyd, namely 'High Hopes', 'Hey You' and 'Mother'. The lyrics to these songs are available as __hopes__, __hey__ and __mother__ respectively.\n",
    "\n",
    "Your task is to compute the pairwise similarity between __mother__ and __hopes__, and __mother__ and __hey__.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "1. Create Doc objects for mother, hopes and hey.\n",
    "2. Compute the similarity between mother and hopes.\n",
    "3. Compute the similarity between mother and hey.\n",
    "\n",
    "**Results**\n",
    "\n",
    "<font color=darkgreen>Excellent work! Notice that 'Mother' and 'Hey You' have a similarity score of 0.9 whereas 'Mother' and 'High Hopes' has a score of only 0.6. This is probably because 'Mother' and 'Hey You' were both songs from the same album 'The Wall' and were penned by Roger Waters. On the other hand, 'High Hopes' was a part of the album 'Division Bell' with lyrics by David Gilmour and his wife, Penny Samson. Treat yourself by listening to these songs. They're some of the best!</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mother - hopes : 0.8653561365788051\n",
      "mother - hey   : 0.9591095703289078\n"
     ]
    }
   ],
   "source": [
    "with open('data/mother.dat', 'r', encoding='utf-8') as f: mother = f.read()\n",
    "with open('data/hopes.dat' , 'r', encoding='utf-8') as f: hopes  = f.read()\n",
    "with open('data/hey.dat'   , 'r', encoding='utf-8') as f: hey    = f.read()\n",
    "    \n",
    "# Create Doc objects\n",
    "mother_doc = nlp(mother)\n",
    "hopes_doc = nlp(hopes)\n",
    "hey_doc = nlp(hey)\n",
    "\n",
    "# Print similarity between mother and hopes\n",
    "print('mother - hopes :', mother_doc.similarity(hopes_doc))\n",
    "\n",
    "# Print similarity between mother and hey\n",
    "print('mother - hey   :', mother_doc.similarity(hey_doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.16 Congratulations!\n",
    "\n",
    "1. Congratulations!\n",
    ">Congratulations on making it to the end of the course!\n",
    "\n",
    "2. Review\n",
    ">In this course, we learned about various feature engineering techniques for natural language processing in python. We started off by computing basic features such as character length and word length of documents. We then moved on to readability scores and learned various metrics that could help us deduce the amount of education required to comprehend a piece of text fully. Next, we were introduced to the spacy library and learned to perform tokenization and lemmatization. Building on these techniques, we proceeded to explore text cleaning. We also learned how to perform part of speech tagging and named entity recognition using spacy models and had a sneak peek at their applications. The third chapter was dedicated to n-gram modeling. We also explored an application of it in sentiment analysis of movie reviews. The final chapter saw us covering tf-idf vectors and cosine similarity. Using these concepts, we built a movie and a TED Talk recommender. The final lesson gave you a sneak peek into word embeddings and their use cases.\n",
    "\n",
    "3. Further resources\n",
    ">This, by no means, is the end of the road. Once you're done with this course, it is highly recommended that you take the following courses, also offered by DataCamp to muscle up your skills further.\n",
    "\n",
    "4. Thank you!\n",
    ">We hope you have enjoyed taking this course as much as we did developing it. Thank you and all the best with your data science journey!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aditional material\n",
    "\n",
    "- Datacamp course: https://learn.datacamp.com/courses/feature-engineering-for-nlp-in-python\n",
    "- POS annotations in spaCy: https://spacy.io/api/annotation#pos-tagging\n",
    "- NER annotations in spaCy: https://spacy.io/api/annotation#named-entities\n",
    "- TfidfVectorizer: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
