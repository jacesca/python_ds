{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "from pprint import pprint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp_es = spacy.load('es_core_news_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head of ted_talk: \n",
      "                                          transcript  \\\n",
      "0  We're going to talk — my — a new lecture, just...   \n",
      "1  This is a representation of your brain, and yo...   \n",
      "2  It's a great honor today to share with you The...   \n",
      "3  My passions are music, technology and making t...   \n",
      "4  It used to be that if you wanted to get a comp...   \n",
      "\n",
      "                                                 url  \n",
      "0  https://www.ted.com/talks/al_seckel_says_our_b...  \n",
      "1  https://www.ted.com/talks/aaron_o_connell_maki...  \n",
      "2  https://www.ted.com/talks/carter_emmart_demos_...  \n",
      "3  https://www.ted.com/talks/jared_ficklin_new_wa...  \n",
      "4  https://www.ted.com/talks/jeremy_howard_the_wo...  \n",
      "\n",
      "\n",
      "Head of fakenews: \n",
      "                                               title label\n",
      "0                       You Can Smell Hillary’s Fear  FAKE\n",
      "1  Watch The Exact Moment Paul Ryan Committed Pol...  FAKE\n",
      "2        Kerry to go to Paris in gesture of sympathy  REAL\n",
      "3  Bernie supporters on Twitter erupt in anger ag...  FAKE\n",
      "4   The Battle of New York: Why This Primary Matters  REAL\n"
     ]
    }
   ],
   "source": [
    "# Read data\n",
    "ted_talk = pd.read_csv('data/ted.csv')\n",
    "print(f'Head of ted_talk: \\n{ted_talk.head()}')\n",
    "\n",
    "fakenews = pd.read_csv('data/fakenews.csv', index_col=0)\n",
    "print(f'\\n\\nHead of fakenews: \\n{fakenews.head()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Text preprocessing, POS tagging and NER\n",
    "\n",
    "In this chapter, you will learn about tokenization and lemmatization. You will then learn how to perform text cleaning, part-of-speech tagging, and named entity recognition using the spaCy library. Upon mastering these concepts, you will proceed to make the Gettysburg address machine-friendly, analyze noun usage in fake news, and identify people mentioned in a TechCrunch article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Tokenization and Lemmatization\n",
    "\n",
    "1. Tokenization and Lemmatization\n",
    ">In NLP, we usually have to deal with texts from a variety of sources. For instance,\n",
    "\n",
    "2. Text sources\n",
    ">it can be a news article where the text is grammatically correct and proofread. It could be tweets containing shorthands and hashtags. It could also be comments on YouTube where people have a tendency to abuse capital letters and punctuations.\n",
    "\n",
    "3. Making text machine friendly\n",
    ">It is important that we standardize these texts into a machine friendly format. We want our models to treat similar words as the same. Consider the words Dogs and dog. Strictly speaking, they are different strings. However, they connotate the same thing. Similarly, reduction, reducing and reduce should also be standardized to the same string regardless of their form and case usage. Other examples include don't and do not, and won't and will not. In the next couple of lessons, we will learn techniques to achieve this.\n",
    "\n",
    "4. Text preprocessing techniques\n",
    ">The text processing techniques you use are dependent on the application you're working on. We'll be covering the common ones, including converting words into lowercase removing unnecessary whitespace, removing punctuation, removing commonly occurring words or stopwords, expanding contracted words like don't and removing special characters such as numbers and emojis.\n",
    "\n",
    "5. Tokenization\n",
    ">To do this, we must first understand tokenization. Tokenization is the process of splitting a string into its constituent tokens. These tokens may be sentences, words or punctuations and is specific to a particular language. In this course, we will primarily be focused with word and punctuation tokens. For instance, consider this sentence. Tokenizing it into its constituent words and punctuations will yield the following list of tokens. Tokenization also involves expanding contracted words. Therefore, a word like don't gets decomposed into two tokens: do and n't as can be seen in this example.\n",
    "\n",
    "6. Tokenization using spaCy\n",
    ">To perform tokenization in python, we will use the spacy library. We first import the spacy library. Next, we load a pre-trained English model 'en_core_web_sm' using spacy.load(). This will return a Language object that has the know-how to perform tokenization. This is stored in the variable nlp. Let's now define a string we want to tokenize. We pass this string into nlp to generate a spaCy Doc object. We store this in a variable named doc. This Doc object contains the required tokens (and many other things, as we will soon find out). We generate the list of tokens by using list comprehension as shown. This is essentially looping over doc and extracting the text of each token in each iteration. The result is as follows.\n",
    "\n",
    "7. Lemmatization\n",
    ">Lemmatization is the process of converting a word into its lowercased base form or lemma. This is an extremely powerful process of standardization. For instance, the words reducing, reduces, reduced and reduction, when lemmatized, are all converted into the base form reduce. Similarly be verbs such as am, are and is are converted into be. Lemmatization also allows us to convert words with apostrophes into their full forms. Therefore, n't is converted to not and 've is converted to have.\n",
    "\n",
    "8. Lemmatization using spaCy\n",
    ">When you pass the string into nlp, spaCy automatically performs lemmatization by default. Therefore, generating lemmas is identical to generating tokens except that we extract token.lemma_ in each iteration inside the list comprehension instead of token.text. Also, observe how spaCy converted the Is into -PRON-. This is standard behavior where every pronoun is converted into the string '-PRON-'.\n",
    "\n",
    "9. Let's practice!\n",
    ">Once we understand how to perform tokenization and lemmatization, performing the text preprocessing techniques described earlier becomes easier. Before we move to that, let's first practice our understanding of the concepts introduced so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I don't know what I'm doing here.\n",
      "Tokens: ['Hello', '!', 'I', 'do', \"n't\", 'know', 'what', 'I', \"'m\", 'doing', 'here', '.']\n",
      "Lemmas: ['hello', '!', '-PRON-', 'do', 'not', 'know', 'what', '-PRON-', 'be', 'do', 'here', '.']\n"
     ]
    }
   ],
   "source": [
    "# Initiliaze string\n",
    "string = \"Hello! I don't know what I'm doing here.\"\n",
    "print(string)\n",
    "\n",
    "# Create a Doc object\n",
    "doc = nlp(string)\n",
    "\n",
    "# Generate list of tokens\n",
    "tokens = [token.text for token in doc]\n",
    "print('Tokens:', tokens)\n",
    "\n",
    "# Generate list of lemmas\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "print('Lemmas:', lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! She doesn't know what They're doing here.\n",
      "Tokens: ['Hello', '!', 'She', 'does', \"n't\", 'know', 'what', 'They', \"'re\", 'doing', 'here', '.']\n",
      "Lemmas: ['hello', '!', '-PRON-', 'do', 'not', 'know', 'what', '-PRON-', 'be', 'do', 'here', '.']\n"
     ]
    }
   ],
   "source": [
    "# Initiliaze string\n",
    "string = \"Hello! She doesn't know what They're doing here.\"\n",
    "print(string)\n",
    "\n",
    "# Create a Doc object\n",
    "doc = nlp(string)\n",
    "\n",
    "# Generate list of tokens\n",
    "tokens = [token.text for token in doc]\n",
    "print('Tokens:', tokens)\n",
    "\n",
    "# Generate list of lemmas\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "print('Lemmas:', lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "¡Hola! Yo no se que estoy haciendo aquí.\n",
      "Tokens: ['¡', 'Hola', '!', 'Yo', 'no', 'se', 'que', 'estoy', 'haciendo', 'aquí', '.']\n",
      "Lemmas: ['¡', 'Hola', '!', 'Yo', 'no', 'se', 'que', 'estar', 'hacer', 'aquí', '.']\n"
     ]
    }
   ],
   "source": [
    "# Repeating in spanish\n",
    "string = \"¡Hola! Yo no se que estoy haciendo aquí.\"\n",
    "print(f'\\n{string}')\n",
    "\n",
    "# Create a Doc object\n",
    "doc = nlp_es(string)\n",
    "\n",
    "# Generate list of tokens\n",
    "tokens = [token.text for token in doc]\n",
    "print('Tokens:', tokens)\n",
    "\n",
    "# Generate list of lemmas\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "print('Lemmas:', lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Identifying lemmas\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "Identify the list of words from the choices which do not have the same lemma.\n",
    "\n",
    "**Answer the question**\n",
    "\n",
    "1. He, She, I, They\n",
    "2. Am, Are, Is, Was\n",
    "3. Increase, Increases, Increasing, Increased\n",
    "4. __Car, Bike, Truck, Bus__\n",
    "\n",
    "**Results**\n",
    "\n",
    "<font color=darkgreen>Good job! Although all these words refer to vehicles, they are words with distinct base forms.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Tokenizing the Gettysburg Address\n",
    "\n",
    "In this exercise, you will be tokenizing one of the most famous speeches of all time: the Gettysburg Address delivered by American President Abraham Lincoln during the American Civil War.\n",
    "\n",
    "The entire speech is available as a string named gettysburg.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "1. Load the en_core_web_sm model using spacy.load().\n",
    "2. Create a Doc object doc for the gettysburg string.\n",
    "3. Using list comprehension, loop over doc to generate the token texts.\n",
    "\n",
    "**Results**\n",
    "\n",
    "<font color=darkgreen>Excellent work! You now know how to tokenize a piece of text. In the next exercise, we will perform similar steps and conduct lemmatization.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Four', 'score', 'and', 'seven', 'years', 'ago', 'our', 'fathers', 'brought', 'forth', 'on', 'this', 'continent', ',', 'a', 'new', 'nation', ',', 'conceived', 'in', 'Liberty', ',', 'and', 'dedicated', 'to', 'the', 'proposition', 'that', 'all', 'men', 'are', 'created', 'equal', '.', 'Now', 'we', \"'re\", 'engaged', 'in', 'a', 'great', 'civil', 'war', ',', 'testing', 'whether', 'that', 'nation', ',', 'or', 'any', 'nation', 'so', 'conceived', 'and', 'so', 'dedicated', ',', 'can', 'long', 'endure', '.', 'We', \"'re\", 'met', 'on', 'a', 'great', 'battlefield', 'of', 'that', 'war', '.', 'We', \"'ve\", 'come', 'to', 'dedicate', 'a', 'portion', 'of', 'that', 'field', ',', 'as', 'a', 'final', 'resting', 'place', 'for', 'those', 'who', 'here', 'gave', 'their', 'lives', 'that', 'that', 'nation', 'might', 'live', '.', 'It', \"'s\", 'altogether', 'fitting', 'and', 'proper', 'that', 'we', 'should', 'do', 'this', '.', 'But', ',', 'in', 'a', 'larger', 'sense', ',', 'we', 'ca', \"n't\", 'dedicate', '-', 'we', 'can', 'not', 'consecrate', '-', 'we', 'can', 'not', 'hallow', '-', 'this', 'ground', '.', 'The', 'brave', 'men', ',', 'living', 'and', 'dead', ',', 'who', 'struggled', 'here', ',', 'have', 'consecrated', 'it', ',', 'far', 'above', 'our', 'poor', 'power', 'to', 'add', 'or', 'detract', '.', 'The', 'world', 'will', 'little', 'note', ',', 'nor', 'long', 'remember', 'what', 'we', 'say', 'here', ',', 'but', 'it', 'can', 'never', 'forget', 'what', 'they', 'did', 'here', '.', 'It', 'is', 'for', 'us', 'the', 'living', ',', 'rather', ',', 'to', 'be', 'dedicated', 'here', 'to', 'the', 'unfinished', 'work', 'which', 'they', 'who', 'fought', 'here', 'have', 'thus', 'far', 'so', 'nobly', 'advanced', '.', 'It', \"'s\", 'rather', 'for', 'us', 'to', 'be', 'here', 'dedicated', 'to', 'the', 'great', 'task', 'remaining', 'before', 'us', '-', 'that', 'from', 'these', 'honored', 'dead', 'we', 'take', 'increased', 'devotion', 'to', 'that', 'cause', 'for', 'which', 'they', 'gave', 'the', 'last', 'full', 'measure', 'of', 'devotion', '-', 'that', 'we', 'here', 'highly', 'resolve', 'that', 'these', 'dead', 'shall', 'not', 'have', 'died', 'in', 'vain', '-', 'that', 'this', 'nation', ',', 'under', 'God', ',', 'shall', 'have', 'a', 'new', 'birth', 'of', 'freedom', '-', 'and', 'that', 'government', 'of', 'the', 'people', ',', 'by', 'the', 'people', ',', 'for', 'the', 'people', ',', 'shall', 'not', 'perish', 'from', 'the', 'earth', '.', '\\n']\n"
     ]
    }
   ],
   "source": [
    "# Read data\n",
    "with open('data/gettysburg.dat','r', encoding='utf-8') as f: \n",
    "    gettysburg = f.read() \n",
    "\n",
    "# Create a Doc object\n",
    "doc = nlp(gettysburg)\n",
    "\n",
    "# Generate the tokens\n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Lemmatizing the Gettysburg address\n",
    "\n",
    "In this exercise, we will perform lemmatization on the same __gettysburg__ address from before.\n",
    "\n",
    "However, this time, we will also take a look at the speech, before and after lemmatization, and try to adjudge the kind of changes that take place to make the piece more machine friendly.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "1. Print the gettysburg address to the console.\n",
    "2. Loop over doc and extract the lemma for each token of gettysburg.\n",
    "3. Convert lemmas into a string using join.\n",
    "\n",
    "**Results**\n",
    "\n",
    "<font color=darkgreen>Excellent! You're now proficient at performing lemmatization using spaCy. Observe the lemmatized version of the speech. It isn't very readable to humans but it is in a much more convenient format for a machine to process.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before lemmatization: \n",
      "Four score and seven years ago our fathers brought forth on this continent, a new nation, conceived in Liberty, and dedicated to the proposition that all men are created equal. Now we're engaged in a great civil war, testing whether that nation, or any nation so conceived and so dedicated, can long endure. We're met on a great battlefield of that war. We've come to dedicate a portion of that field, as a final resting place for those who here gave their lives that that nation might live. It's altogether fitting and proper that we should do this. But, in a larger sense, we can't dedicate - we can not consecrate - we can not hallow - this ground. The brave men, living and dead, who struggled here, have consecrated it, far above our poor power to add or detract. The world will little note, nor long remember what we say here, but it can never forget what they did here. It is for us the living, rather, to be dedicated here to the unfinished work which they who fought here have thus far so nobly advanced. It's rather for us to be here dedicated to the great task remaining before us - that from these honored dead we take increased devotion to that cause for which they gave the last full measure of devotion - that we here highly resolve that these dead shall not have died in vain - that this nation, under God, shall have a new birth of freedom - and that government of the people, by the people, for the people, shall not perish from the earth.\n",
      "\n",
      "After lemmatization: \n",
      "four score and seven year ago -PRON- father bring forth on this continent , a new nation , conceive in Liberty , and dedicate to the proposition that all man be create equal . now -PRON- be engage in a great civil war , test whether that nation , or any nation so conceive and so dedicated , can long endure . -PRON- be meet on a great battlefield of that war . -PRON- have come to dedicate a portion of that field , as a final resting place for those who here give -PRON- life that that nation may live . -PRON- be altogether fitting and proper that -PRON- should do this . but , in a large sense , -PRON- can not dedicate - -PRON- can not consecrate - -PRON- can not hallow - this ground . the brave man , living and dead , who struggle here , have consecrate -PRON- , far above -PRON- poor power to add or detract . the world will little note , nor long remember what -PRON- say here , but -PRON- can never forget what -PRON- do here . -PRON- be for -PRON- the living , rather , to be dedicate here to the unfinished work which -PRON- who fight here have thus far so nobly advanced . -PRON- be rather for -PRON- to be here dedicated to the great task remain before -PRON- - that from these honor dead -PRON- take increase devotion to that cause for which -PRON- give the last full measure of devotion - that -PRON- here highly resolve that these dead shall not have die in vain - that this nation , under God , shall have a new birth of freedom - and that government of the people , by the people , for the people , shall not perish from the earth . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the gettysburg address\n",
    "print(f'Before lemmatization: \\n{gettysburg}')\n",
    "\n",
    "# Generate lemmas\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "\n",
    "# Convert lemmas into a string\n",
    "print('After lemmatization: \\n{}'.format(' '.join(lemmas)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. Text cleaning\n",
    "\n",
    "1. Text cleaning\n",
    ">Now that we know how to convert a string into a list of lemmas, we are now in a good position to perform basic text cleaning.\n",
    "\n",
    "2. Text cleaning techniques\n",
    ">Some of the most common text cleaning steps include removing extra whitespaces, escape sequences, punctuations, special characters such as numbers and stopwords. In other words, it is very common to remove non-alphabetic tokens and words that occur so commonly that they are not very useful for analysis.\n",
    "\n",
    "3. isalpha()\n",
    ">Every python string has an isalpha() method that returns true if all the characters of the string are alphabets. Therefore, the \"Dog\".isalpha() will return true but \"3dogs\".isalpha() will return false as it has a non-alphabetic character 3. Similarly, numbers, punctuations and emojis will all return false too. This is an extremely convenient method to remove all (lemmatized) tokens that are or contain numbers, punctuation and emojis.\n",
    "\n",
    "4. A word of caution\n",
    ">If isalpha() as a silver bullet that cleans text meticulously seems too good to be true, it's because it is. Remember that isalpha() has a tendency of returning false on words we would not want to remove. Examples include abbreviations such as USA and UK which have periods in them, and proper nouns with numbers in them such as word2vec and xto10x. For such nuanced cases, isalpha() may not be sufficient. It may be advisable to write your own custom functions, typically using regular expressions, to ensure you're not inadvertently removing useful words.\n",
    "\n",
    "5. Removing non-alphabetic characters\n",
    ">Consider the string here. This has a lot of punctuations, unnecessary extra whitespace, escape sequences, numbers and emojis. We will generate the lemmatized tokens like before.\n",
    "\n",
    "6. Removing non-alphabetic characters\n",
    ">Next, we loop through the tokens again and choose only those words that are either -PRON- or contain only alphabetic characters. Let's now print out the sanitized string. We see that all the non-alphabetic characters have been removed and each word is separated by a single space.\n",
    "\n",
    "7. Stopwords\n",
    ">There are some words in the English language that occur so commonly that it is often a good idea to just ignore them. Examples include articles such as a and the, be verbs such as is and am and pronouns such as he and she.\n",
    "\n",
    "8. Removing stopwords using spaCy\n",
    ">spaCy has a built-in list of stopwords which we can access using spacy.lang.en.stop_words.STOP_WORDS..\n",
    "\n",
    "9. Removing stopwords using spaCy\n",
    ">We make a small tweak to a_lemmas generation step. Notice that we have removed the -PRON- condition as pronouns are stopwords anyway and should be removed. Additionally, we have introduced a new condition to check if the word belongs to spacy's list of stopwords. The output is as follows. Notice how the string consists only of base form words. Always exercise caution while using third party stopword lists. It is common that an application find certain words useful that may be considered a stopword by third party lists. It is often advisable to create your custom stopword lists.\n",
    "\n",
    "10. Other text preprocessing techniques\n",
    ">There are other preprocessing techniques that are used but have been omitted for the sake of brevity. Some of them include removing HTML or XML tags, replacing accented characters and correcting spelling errors and shorthands\n",
    "\n",
    "11. A word of caution\n",
    ">We have covered a lot of text preprocessing techniques in the last couple of lessons. However, a word of caution is in place. The text preprocessing techniques you use is always dependent on the application. There are many applications which may find punctuations, numbers and emojis useful, so it may be wise to not remove them. In other cases, using all caps may be a good indicator of something. Remember to always use only those techniques that are relevant to your particular use case.\n",
    "\n",
    "12. Let's practice!\n",
    ">It's now time to practice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String to treat: \n",
      "\n",
      "OMG!!!! This is like the best thing ever \t\n",
      ".\n",
      "Wow, such an amazing song! I'm hooked. Top 5 definitely. ?\n",
      "\n",
      "\n",
      "After lemmatization: \n",
      "['\\n', 'OMG', '!', '!', '!', '!', 'this', 'be', 'like', 'the', 'good', 'thing', 'ever', '\\t\\n', '.', '\\n', 'wow', ',', 'such', 'an', 'amazing', 'song', '!', '-PRON-', 'be', 'hooked', '.', 'Top', '5', 'definitely', '.', '?', '\\n']\n",
      "\n",
      "After removing not alphabetic tokens:\n",
      "OMG this be like the good thing ever wow such an amazing song -PRON- be hooked Top definitely\n",
      "\n",
      "After removing stopwords:\n",
      "OMG like good thing wow amazing song hooked Top definitely \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Removing non-alphabetic characters\n",
    "string = \"\"\"\n",
    "OMG!!!! This is like the best thing ever \\t\\n.\n",
    "Wow, such an amazing song! I'm hooked. Top 5 definitely. ?\n",
    "\"\"\"\n",
    "print(f'String to treat: \\n{string}')\n",
    "\n",
    "# Generate list of tokens\n",
    "doc = nlp(string)\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "print(f'\\nAfter lemmatization: \\n{lemmas}')\n",
    "\n",
    "# Remove tokens that are not alphabetic\n",
    "a_lemmas = [lemma for lemma in lemmas \n",
    "            if lemma.isalpha() or lemma == '-PRON-']\n",
    "# Print string after text cleaning\n",
    "print('\\nAfter removing not alphabetic tokens:')\n",
    "print(' '.join(a_lemmas))\n",
    "\n",
    "# Get list of stopwords\n",
    "stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "# Remove stopwords and non-alphabetic tokens\n",
    "a_lemmas = [lemma for lemma in lemmas\n",
    "            if lemma.isalpha() and lemma not in stopwords]\n",
    "# Print string after text cleaning\n",
    "print('\\nAfter removing stopwords:')\n",
    "print(' '.join(a_lemmas),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['actualmente', 'acuerdo', 'adelante', 'ademas', 'además', 'adrede', 'afirmó', 'agregó', 'ahi', 'ahora']\n"
     ]
    }
   ],
   "source": [
    "# First 10 stopwords in spanish\n",
    "print(sorted(list(spacy.lang.es.stop_words.STOP_WORDS))[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6. Cleaning a blog post\n",
    "\n",
    "In this exercise, you have been given an excerpt from a blog post. Your task is to clean this text into a more machine friendly format. This will involve converting to lowercase, lemmatization and removing stopwords, punctuations and non-alphabetic characters.\n",
    "\n",
    "The excerpt is available as a string __blog__ and has been printed to the console. The list of __stopwords__ are available as stopwords.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "1. Using list comprehension, loop through doc to extract the lemma_ of each token.\n",
    "2. Remove stopwords and non-alphabetic tokens using stopwords and isalpha().\n",
    "\n",
    "**Results**\n",
    "\n",
    "<font color=darkgreen>Great job! Take a look at the cleaned text; it is lowercased and devoid of numbers, punctuations and commonly used stopwords. Also, note that the word U.S. was present in the original text. Since it had periods in between, our text cleaning process completely removed it. This may not be ideal behavior. It is always advisable to use your custom functions in place of isalpha() for more nuanced cases.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data: \n",
      "Twenty-first-century politics has witnessed an alarming rise of populism in the U.S. and Europe. The first warning signs came with the UK Brexit Referendum vote in 2016 swinging in the way of Leave. This was followed by a stupendous victory by billionaire Donald Trump to become the 45th President of the United States in November 2016. Since then, Europe has seen a steady rise in populist and far-right parties that have capitalized on Europe’s Immigration Crisis to raise nationalist and anti-Europe sentiments. Some instances include Alternative for Germany (AfD) winning 12.6% of all seats and entering the Bundestag, thus upsetting Germany’s political order for the first time since the Second World War, the success of the Five Star Movement in Italy and the surge in popularity of neo-nazism and neo-fascism in countries such as Hungary, Czech Republic, Poland and Austria.\n",
      "\n",
      "\n",
      "After lematization: \n",
      "['twenty', '-', 'first', '-', 'century', 'politic', 'have', 'witness', 'an', 'alarm', 'rise', 'of', 'populism', 'in', 'the', 'U.S.', 'and', 'Europe', '.', 'the', 'first', 'warning', 'sign', 'come', 'with', 'the', 'UK', 'Brexit', 'Referendum', 'vote', 'in', '2016', 'swinge', 'in', 'the', 'way', 'of', 'Leave', '.', 'this', 'be', 'follow', 'by', 'a', 'stupendous', 'victory', 'by', 'billionaire', 'Donald', 'Trump', 'to', 'become', 'the', '45th', 'President', 'of', 'the', 'United', 'States', 'in', 'November', '2016', '.', 'since', 'then', ',', 'Europe', 'have', 'see', 'a', 'steady', 'rise', 'in', 'populist', 'and', 'far', '-', 'right', 'party', 'that', 'have', 'capitalize', 'on', 'Europe', '’s', 'Immigration', 'Crisis', 'to', 'raise', 'nationalist', 'and', 'anti', '-', 'europe', 'sentiment', '.', 'some', 'instance', 'include', 'Alternative', 'for', 'Germany', '(', 'AfD', ')', 'win', '12.6', '%', 'of', 'all', 'seat', 'and', 'enter', 'the', 'Bundestag', ',', 'thus', 'upset', 'Germany', '’s', 'political', 'order', 'for', 'the', 'first', 'time', 'since', 'the', 'Second', 'World', 'War', ',', 'the', 'success', 'of', 'the', 'five', 'Star', 'Movement', 'in', 'Italy', 'and', 'the', 'surge', 'in', 'popularity', 'of', 'neo', '-', 'nazism', 'and', 'neo', '-', 'fascism', 'in', 'country', 'such', 'as', 'Hungary', ',', 'Czech', 'Republic', ',', 'Poland', 'and', 'Austria', '.', '\\n']\n",
      "\n",
      "After removing stopwords and non-alphabetic tokesn: \n",
      "century politic witness alarm rise populism Europe warning sign come UK Brexit Referendum vote swinge way Leave follow stupendous victory billionaire Donald Trump President United States November Europe steady rise populist far right party capitalize Europe Immigration Crisis raise nationalist anti europe sentiment instance include Alternative Germany AfD win seat enter Bundestag upset Germany political order time Second World War success Star Movement Italy surge popularity neo nazism neo fascism country Hungary Czech Republic Poland Austria\n"
     ]
    }
   ],
   "source": [
    "# Read data\n",
    "with open('data/blog.dat','r', encoding='utf-8') as f: \n",
    "    blog = f.read() \n",
    "print(f'Original data: \\n{blog}\\n')    \n",
    "\n",
    "# Create Doc object\n",
    "doc = nlp(blog)\n",
    "\n",
    "# Generate lemmatized tokens\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "print(f'After lematization: \\n{lemmas}\\n')\n",
    "\n",
    "# Remove stopwords and non-alphabetic tokens\n",
    "a_lemmas = [lemma for lemma in lemmas \n",
    "            if lemma.isalpha() and lemma not in stopwords]\n",
    "\n",
    "# Print string after text cleaning\n",
    "print('After removing stopwords and non-alphabetic tokesn: ')\n",
    "print(' '.join(a_lemmas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7. Cleaning TED talks in a dataframe\n",
    "\n",
    "In this exercise, we will revisit the TED Talks from the first chapter. You have been a given a dataframe __ted__ consisting of 5 TED Talks. Your task is to clean these talks using techniques discussed earlier by writing a function __preprocess__ and applying it to the __transcript__ feature of the dataframe.\n",
    "\n",
    "The stopwords list is available as __stopwords__.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "1. Generate the Doc object for text. Ignore the disable argument for now.\n",
    "2. Generate lemmas using list comprehension using the lemma_ attribute.\n",
    "3. Remove non-alphabetic characters using isalpha() in the if condition.\n",
    "\n",
    "**Results**\n",
    "\n",
    "<font color=darkgreen>Excellent job! You have preprocessed all the TED talk transcripts contained in ted and it is now in a good shape to perform operations such as vectorization (as we will soon see how). You now have a good understanding of how text preprocessing works and why it is important. In the next lessons, we will move on to generating word level features for our texts.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          transcript  \\\n",
      "0  We're going to talk — my — a new lecture, just...   \n",
      "1  This is a representation of your brain, and yo...   \n",
      "2  It's a great honor today to share with you The...   \n",
      "3  My passions are music, technology and making t...   \n",
      "4  It used to be that if you wanted to get a comp...   \n",
      "\n",
      "                                                 url  \\\n",
      "0  https://www.ted.com/talks/al_seckel_says_our_b...   \n",
      "1  https://www.ted.com/talks/aaron_o_connell_maki...   \n",
      "2  https://www.ted.com/talks/carter_emmart_demos_...   \n",
      "3  https://www.ted.com/talks/jared_ficklin_new_wa...   \n",
      "4  https://www.ted.com/talks/jeremy_howard_the_wo...   \n",
      "\n",
      "                                    clean_transcript  \n",
      "0  talk new lecture TED illusion create TED try r...  \n",
      "1  representation brain brain break left half log...  \n",
      "2  great honor today share Digital Universe creat...  \n",
      "3  passion music technology thing combination thi...  \n",
      "4  use want computer new program programming requ...  \n"
     ]
    }
   ],
   "source": [
    "# Read data\n",
    "ted = ted_talk.head().copy(deep = True)\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess(text, model=nlp):\n",
    "    \"\"\"Lemmatize a text and return it after cleaning stopwords and not alphanumerics tokens.\"\"\"\n",
    "    # Create Doc object without ner and parser\n",
    "    # ner: EntityRecognizer, parser: owers the sentence boundary detection\n",
    "    # Return lemmas without stopwords and non-alphabetic characters\n",
    "    return ' '.join([token.lemma_ for token in model(text, disable=['ner', 'parser']) \n",
    "                     if token.lemma_.isalpha() and token.lemma_ not in stopwords])\n",
    "  \n",
    "# Apply preprocess to ted['transcript']\n",
    "ted['clean_transcript'] = ted['transcript'].apply(preprocess)\n",
    "print(ted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.8. Part-of-speech tagging\n",
    "\n",
    "1. Part-of-speech tagging\n",
    ">In this lesson, we will cover part-of-speech tagging, which is one of the most popularly used feature engineering techniques in NLP.\n",
    "\n",
    "2. Applications\n",
    ">Part-of speech tagging or POS tagging has an immense number of applications in NLP. It is used in word-sense disambiguation to identify the sense of a word in a sentence. For instance, consider the sentences \"the bear is a majestic animal\" and \"please bear with me\". Both sentences use the word 'bear' but they mean different things. POS tagging helps in identifying this distinction by identifying one bear as a noun and the other as a verb. Consequentially, POS tagging is also used in sentiment analysis, question answering systems and linguistic approaches to detect fake news and opinion spam. For example, one paper discovered that fake news headlines, on average, tend to use lesser common nouns and more proper nouns than mainstream headlines. Generating the POS tags for these words proved extremely useful in detecting false or hyperpartisan news.\n",
    "\n",
    "3. POS tagging\n",
    ">So what is POS tagging? It is the process of assigning every word (or token) in a piece of text, its corresponding part-of-speech. For instance, consider the sentence \"Jane is an amazing guitarist\". A typical POS tagger will label Jane as a proper noun, is as a verb, an as a determiner (or an article), amazing as an adjective and finally, guitarist as a noun.\n",
    "\n",
    "4. POS tagging using spaCy\n",
    ">POS Tagging is extremely easy to do using spaCy's models and performing it is almost identical to generating tokens or lemmas. As usual, we import the spacy library and load the en_core_web_sm model as nlp. We will use the same sentence \"Jane is an amazing guitarist\" from before. We will then create a Doc object that will perform POS tagging, by default.\n",
    "\n",
    "5. POS tagging using spaCy\n",
    ">Using list comprehension, we generate a list of tuples pos where the first element of the tuple is the token and is generated using token.text and the second element is its POS tag, which is generated using token.pos_. Printing pos will give us the following output. Note how the tagger correctly identified all the parts-of-speech as we had discussed earlier. That said, remember that POS tagging is not an exact science. spaCy infers the POS tags of these words based on the predictions given by its pre-trained models. In other words, the accuracy of the POS tagging is dependent on the data that the model has been trained on and the data that it is being used on.\n",
    "\n",
    "6. POS annotations in spaCy\n",
    ">spaCy is capable of identifying close to 20 parts-of-speech and as we saw in the previous slide, it uses specific annotations to denote a particular part of speech. For instance, PROPN referred to a proper noun and DET referred to a determinant. You can find the complete list of POS annotations used by spaCy in spaCy's documentation. Here is a snapshot of the web page.\n",
    "\n",
    "7. Let's practice!\n",
    ">Great! Let's now practice our understanding of POS tagging in the next few exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original string: \n",
      "Jane is an amazing guitarist\n",
      "\n",
      "Part-of-speech tagging:\n",
      "[('Jane', 'PROPN'),\n",
      " ('is', 'AUX'),\n",
      " ('an', 'DET'),\n",
      " ('amazing', 'ADJ'),\n",
      " ('guitarist', 'NOUN')]\n"
     ]
    }
   ],
   "source": [
    "# Initiliaze string\n",
    "string = \"Jane is an amazing guitarist\"\n",
    "print(f'Original string: \\n{string}')\n",
    "\n",
    "# Create a Doc object\n",
    "doc = nlp(string)\n",
    "\n",
    "# Generate list of tokens and pos tags\n",
    "pos = [(token.text, token.pos_) for token in doc]\n",
    "print('\\nPart-of-speech tagging:')\n",
    "pprint(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original string: \n",
      "Ana es una fantástica guitarrista\n",
      "\n",
      "Part-of-speech tagging:\n",
      "[('Ana', 'PROPN'),\n",
      " ('es', 'AUX'),\n",
      " ('una', 'DET'),\n",
      " ('fantástica', 'ADJ'),\n",
      " ('guitarrista', 'NOUN')]\n"
     ]
    }
   ],
   "source": [
    "# Repeating in Spanish\n",
    "string = \"Ana es una fantástica guitarrista\"\n",
    "print(f'\\nOriginal string: \\n{string}')\n",
    "\n",
    "# Create a Doc object\n",
    "doc = nlp_es(string)\n",
    "\n",
    "# Generate list of tokens and pos tags\n",
    "pos = [(token.text, token.pos_) for token in doc]\n",
    "print('\\nPart-of-speech tagging:')\n",
    "pprint(pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.9. POS tagging in Lord of the Flies\n",
    "\n",
    "In this exercise, you will perform part-of-speech tagging on a famous passage from one of the most well-known novels of all time, Lord of the Flies, authored by William Golding.\n",
    "\n",
    "The passage is available as __lotf__ and has already been printed to the console.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "1. Load the en_core_web_sm model.\n",
    "2. Create a doc object for lotf using nlp().\n",
    "3. Using the text and pos_ attributes, generate tokens and their corresponding POS tags.\n",
    "\n",
    "**Results**\n",
    "\n",
    "<font color=darkgreen>Good job! Examine the various POS tags attached to each token and evaluate if they make intuitive sense to you. You will notice that they are indeed labelled correctly according to the standard rules of English grammar.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data: \n",
      "He found himself understanding the wearisomeness of this life, where every path was an improvisation and a considerable part of one’s waking life was spent watching one’s feet.\n",
      "\n",
      "[('He', 'PRON'),\n",
      " ('found', 'VERB'),\n",
      " ('himself', 'PRON'),\n",
      " ('understanding', 'VERB'),\n",
      " ('the', 'DET'),\n",
      " ('wearisomeness', 'NOUN'),\n",
      " ('of', 'ADP'),\n",
      " ('this', 'DET'),\n",
      " ('life', 'NOUN'),\n",
      " (',', 'PUNCT'),\n",
      " ('where', 'ADV'),\n",
      " ('every', 'DET'),\n",
      " ('path', 'NOUN'),\n",
      " ('was', 'AUX'),\n",
      " ('an', 'DET'),\n",
      " ('improvisation', 'NOUN'),\n",
      " ('and', 'CCONJ'),\n",
      " ('a', 'DET'),\n",
      " ('considerable', 'ADJ'),\n",
      " ('part', 'NOUN'),\n",
      " ('of', 'ADP'),\n",
      " ('one', 'NOUN'),\n",
      " ('’s', 'PART'),\n",
      " ('waking', 'VERB'),\n",
      " ('life', 'NOUN'),\n",
      " ('was', 'AUX'),\n",
      " ('spent', 'VERB'),\n",
      " ('watching', 'VERB'),\n",
      " ('one', 'PRON'),\n",
      " ('’s', 'PART'),\n",
      " ('feet', 'NOUN'),\n",
      " ('.', 'PUNCT')]\n"
     ]
    }
   ],
   "source": [
    "# Read data\n",
    "with open('data/lotf.dat','r', encoding='utf-8') as f: \n",
    "    lotf = f.read() \n",
    "print(f'Original data: \\n{lotf}\\n')    \n",
    "\n",
    "# Create a Doc object\n",
    "doc = nlp(lotf)\n",
    "\n",
    "# Generate tokens and pos tags\n",
    "pos = [(token.text, token.pos_) for token in doc]\n",
    "pprint(pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.10. Counting nouns in a piece of text\n",
    "\n",
    "In this exercise, we will write two functions, __nouns()__ and __proper_nouns()__ that will count the number of other nouns and proper nouns in a piece of text respectively.\n",
    "\n",
    "These functions will take in a piece of text and generate a list containing the POS tags for each word. It will then return the number of proper nouns/other nouns that the text contains. We will use these functions in the next exercise to generate interesting insights about fake news.\n",
    "\n",
    "The __en_core_web_sm__ model has already been loaded as nlp in this exercise.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "1. Using the list count method, count the number of proper nouns (annotated as PROPN) in the pos list.\n",
    "2. Using the list count method, count the number of other nouns (annotated as NOUN) in the pos list.\n",
    "\n",
    "**Results**\n",
    "\n",
    "<font color=darkgreen>Great job! You now know how to write functions that compute the number of instances of a particulat POS tag in a given piece of text. In the next exercise, we will use these functions to generate features from text in a dataframe.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns number of proper nouns\n",
    "def proper_nouns(text, model=nlp):\n",
    "    \"\"\"Return number of total proper nouns.\"\"\"\n",
    "    # Create doc object, generate list of POS tags and return number of proper nouns\n",
    "    return [token.pos_ for token in model(text)].count('PROPN')\n",
    "\n",
    "# Returns number of other nouns\n",
    "def nouns(text, model=nlp):\n",
    "    \"\"\"Returns number of total other nouns.\"\"\"\n",
    "    # Create doc object, generate list of POS tags and return number of other nouns\n",
    "    return [token.pos_ for token in model(text)].count('NOUN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original string   : Abdul, Bill and Cathy went to the market to buy apples.\n",
      "Proper nouns found: 3\n",
      "Other nouns found : 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First example in English\n",
    "string = \"Abdul, Bill and Cathy went to the market to buy apples.\"\n",
    "print(f'Original string   : {string}')\n",
    "print( 'Proper nouns found: {}'.format(proper_nouns(string, nlp)))\n",
    "print( 'Other nouns found : {}\\n'.format(nouns(string, nlp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original string   : Gabriela y José fueron al mercado a comprar manzanas.\n",
      "Proper nouns found: 2\n",
      "Other nouns found : 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Second example in Spanish\n",
    "string = 'Gabriela y José fueron al mercado a comprar manzanas.'\n",
    "print(f'Original string   : {string}')\n",
    "print( 'Proper nouns found: {}'.format(proper_nouns(string, nlp_es)))\n",
    "print( 'Other nouns found : {}\\n'.format(nouns(string, nlp_es)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.11. Noun usage in fake news\n",
    "\n",
    "In this exercise, you have been given a dataframe __headlines__ that contains news headlines that are either fake or real. Your task is to generate two new features __num_propn__ and __num_noun__ that represent the number of proper nouns and other nouns contained in the __title__ feature of __headlines__.\n",
    "\n",
    "Next, we will compute the mean number of proper nouns and other nouns used in fake and real news headlines and compare the values. If there is a remarkable difference, then there is a good chance that using the __num_propn__ and __num_noun__ features in fake news detectors will improve its performance.\n",
    "\n",
    "To accomplish this task, the functions __proper_nouns__ and __nouns__ that you had built in the previous exercise have already been made available to you.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "1. Create a new feature num_propn by applying proper_nouns to headlines['title'].\n",
    "2. Filter headlines to compute the mean number of proper nouns in fake news using the mean method.\n",
    "3. Repeat the process for other nous: create a feature 'num_noun' using nouns and compute the mean of other nouns\n",
    "\n",
    "**Results**\n",
    "\n",
    "<font color=darkgreen>Excellent work! You now know to construct features using POS tags information. Notice how the mean number of proper nouns is considerably higher for fake news than it is for real news. The opposite seems to be true in the case of other nouns. This fact can be put to great use in designing fake news detectors.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean no. of proper nouns in real and fake headlines are 2.42 and 4.58 respectively\n",
      "Mean no. of other nouns in real and fake headlines are 2.30 and 1.67 respectively\n"
     ]
    }
   ],
   "source": [
    "# Read data\n",
    "headlines = fakenews.copy(deep=True)\n",
    "\n",
    "# Compute the features: num_propn and num_noun\n",
    "headlines['num_propn'] = headlines['title'].apply(proper_nouns)\n",
    "headlines['num_noun'] = headlines['title'].apply(nouns)\n",
    "\n",
    "# Compute mean of proper nouns\n",
    "real_propn = headlines[headlines['label'] == 'REAL']['num_propn'].mean()\n",
    "fake_propn = headlines[headlines['label'] == 'FAKE']['num_propn'].mean()\n",
    "\n",
    "# Compute mean of other nouns\n",
    "real_noun = headlines[headlines['label'] == 'REAL']['num_noun'].mean()\n",
    "fake_noun = headlines[headlines['label'] == 'FAKE']['num_noun'].mean()\n",
    "\n",
    "# Print results\n",
    "print(\"Mean no. of proper nouns in real and fake headlines are %.2f and %.2f respectively\"%(real_propn, fake_propn))\n",
    "print(\"Mean no. of other nouns in real and fake headlines are %.2f and %.2f respectively\"%(real_noun, fake_noun))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.12. Named entity recognition\n",
    "\n",
    "1. Named entity recognition\n",
    ">The final technique we will learn as part of this chapter is named entity recognition.\n",
    "\n",
    "2. Applications\n",
    ">Named entity recognition or NER has a host of extremely useful applications. It is used to build efficient search algorithms and question answering systems. For instance, let us say you have a piece of text and you ask your system about the people that are being talked about in the text. NER would help the system in answering this question by identifying all the entities that refer to a person in the text. NER also found application with News Providers who use it to categorize their articles and Customer Service centers who use it to classify and record their complaints efficiently.\n",
    "\n",
    "3. Named entity recognition\n",
    ">Let us now get down to the definitions. A named entity is anything that can be denoted with a proper name or a proper noun. Named entity recognition or NER, therefore, is the process of identifying such named entities in a piece of text and classifying them into predefined categories such as person, organization, country, etc. For example, consider the text \"John Doe is a software engineer working at Google. He lives in France.\" Performing NER on this text will tell us that there are three named entities: John Doe, who is a person, Google, which is an organization and France, which is a country (or geopolitical entity)\n",
    "\n",
    "4. NER using spaCy\n",
    ">Like POS tagging, performing NER is extremely easy using spaCy's pre-trained models. Let's try to find the named entities in the same sentence we used earlier. As usual, we import the spacy library, load the required model and create a Doc object for the string. When we do this, spaCy automatically computes all the named entities and makes it available as the ents attribute of doc. Therefore, to access the named entity and its category, we use list comprehension to loop over doc.ents and create a tuple containing the entity name, which is accessed using ent.text, and entity category, which is accessed using ent.label_. Printing this list out will give the following output. We see that spaCy has correctly identified and classified all the named entities in this string.\n",
    "\n",
    "5. NER annotations in spaCy\n",
    ">Currently, spaCy's models are capable of identifying more than 15 different types of named entities. The complete list of categories and their annotations can be found in spaCy's documentatiion. Here is a snapshot of the page.\n",
    "\n",
    "6. A word of caution\n",
    ">In this chapter, we have used spacy's models to accomplish several tasks. However, remember that spacy's models are not perfect and its performance depends on the data it was trained with and the data it is being used on. For instance, if we are trying extract named entities for texts from a heavily technical field, such as medicine, spacy's pretrained models may not perform such a great job. In such nuanced cases, it is better to train your models with your specialized data. Also, remember that spacy's models are language specific. This is understandable considering that each language has its own grammar and nuances. The en_core_web_sm model that we've been using is, as the name suggests, only suitable for English texts.\n",
    "\n",
    "7. Let's practice!\n",
    ">This concludes our lesson on named entity recognition. Let us practice our understanding of this technique in the exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original string: John Doe is a software engineer working at Google. He lives in France.\n",
      "[('John Doe', 'PERSON'), ('Google', 'ORG'), ('France', 'GPE')]\n"
     ]
    }
   ],
   "source": [
    "# NER using spaCy with english language\n",
    "string = \"John Doe is a software engineer working at Google. He lives in France.\"\n",
    "print('Original string:', string)\n",
    "\n",
    "# Create Doc object\n",
    "doc = nlp(string)\n",
    "\n",
    "# Generate named entities\n",
    "ne = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "print(ne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original string: Gabriela Cortez es una arquitecta que trabaja en HDR, ella vive en Irlanda.\n",
      "Entities identified: [('Gabriela Cortez', 'PER'), ('Irlanda', 'LOC')]\n"
     ]
    }
   ],
   "source": [
    "# NER using spaCy with spanish language\n",
    "string = \"Gabriela Cortez es una arquitecta que trabaja en HDR, ella vive en Irlanda.\"\n",
    "print('Original string:', string)\n",
    "\n",
    "# Create Doc object\n",
    "doc = nlp_es(string)\n",
    "\n",
    "# Generate named entities\n",
    "ne = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "print('Entities identified:', ne)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.13. Named entities in a sentence\n",
    "\n",
    "In this exercise, we will identify and classify the labels of various named entities in a body of text using one of spaCy's statistical models. We will also verify the veracity of these labels.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "1. Use spacy.load() to load the en_core_web_sm model.\n",
    "2. Create a Doc instance doc using text and nlp.\n",
    "3. Loop over doc.ents to print all the named entities and their corresponding labels.\n",
    "\n",
    "**Results**\n",
    "\n",
    "<font color=darkgreen>Good job! Notice how the model correctly predicted the labels of Google and Mountain View but mislabeled Sundar Pichai as an organization. As discussed in the video, the predictions of the model depend strongly on the data it is trained on. It is possible to train spaCy models on your custom data. You will learn to do this in more advanced NLP courses.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original string: Sundar Pichai is the CEO of Google. Its headquarters is in Mountain View.\n",
      "Entities identified: [('Sundar Pichai', 'PERSON'), ('Google', 'ORG'), ('Mountain View', 'GPE')]\n"
     ]
    }
   ],
   "source": [
    "# Create a Doc instance \n",
    "string = 'Sundar Pichai is the CEO of Google. Its headquarters is in Mountain View.'\n",
    "doc = nlp(string)\n",
    "print('Original string:', string)\n",
    "\n",
    "# Generate named entities\n",
    "ne = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "print('Entities identified:', ne)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.14. Identifying people mentioned in a news article\n",
    "\n",
    "In this exercise, you have been given an excerpt from a news article published in TechCrunch. Your task is to write a function __find_people__ that identifies the names of people that have been mentioned in a particular piece of text. You will then use __find_people__ to identify the people of interest in the article.\n",
    "\n",
    "The article is available as the string tc and has been printed to the console. The required spacy model has also been already loaded as __nlp__.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "1. Create a Doc object for text.\n",
    "2. Using list comprehension, loop through doc.ents and create a list of named entities whose label is PERSON.\n",
    "3. Using find_persons(), print the people mentioned in tc.\n",
    "\n",
    "**Results**\n",
    "\n",
    "<font color=darkgreen>Excellent work! The article was related to Facebook and our function correctly identified both the people mentioned. You can now see how NER could be used in a variety of applications. Publishers may use a technique like this to classify news articles by the people mentioned in them. A question answering system could also use something like this to answer questions such as 'Who are the people mentioned in this passage?'. With this, we come to an end of this chapter. In the next, we will learn how to conduct vectorization on documents.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data: \n",
      "It’s' been a busy day for Facebook  exec op-eds. \n",
      "Earlier this morning, Sheryl Sandberg broke the site’s silence around the Christchurch massacre, \n",
      "and now Mark Zuckerberg is calling on governments and other bodies to increase regulation around the sorts of data Facebook traffics in. \n",
      "He’s hoping to get out in front of heavy-handed regulation and get a seat at the table shaping it.\n",
      "\n",
      "Identified persons:  ['Sheryl Sandberg', 'Mark Zuckerberg']\n"
     ]
    }
   ],
   "source": [
    "# Read the data\n",
    "with open('data/tc.dat','r', encoding='utf-8') as f: \n",
    "    tc = f.read() \n",
    "print(f'Original data: \\n{tc}\\n')   \n",
    "\n",
    "def find_persons(text, model=nlp):\n",
    "    \"\"\"Return the identified persons.\"\"\"\n",
    "    # Create Doc object and eturn the identified persons\n",
    "    return [ent.text for ent in model(text).ents if ent.label_ == 'PERSON']\n",
    "\n",
    "print(\"Identified persons: \", find_persons(tc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aditional material\n",
    "\n",
    "- Datacamp course: https://learn.datacamp.com/courses/feature-engineering-for-nlp-in-python\n",
    "- POS annotations in spaCy: https://spacy.io/api/annotation#pos-tagging\n",
    "- NER annotations in spaCy: https://spacy.io/api/annotation#named-entities"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
