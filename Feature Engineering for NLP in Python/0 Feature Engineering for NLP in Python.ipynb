{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering for NLP in Python\n",
    "\n",
    "In this course, you will learn techniques that will allow you to extract useful information from text and process them into a format suitable for applying ML models. More specifically, you will learn about POS tagging, named entity recognition, readability scores, the n-gram and tf-idf models, and how to implement them using scikit-learn and spaCy. You will also learn to compute how similar two documents are to each other. In the process, you will predict the sentiment of movie reviews and build movie and Ted Talk recommenders. Following the course, you will be able to engineer critical features out of any text and solve some of the most challenging problems in data science!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic features and readability scores\n",
    "\n",
    "Learn to compute basic features such as number of words, number of characters, average word length and number of special characters (such as Twitter hashtags and mentions). You will also learn to compute readability scores and determine the amount of education required to comprehend a piece of text.\n",
    "\n",
    "    1.1. Introduction to NLP feature engineering\n",
    "    1.2. Data format for ML algorithms\n",
    "    1.3. One-hot encoding\n",
    "    1.4. Basic feature extraction\n",
    "    1.5. Character count of Russian tweets\n",
    "    1.6. Word count of TED talks\n",
    "    1.7. Hashtags and mentions in Russian tweets\n",
    "    1.8. Readability tests\n",
    "    1.9. Readability of 'The Myth of Sisyphus'\n",
    "    1.10. Readability of various publications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text preprocessing, POS tagging and NER\n",
    "\n",
    "In this chapter, you will learn about tokenization and lemmatization. You will then learn how to perform text cleaning, part-of-speech tagging, and named entity recognition using the spaCy library. Upon mastering these concepts, you will proceed to make the Gettysburg address machine-friendly, analyze noun usage in fake news, and identify people mentioned in a TechCrunch article.\n",
    "\n",
    "    2.1. Tokenization and Lemmatization\n",
    "    2.2. Identifying lemmas\n",
    "    2.3. Tokenizing the Gettysburg Address\n",
    "    2.4. Lemmatizing the Gettysburg address\n",
    "    2.5. Text cleaning\n",
    "    2.6. Cleaning a blog post\n",
    "    2.7. Cleaning TED talks in a dataframe\n",
    "    2.8. Part-of-speech tagging\n",
    "    2.9. POS tagging in Lord of the Flies\n",
    "    2.10. Counting nouns in a piece of text\n",
    "    2.11. Noun usage in fake news\n",
    "    2.12. Named entity recognition\n",
    "    2.13. Named entities in a sentence\n",
    "    2.14. Identifying people mentioned in a news article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. N-Gram models\n",
    "\n",
    "Learn about n-gram modeling and use it to perform sentiment analysis on movie reviews.\n",
    "\n",
    "    3.1 Building a bag of words model\n",
    "    3.2 Word vectors with a given vocabulary\n",
    "    3.3 BoW model for movie taglines\n",
    "    3.4 Analyzing dimensionality and preprocessing\n",
    "    3.5 Mapping feature indices with feature names\n",
    "    3.6 Building a BoW Naive Bayes classifier\n",
    "    3.7 BoW vectors for movie reviews\n",
    "    3.8 Predicting the sentiment of a movie review\n",
    "    3.9 Building n-gram models\n",
    "    3.10 n-gram models for movie tag lines\n",
    "    3.11 Higher order n-grams for sentiment analysis\n",
    "    3.12 Comparing performance of n-gram models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. TF-IDF and similarity scores\n",
    "\n",
    "Learn how to compute tf-idf weights and the cosine similarity score between two vectors. You will use these concepts to build a movie and a TED Talk recommender. Finally, you will also learn about word embeddings and using word vector representations, you will compute similarities between various Pink Floyd songs.\n",
    "\n",
    "\n",
    "    4.1 Building tf-idf document vectors\n",
    "    4.2 tf-idf weight of commonly occurring words\n",
    "    4.3 tf-idf vectors for TED talks\n",
    "    4.4 Cosine similarity\n",
    "    4.5 Range of cosine scores\n",
    "    4.6 Computing dot product\n",
    "    4.7 Cosine similarity matrix of a corpus\n",
    "    4.8 Building a plot line based recommender\n",
    "    4.9 Comparing linear_kernel and cosine_similarity\n",
    "    4.10 Plot recommendation engine\n",
    "    4.11 The recommender function\n",
    "    4.12 TED talk recommender\n",
    "    4.13 Beyond n-grams: word embeddings\n",
    "    4.14 Generating word vectors\n",
    "    4.15 Computing similarity of Pink Floyd songs\n",
    "    4.16 Congratulations!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aditional material\n",
    "\n",
    "- Datacamp course: https://learn.datacamp.com/courses/feature-engineering-for-nlp-in-python"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
