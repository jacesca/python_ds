{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define global variables\n",
    "SEED = 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   MSSubClass  LotFrontage  LotArea  OverallQual  OverallCond  YearBuilt  \\\n",
      "0          60         65.0     8450            7            5       2003   \n",
      "1          20         80.0     9600            6            8       1976   \n",
      "2          60         68.0    11250            7            5       2001   \n",
      "3          70         60.0     9550            7            5       1915   \n",
      "4          60         84.0    14260            8            5       2000   \n",
      "\n",
      "   Remodeled  GrLivArea  BsmtFullBath  BsmtHalfBath  ...  HouseStyle_1.5Unf  \\\n",
      "0          0       1710             1             0  ...                  0   \n",
      "1          0       1262             0             1  ...                  0   \n",
      "2          1       1786             1             0  ...                  0   \n",
      "3          1       1717             1             0  ...                  0   \n",
      "4          0       2198             1             0  ...                  0   \n",
      "\n",
      "   HouseStyle_1Story  HouseStyle_2.5Fin  HouseStyle_2.5Unf  HouseStyle_2Story  \\\n",
      "0                  0                  0                  0                  1   \n",
      "1                  1                  0                  0                  0   \n",
      "2                  0                  0                  0                  1   \n",
      "3                  0                  0                  0                  1   \n",
      "4                  0                  0                  0                  1   \n",
      "\n",
      "   HouseStyle_SFoyer  HouseStyle_SLvl  PavedDrive_P  PavedDrive_Y  SalePrice  \n",
      "0                  0                0             0             1     208500  \n",
      "1                  0                0             0             1     181500  \n",
      "2                  0                0             0             1     223500  \n",
      "3                  0                0             0             1     140000  \n",
      "4                  0                0             0             1     250000  \n",
      "\n",
      "[5 rows x 57 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1460 entries, 0 to 1459\n",
      "Data columns (total 57 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   MSSubClass            1460 non-null   int64  \n",
      " 1   LotFrontage           1460 non-null   float64\n",
      " 2   LotArea               1460 non-null   int64  \n",
      " 3   OverallQual           1460 non-null   int64  \n",
      " 4   OverallCond           1460 non-null   int64  \n",
      " 5   YearBuilt             1460 non-null   int64  \n",
      " 6   Remodeled             1460 non-null   int64  \n",
      " 7   GrLivArea             1460 non-null   int64  \n",
      " 8   BsmtFullBath          1460 non-null   int64  \n",
      " 9   BsmtHalfBath          1460 non-null   int64  \n",
      " 10  FullBath              1460 non-null   int64  \n",
      " 11  HalfBath              1460 non-null   int64  \n",
      " 12  BedroomAbvGr          1460 non-null   int64  \n",
      " 13  Fireplaces            1460 non-null   int64  \n",
      " 14  GarageArea            1460 non-null   int64  \n",
      " 15  MSZoning_FV           1460 non-null   int64  \n",
      " 16  MSZoning_RH           1460 non-null   int64  \n",
      " 17  MSZoning_RL           1460 non-null   int64  \n",
      " 18  MSZoning_RM           1460 non-null   int64  \n",
      " 19  Neighborhood_Blueste  1460 non-null   int64  \n",
      " 20  Neighborhood_BrDale   1460 non-null   int64  \n",
      " 21  Neighborhood_BrkSide  1460 non-null   int64  \n",
      " 22  Neighborhood_ClearCr  1460 non-null   int64  \n",
      " 23  Neighborhood_CollgCr  1460 non-null   int64  \n",
      " 24  Neighborhood_Crawfor  1460 non-null   int64  \n",
      " 25  Neighborhood_Edwards  1460 non-null   int64  \n",
      " 26  Neighborhood_Gilbert  1460 non-null   int64  \n",
      " 27  Neighborhood_IDOTRR   1460 non-null   int64  \n",
      " 28  Neighborhood_MeadowV  1460 non-null   int64  \n",
      " 29  Neighborhood_Mitchel  1460 non-null   int64  \n",
      " 30  Neighborhood_NAmes    1460 non-null   int64  \n",
      " 31  Neighborhood_NPkVill  1460 non-null   int64  \n",
      " 32  Neighborhood_NWAmes   1460 non-null   int64  \n",
      " 33  Neighborhood_NoRidge  1460 non-null   int64  \n",
      " 34  Neighborhood_NridgHt  1460 non-null   int64  \n",
      " 35  Neighborhood_OldTown  1460 non-null   int64  \n",
      " 36  Neighborhood_SWISU    1460 non-null   int64  \n",
      " 37  Neighborhood_Sawyer   1460 non-null   int64  \n",
      " 38  Neighborhood_SawyerW  1460 non-null   int64  \n",
      " 39  Neighborhood_Somerst  1460 non-null   int64  \n",
      " 40  Neighborhood_StoneBr  1460 non-null   int64  \n",
      " 41  Neighborhood_Timber   1460 non-null   int64  \n",
      " 42  Neighborhood_Veenker  1460 non-null   int64  \n",
      " 43  BldgType_2fmCon       1460 non-null   int64  \n",
      " 44  BldgType_Duplex       1460 non-null   int64  \n",
      " 45  BldgType_Twnhs        1460 non-null   int64  \n",
      " 46  BldgType_TwnhsE       1460 non-null   int64  \n",
      " 47  HouseStyle_1.5Unf     1460 non-null   int64  \n",
      " 48  HouseStyle_1Story     1460 non-null   int64  \n",
      " 49  HouseStyle_2.5Fin     1460 non-null   int64  \n",
      " 50  HouseStyle_2.5Unf     1460 non-null   int64  \n",
      " 51  HouseStyle_2Story     1460 non-null   int64  \n",
      " 52  HouseStyle_SFoyer     1460 non-null   int64  \n",
      " 53  HouseStyle_SLvl       1460 non-null   int64  \n",
      " 54  PavedDrive_P          1460 non-null   int64  \n",
      " 55  PavedDrive_Y          1460 non-null   int64  \n",
      " 56  SalePrice             1460 non-null   int64  \n",
      "dtypes: float64(1), int64(56)\n",
      "memory usage: 650.3 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Read data\n",
    "housing_data = pd.read_csv(\"ames_housing_trimmed_processed.csv\")\n",
    "print(housing_data.head())\n",
    "print(housing_data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Fine-tuning your XGBoost model\n",
    "\n",
    "This chapter will teach you how to make your XGBoost models as performant as possible. You'll learn about the variety of parameters that can be adjusted to alter the behavior of XGBoost and how to tune them efficiently so that you can supercharge the performance of your models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Why tune your model?\n",
    "\n",
    "1. Why tune your model?\n",
    ">So far, you've learned how to use XGBoost to solve classification and regression problems. Now, you'll learn how to supercharge those models by tuning them. To motivate the reason behind this chapter on tuning your XGBoost model, let's just take a look at 2 cases, one where we take the simplest XGBoost model possible and compute a cross-validated RMSE, and then do the same exact thing with a tuned XGBoost model. What do you think the effect of model tuning on the overall reduction in RMSE will be?\n",
    "\n",
    "2. Untuned model example\n",
    ">In lines 1-6, we simply load in the necessary libraries and ames housing data, and then convert our data into a DMatrix. In line 7, we create the most basic parameter configuration possible, only passing in the objective function we need to create a regression XGBoost model. This parameter configuration will be made much more complex as we tune our models. In fact, when performing parameter searches, we will use a dictionary that we typically call a parameter grid, because it will contain ranges of values over which we will search to find an optimal configuration. More on that later. In line 8, we run our cross-validation in XGBoost, passing in the simple parameter grid and telling it to run 4-fold cross validation, and to ouput the rmse as an evaluation metric. In line 9, we simply print the final rmse of the untuned model to screen, which is around 34600 dollars.\n",
    "\n",
    "3. Tuned model example\n",
    ">Now let's take a look at a tuned example. Again, in lines 1-6, we load in the necessary libraries and ames housing data, and then convert our data into a DMatrix. In line 7, we create a more tuned parameter configuration, setting colsample_bytree, learning_rate, and max_depth to better values. These are a few of the more important xgboost parameters that can be tuned, and you will learn more about and practice tuning these parameters later in this chapter. In line 8, we run our cross-validation in XGBoost, passing in our tuned parameter grid, as well as setting the number of trees to be constructed at 200, and again running 4-fold cross validation, and outputting the rmse as an evaluation metric. In line 9, we print the final rmse of the tuned model to screen, which is around 29800 dollars. That's an almost 14% reduction in RMSE!\n",
    "\n",
    "4. Let's tune some models!\n",
    ">Now that you see that you can get a significant improvement in model performance by tuning an XGBoost model, let's have you start doing some tuning yourself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untuned rmse: 34624.229980\n"
     ]
    }
   ],
   "source": [
    "# Untuned model example\n",
    "#X, y = housing_data[housing_data.columns.tolist()[:-1]],\n",
    "#       housing_data[housing_data.columns.tolist()[-1]]\n",
    "X, y = housing_data.iloc[:,:-1],housing_data.iloc[:,-1]\n",
    "\n",
    "\n",
    "housing_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "\n",
    "untuned_params={\"objective\":\"reg:squarederror\"}\n",
    "untuned_cv_results_rmse = xgb.cv(dtrain=housing_dmatrix,\n",
    "                                 params=untuned_params,\n",
    "                                 nfold=4,\n",
    "                                 metrics=\"rmse\",\n",
    "                                 as_pandas=True,\n",
    "                                 seed=SEED)\n",
    "print(\"Untuned rmse: %f\" %((untuned_cv_results_rmse[\"test-rmse-mean\"]).tail(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned rmse: 29965.413086\n"
     ]
    }
   ],
   "source": [
    "# Tuned model example\n",
    "tuned_params = {\"objective\":\"reg:squarederror\",\n",
    "                'colsample_bytree': 0.3,\n",
    "                'learning_rate': 0.1, \n",
    "                'max_depth': 5}\n",
    "\n",
    "tuned_cv_results_rmse = xgb.cv(dtrain=housing_dmatrix,\n",
    "                               params=tuned_params, \n",
    "                               nfold=4, \n",
    "                               num_boost_round=200, \n",
    "                               metrics=\"rmse\",\n",
    "                               as_pandas=True, \n",
    "                               seed=SEED)\n",
    "print(\"Tuned rmse: %f\" %((tuned_cv_results_rmse[\"test-rmse-mean\"]).tail(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 When is tuning your model a bad idea?\n",
    "\n",
    "Now that you've seen the effect that tuning has on the overall performance of your XGBoost model, let's turn the question on its head and see if you can figure out when tuning your model might not be the best idea. Given that model tuning can be time-intensive and complicated, which of the following scenarios would NOT call for careful tuning of your model?\n",
    "\n",
    "**Possible Answers**\n",
    "\n",
    "1. You have lots of examples from some dataset and very many features at your disposal.\n",
    "2. <font color=red>You are very short on time before you must push an initial model to production and have little data to train your model on</font>. **Correct!**.\n",
    "3. You have access to a multi-core (64 cores) server with lots of memory (200GB RAM) and no time constraints.\n",
    "4. You must squeeze out every last bit of performance out of your xgboost model.\n",
    "\n",
    "**Results**\n",
    "\n",
    "<font color=darkgreen>Yup! You cannot tune if you do not have time!</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Tuning the number of boosting rounds\n",
    "\n",
    "Let's start with parameter tuning by seeing how the number of boosting rounds (number of trees you build) impacts the out-of-sample performance of your XGBoost model. You'll use xgb.cv() inside a for loop and build one model per num_boost_round parameter.\n",
    "\n",
    "Here, you'll continue working with the Ames housing dataset. The features are available in the array X, and the target vector is contained in y.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "1. Create a DMatrix called housing_dmatrix from X and y.\n",
    "2. Create a parameter dictionary called params, passing in the appropriate \"objective\" (\"reg:linear\") and \"max_depth\" (set it to 3).\n",
    "3. Iterate over num_rounds inside a for loop and perform 3-fold cross-validation. In each iteration of the loop, pass in the current number of boosting rounds (curr_num_rounds) to xgb.cv() as the argument to num_boost_round.\n",
    "4. Append the final boosting round RMSE for each cross-validated XGBoost model to the final_rmse_per_round list.\n",
    "5. num_rounds and final_rmse_per_round have been zipped and converted into a DataFrame so you can easily see how the model performs with each boosting round. Hit 'Submit Answer' to see the results!\n",
    "\n",
    "**Results**\n",
    "\n",
    "<font color=darkgreen>Awesome! As you can see, increasing the number of boosting rounds decreases the RMSE.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   num_boosting_rounds          rmse\n",
      "0                    5  50903.298177\n",
      "1                   10  34774.192709\n",
      "2                   15  32895.097656\n",
      "3                  100  30680.307292\n",
      "4                  150  30591.870443\n",
      "5                  175  30494.973958\n",
      "6                  180  30565.673177\n",
      "7                  200  30691.264974\n",
      "8                  300  30861.048177\n"
     ]
    }
   ],
   "source": [
    "# Create arrays for the features and the target: X, y\n",
    "X, y = housing_data.iloc[:,:-1],housing_data.iloc[:,-1]\n",
    "\n",
    "# Create the DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary for each tree: params \n",
    "params = {\"objective\":\"reg:squarederror\", \"max_depth\":3}\n",
    "\n",
    "# Create list of number of boosting rounds\n",
    "num_rounds = [5, 10, 15, 100, 150, 175, 180, 200, 300]\n",
    "\n",
    "# Empty list to store final round rmse per XGBoost model\n",
    "final_rmse_per_round = []\n",
    "\n",
    "# Iterate over num_rounds and build one model per num_boost_round parameter\n",
    "for curr_num_rounds in num_rounds:\n",
    "\n",
    "    # Perform cross-validation: cv_results\n",
    "    cv_results = xgb.cv(dtrain=housing_dmatrix, \n",
    "                        params=params, \n",
    "                        nfold=3, \n",
    "                        num_boost_round=curr_num_rounds, \n",
    "                        metrics=\"rmse\", \n",
    "                        as_pandas=True, \n",
    "                        seed=SEED)\n",
    "    \n",
    "    # Append final round RMSE\n",
    "    final_rmse_per_round.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "num_rounds_rmses = list(zip(num_rounds, final_rmse_per_round))\n",
    "print(pd.DataFrame(num_rounds_rmses,columns=[\"num_boosting_rounds\",\"rmse\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Automated boosting round selection using early_stopping\n",
    "\n",
    "Now, instead of attempting to cherry pick the best possible number of boosting rounds, you can very easily have XGBoost automatically select the number of boosting rounds for you within xgb.cv(). This is done using a technique called early stopping.\n",
    "\n",
    "Early stopping works by testing the XGBoost model after every boosting round against a hold-out dataset and stopping the creation of additional boosting rounds (thereby finishing training of the model early) if the hold-out metric (\"rmse\" in our case) does not improve for a given number of rounds. Here you will use the early_stopping_rounds parameter in xgb.cv() with a large possible number of boosting rounds (50). Bear in mind that if the holdout metric continuously improves up through when num_boost_rounds is reached, then early stopping does not occur.\n",
    "\n",
    "Here, the DMatrix and parameter dictionary have been created for you. Your task is to use cross-validation with early stopping. Go for it!\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "1. Perform 3-fold cross-validation with early stopping and \"rmse\" as your metric. Use 10 early stopping rounds and 50 boosting rounds. Specify a seed of 123 and make sure the output is a pandas DataFrame. Remember to specify the other parameters such as dtrain, params, and metrics.\n",
    "2. Print cv_results.\n",
    "\n",
    "**Results**\n",
    "\n",
    "<font color=darkgreen>Great work!</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    train-rmse-mean  train-rmse-std  test-rmse-mean  test-rmse-std\n",
      "0     141871.635417      403.636200   142640.656250     705.559400\n",
      "1     103057.036459       73.769561   104907.666667     111.114933\n",
      "2      75975.968750      253.729627    79262.057292     563.763447\n",
      "3      57420.531250      521.656755    61620.135417    1087.693857\n",
      "4      44552.955729      544.170190    50437.559896    1846.446535\n",
      "5      35763.947917      681.797248    43035.661458    2034.469207\n",
      "6      29861.463542      769.572072    38600.881510    2169.798065\n",
      "7      25994.675781      756.521419    36071.817708    2109.795430\n",
      "8      23306.836588      759.238254    34383.184896    1934.546688\n",
      "9      21459.769531      745.624999    33509.141276    1887.375284\n",
      "10     20148.721354      749.612769    32916.807943    1850.894476\n",
      "11     19215.382813      641.387565    32197.833333    1734.456784\n",
      "12     18627.389323      716.256794    31770.852865    1802.154800\n",
      "13     17960.694661      557.043073    31482.782552    1779.123767\n",
      "14     17559.736979      631.412969    31389.990886    1892.319967\n",
      "15     17205.712891      590.171393    31302.882162    1955.165902\n",
      "16     16876.571940      703.631755    31234.058594    1880.705796\n",
      "17     16597.662110      703.677609    31318.348959    1828.860617\n",
      "18     16330.460938      607.274493    31323.633464    1775.908279\n",
      "19     16005.972982      520.470911    31204.134766    1739.075860\n",
      "20     15814.301107      518.604304    31089.863281    1756.022288\n",
      "21     15493.405924      505.615987    31047.996094    1624.673955\n",
      "22     15270.734049      502.019527    31056.916016    1668.042813\n",
      "23     15086.381836      503.912899    31024.984375    1548.985605\n",
      "24     14917.608724      486.206468    30983.684245    1663.129605\n",
      "25     14709.590169      449.668438    30989.477865    1686.667141\n",
      "26     14457.286133      376.787666    30952.113281    1613.173550\n",
      "27     14185.567383      383.102234    31066.902344    1648.534310\n",
      "28     13934.066732      473.465449    31095.641927    1709.224745\n",
      "29     13749.645182      473.670437    31103.886068    1778.879154\n",
      "30     13549.836263      454.898488    30976.085287    1744.514533\n",
      "31     13413.485351      399.603618    30938.469401    1746.053896\n",
      "32     13275.916341      415.409108    30931.000651    1772.469906\n",
      "33     13085.878255      493.792509    30929.055990    1765.541073\n",
      "34     12947.181315      517.789250    30890.630208    1786.511479\n",
      "35     12846.027018      547.732805    30884.492187    1769.728223\n",
      "36     12702.378907      505.522877    30833.542969    1691.002065\n",
      "37     12532.244141      508.298241    30856.688151    1771.446377\n",
      "38     12384.054362      536.225108    30818.016927    1782.784630\n",
      "39     12198.444336      545.165562    30839.393229    1847.326009\n",
      "40     12054.583333      508.841412    30776.966146    1912.780507\n",
      "41     11897.036133      477.177991    30794.701823    1919.674347\n",
      "42     11756.221680      502.992782    30780.955078    1906.818667\n",
      "43     11618.846354      519.837088    30783.755860    1951.260705\n",
      "44     11484.080078      578.428250    30776.731120    1953.446310\n",
      "45     11356.553060      565.368380    30758.543620    1947.454953\n",
      "46     11193.558594      552.298906    30729.971354    1985.700237\n",
      "47     11071.315429      604.089960    30732.663411    1966.997809\n",
      "48     10950.777995      574.863209    30712.240886    1957.751118\n",
      "49     10824.865885      576.665756    30720.854818    1950.511520\n"
     ]
    }
   ],
   "source": [
    "# Create your housing DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary for each tree: params\n",
    "params = {\"objective\":\"reg:squarederror\", \"max_depth\":4}\n",
    "\n",
    "# Perform cross-validation with early stopping: cv_results\n",
    "cv_results = xgb.cv(dtrain = housing_dmatrix,\n",
    "                    params = params,\n",
    "                    nfold = 3,\n",
    "                    metrics = 'rmse',\n",
    "                    early_stopping_rounds = 10,\n",
    "                    num_boost_round = 50,\n",
    "                    seed = SEED,\n",
    "                    as_pandas = True)\n",
    "\n",
    "# Print cv_results\n",
    "print(cv_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Overview of XGBoost's hyperparameters\n",
    "\n",
    "1. Tunable parameters in XGBoost\n",
    ">Let's now go over the differences in what parameters can be tuned for each kind of base model in XGBoost. The parameters that can be tuned are significantly different for each base learner.\n",
    "\n",
    "2. Common tree tunable parameters\n",
    ">For the tree base learner, which is the one you should use in almost every single case, the most frequently tuned parameters are outlined below. The learning rate affects how quickly the model fits the residual error using additional base learners. A low learning rate will require more boosting rounds to achieve the same reduction in residual error as an XGBoost model with a high learning rate. Gamma, alpha, and lambda were described in chapter 2 and all have an effect on how strongly regularized the trained model will be. Max_depth must a positive integer value and affects how deeply each tree is allowed to grow during any given boosting round. Subsample must be a value between 0 and 1 and is the fraction of the total training set that can be used for any given boosting round. If the value is low, then the fraction of your training data used would per boosting round would be low and you may run into underfitting problems, a value that is very high can lead to overfitting as well. Colsample_bytree is the fraction of features you can select from during any given boosting round and must also be a value between 0 and 1. A large value means that almost all features can be used to build a tree during a given boosting round, whereas a small value means that the fraction of features that can be selected from is very small. In general, smaller colsample_bytree values can be thought of as providing additional regularization to the model, whereas using all columns may in certain cases overfit a trained model.\n",
    "\n",
    "3. Linear tunable parameters\n",
    ">For the linear base learner, the number of tunable parameters is significantly smaller. You only have access to l1 and l2 regularization on the weights associated with any given feature, and then another regularization term that can be applied to the model's bias. Finally, its important to mention that the number of boosting rounds (that is, either the number of trees you build or the number of linear base learners you construct) is itself a tunable parameter.\n",
    "\n",
    "4. Let's get to some tuning!\n",
    ">Now that we've covered the parameters that are usually tuned when using XGBoost, lets get to some tuning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree parameters\n",
    "\n",
    "- **learning rate**: affects how quickly the model fits the residual error using additional base learners. A low values require more boosting rounds to achieve the same reduction in residual error.\n",
    "- **gamma**, **lambda**, and **alpha**: have an effect on how strongly regularized the trained model will be.\n",
    ">From chapter 2:\n",
    "    * gamma (tree base learners' parameter): controls whether a given node on a base learner will split based on the expected reduction in the loss function. Higher values lead to fewer splits.\n",
    "    * alpha (L1 regularization): is a penaltty on leaf weights, rather on feature weights (as in the case in linear or logistic regression). Higher values lead to stronger regularization, causing many leaf weights in the base learners to go to 0.\n",
    "    * lambda (L2 regularization): causes leaf weights to smoothly decrease, instead of enforcing strong sparsity constraints on the leaf weights as in L1.\n",
    "\n",
    "- **max_depth**: must be a positive integer value, and affects how deeply each tree is allowed to grow any given boosting round.\n",
    "- **subsample**: must be a value between 0 and 1 and is the fraction of the total training set that can be used for any given boosting round. For low values, the fraction of your training data used, per boosting round, would be low and you may run into underfitting problems. High values can lead to overfitting as well.\n",
    "- **colsample_bytree**: is the fraction of features you can select from during any given boosting round and must also be a value between 0 and 1. Large value means that almost all features can be used to build a tree during a given boosting. Small values means that the fraction of features that can be selected from is very small. Smaller values can be thought of as providing additional regularization to the model, whereas using all columns may in certain cases overfit a trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear parameters\n",
    "\n",
    "- **alpha** (L1 regularization): is a penaltty on leaf weights, rather on feature weights (as in the case in linear or logistic regression). Higher values lead to stronger regularization, causing many leaf weights in the base learners to go to 0.\n",
    "- **lambda** (L2 regularization): causes leaf weights to smoothly decrease, instead of enforcing strong sparsity constraints on the leaf weights as in L1.\n",
    "- **lambda_bias**: L2 regularization on bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Tuning eta\n",
    "\n",
    "It's time to practice tuning other XGBoost hyperparameters in earnest and observing their effect on model performance! You'll begin by tuning the \"eta\", also known as the learning rate.\n",
    "\n",
    "The learning rate in XGBoost is a parameter that can range between 0 and 1, with higher values of \"eta\" penalizing feature weights more strongly, causing much stronger regularization.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "1. Create a list called eta_vals to store the following \"eta\" values: 0.001, 0.01, and 0.1.\n",
    "2. Iterate over your eta_vals list using a for loop.\n",
    "3. In each iteration of the for loop, set the \"eta\" key of params to be equal to curr_val. Then, perform 3-fold cross-validation with early stopping (5 rounds), 10 boosting rounds, a metric of \"rmse\", and a seed of 123. Ensure the output is a DataFrame.\n",
    "4. Append the final round RMSE to the best_rmse list.\n",
    "\n",
    "**Results**\n",
    "\n",
    "<font color=darkgreen>Great work!</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     eta      best_rmse\n",
      "0  0.001  195736.406250\n",
      "1  0.010  179932.187500\n",
      "2  0.100   79759.411459\n"
     ]
    }
   ],
   "source": [
    "# Create arrays for the features and the target: X, y\n",
    "X, y = housing_data.iloc[:,:-1],housing_data.iloc[:,-1]\n",
    "\n",
    "# Create your housing DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary for each tree (boosting round)\n",
    "params = {\"objective\":\"reg:squarederror\", \"max_depth\":3}\n",
    "\n",
    "# Create list of eta values and empty list to store final round rmse per xgboost model\n",
    "eta_vals = [0.001, 0.01, 0.1]\n",
    "best_rmse = []\n",
    "\n",
    "# Systematically vary the eta \n",
    "for curr_val in eta_vals:\n",
    "\n",
    "    params[\"eta\"] = curr_val\n",
    "    \n",
    "    # Perform cross-validation: cv_results\n",
    "    cv_results = xgb.cv(dtrain = housing_dmatrix,\n",
    "                        params = params,\n",
    "                        nfold = 3,\n",
    "                        early_stopping_rounds = 5,\n",
    "                        num_boost_round = 10,\n",
    "                        metrics = 'rmse',\n",
    "                        seed = SEED,\n",
    "                        as_pandas = True)    \n",
    "    \n",
    "    # Append the final round rmse to best_rmse\n",
    "    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "print(pd.DataFrame(list(zip(eta_vals, best_rmse)), columns=[\"eta\",\"best_rmse\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 Tuning max_depth\n",
    "\n",
    "In this exercise, your job is to tune max_depth, which is the parameter that dictates the maximum depth that each tree in a boosting round can grow to. Smaller values will lead to shallower trees, and larger values to deeper trees.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "1. Create a list called max_depths to store the following \"max_depth\" values: 2, 5, 10, and 20.\n",
    "2. Iterate over your max_depths list using a for loop.\n",
    "3. Systematically vary \"max_depth\" in each iteration of the for loop and perform 2-fold cross-validation with early stopping (5 rounds), 10 boosting rounds, a metric of \"rmse\", and a seed of 123. Ensure the output is a DataFrame.\n",
    "\n",
    "**Results**\n",
    "\n",
    "<font color=darkgreen>Great work!</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   max_depth     best_rmse\n",
      "0          2  37957.468750\n",
      "1          5  35596.599610\n",
      "2         10  36065.548829\n",
      "3         20  36739.578125\n"
     ]
    }
   ],
   "source": [
    "# Create the parameter dictionary\n",
    "params = {\"objective\":\"reg:squarederror\"}\n",
    "\n",
    "# Create list of max_depth values\n",
    "max_depths = [2, 5, 10, 20]\n",
    "best_rmse = []\n",
    "\n",
    "# Systematically vary the max_depth\n",
    "for curr_val in max_depths:\n",
    "\n",
    "    params[\"max_depth\"] = curr_val\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    cv_results = xgb.cv(dtrain = housing_dmatrix,\n",
    "                        params = params,\n",
    "                        nfold = 2,\n",
    "                        early_stopping_rounds = 5,\n",
    "                        num_boost_round = 10,\n",
    "                        metrics = 'rmse',\n",
    "                        seed = SEED,\n",
    "                        as_pandas = True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Append the final round rmse to best_rmse\n",
    "    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "print(pd.DataFrame(list(zip(max_depths, best_rmse)),columns=[\"max_depth\",\"best_rmse\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.8 Tuning colsample_bytree\n",
    "\n",
    "Now, it's time to tune \"colsample_bytree\". You've already seen this if you've ever worked with scikit-learn's RandomForestClassifier or RandomForestRegressor, where it just was called max_features. In both xgboost and sklearn, this parameter (although named differently) simply specifies the fraction of features to choose from at every split in a given tree. In xgboost, colsample_bytree must be specified as a float between 0 and 1.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "1. Create a list called colsample_bytree_vals to store the values 0.1, 0.5, 0.8, and 1.\n",
    "2. Systematically vary \"colsample_bytree\" and perform cross-validation, exactly as you did with max_depth and eta previously.\n",
    "\n",
    "**Results**\n",
    "\n",
    "<font color=darkgreen>Awesome! There are several other individual parameters that you can tune, such as \"subsample\", which dictates the fraction of the training data that is used during any given boosting round. Next up: Grid Search and Random Search to tune XGBoost hyperparameters more efficiently!</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   colsample_bytree     best_rmse\n",
      "0               0.1  40918.117188\n",
      "1               0.5  35813.906250\n",
      "2               0.8  35995.677735\n",
      "3               1.0  35836.042969\n"
     ]
    }
   ],
   "source": [
    "# Create the parameter dictionary\n",
    "params={\"objective\":\"reg:squarederror\",\"max_depth\":3}\n",
    "\n",
    "# Create list of hyperparameter values: colsample_bytree_vals\n",
    "colsample_bytree_vals = [0.1, 0.5, 0.8, 1]\n",
    "best_rmse = []\n",
    "\n",
    "# Systematically vary the hyperparameter value \n",
    "for curr_val in colsample_bytree_vals:\n",
    "\n",
    "    params['colsample_bytree'] = curr_val\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    cv_results = xgb.cv(dtrain=housing_dmatrix, \n",
    "                        params=params, \n",
    "                        nfold=2,\n",
    "                        num_boost_round=10, \n",
    "                        early_stopping_rounds=5,\n",
    "                        metrics=\"rmse\", \n",
    "                        as_pandas=True, \n",
    "                        seed=123)\n",
    "    \n",
    "    # Append the final round rmse to best_rmse\n",
    "    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "print(pd.DataFrame(list(zip(colsample_bytree_vals, best_rmse)), columns=[\"colsample_bytree\",\"best_rmse\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.9 Review of grid search and random search\n",
    "\n",
    "1. Review of grid search and random search\n",
    ">How do we find the optimal values for several hyperparameters simultaneously, leading to the lowest loss possible, when their values interact in in non-obvious, non-linear ways? Two common strategies for choosing several hyperparameter values simultaneously are Grid Search and Random Search, so it's important that we review them here, and see what their advantages and disadvantages are, by looking at some examples of how both can be used with the XGBoost and scikit-learn packages.\n",
    "\n",
    "2. Grid search: review\n",
    ">Grid Search is a method of exhaustively searching through a collection of possible parameter values. For example, if you have 2 hyperparameters you would like to tune, and 4 possible values for each hyperparameter, then a grid search over that parameter space would try all 16 possible parameter configurations. In a grid search, you try every parameter configuration, evaluate some metric for that configuration, and pick the parameter configuration that gave you the best value for the metric you were using, which in our case will be the root mean squared error.\n",
    "\n",
    "3. Grid search: example\n",
    ">Let's go over an example of how to grid search over several hyperparameters using XGBoost and scikit learn. In lines 1-4 we load in the necessary libraries, including GridSearchCV from sklearn dot model_selection. In lines 5-7 we load in our dataset and convert it into a DMatrix. In line 8 we create our grid of hyperparameters we want to search over. We selected 4 different learning rates (or eta values), 3 different subsample values, and a single number of trees. The total number of distinct hyperparameter configurations is 12, so 12 different models will be built. In line 9 we create our regressor, and then in line 10 we pass the xgbregressor object, parameter grid, evaluation metric, and number of cross validation folds to GridSearchCV and then immediately fit that gridsearch object in line 11, just like every other scikit learn estimator object we've done this to in the past. In line 12, having fit the gridsearch object, we can extract the best parameters the grid search found, and print them to the screen. In line 13, we get the RMSE that corresponds to the best parameters found, and see that it's ~28500 dollars.\n",
    "\n",
    "4. Random search: review\n",
    ">Random search is significantly different from grid search in that the number of models that you are required to iterate over doesn't grow as you expand the overall hyperparameter space. In random search, you get to decide how many models, or iterations, you want to try out before stopping. Random search simply involves drawing a random combination of possible hyperparameter values from the range of allowable hyperparameters a set number of times. Each time, you train a model with the selected hyperparameters, evaluate the performance of that model, and then rinse and repeat. When you've created the number of models you had specified initially, you simply pick the best one. To finish this lesson off,\n",
    "\n",
    "5. Random search: example\n",
    ">let's look at a full random search example. In lines 1-7, we load in the necessary modules, this time loading in RandomizedSearchCV from sklearn dot model_selection, and then load in and convert the data we need to a DMatrix object as always. In line 8 we create our parameter grid, this time generating a large number of learning rate values and subsample values using np-dot-arange. There are 20 values for learning_rate (or eta) and 20 values for subsample, which would be 400 models to try if we were to run a grid search (which we aren't doing here). In line 9 we create our xgbregressor object, and in line 10 we create our RandomizedSearchCV object, passing in the xgbregressor and parameter grid we had just created. We also set the number of iterations we want the random search to proceed to 25, so we know it will not be able to try all 400 possible parameter configurations. We also specify the evaluation metric we want to use, and that we want to run 4-fold cross-validation on each iteration. In line 11 we fit our randomizedsearchcv object, which can take a bit of time. Finally, lines 12 and 13 print the best model parameters found, and the corresponding best RMSE.\n",
    "\n",
    "6. Let's practice!\n",
    ">Ok, now let's have you practice both grid search and random search in the following exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 12 candidates, totalling 48 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  48 out of  48 | elapsed:   15.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'learning_rate': 0.1, 'n_estimators': 200, 'subsample': 0.5}\n",
      "Lowest RMSE found:  29105.179169382693\n"
     ]
    }
   ],
   "source": [
    "# Grid search: example\n",
    "X, y = housing_data.iloc[:,:-1],housing_data.iloc[:,-1]\n",
    "housing_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "\n",
    "gbm_param_grid = {'learning_rate': [0.01,0.1,0.5,0.9],\n",
    "                  'n_estimators': [200],\n",
    "                  'subsample': [0.3, 0.5, 0.9]}\n",
    "\n",
    "gbm = xgb.XGBRegressor()\n",
    "\n",
    "grid_mse = GridSearchCV(estimator = gbm,\n",
    "                        param_grid = gbm_param_grid,\n",
    "                        scoring = 'neg_mean_squared_error', \n",
    "                        cv = 4, \n",
    "                        verbose = 1)\n",
    "grid_mse.fit(X, y)\n",
    "\n",
    "print(\"Best parameters found: \",grid_mse.best_params_)\n",
    "print(\"Lowest RMSE found: \", np.sqrt(np.abs(grid_mse.best_score_)))             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 25 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:   38.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'subsample': 0.5, 'n_estimators': 200, 'learning_rate': 0.05}\n",
      "Lowest RMSE found:  28804.71293556027\n"
     ]
    }
   ],
   "source": [
    "# Random search: example\n",
    "gbm_param_grid = {'learning_rate': np.arange(0.05,1.05,.05),\n",
    "                  'n_estimators': [200],\n",
    "                  'subsample': np.arange(0.05,1.05,.05)}\n",
    "\n",
    "gbm = xgb.XGBRegressor()\n",
    "\n",
    "randomized_mse = RandomizedSearchCV(estimator = gbm, \n",
    "                                    param_distributions = gbm_param_grid,\n",
    "                                    n_iter = 25, \n",
    "                                    scoring = 'neg_mean_squared_error', \n",
    "                                    cv = 4, \n",
    "                                    random_state = SEED,\n",
    "                                    verbose = 1)\n",
    "\n",
    "randomized_mse.fit(X, y)\n",
    "\n",
    "print(\"Best parameters found: \",randomized_mse.best_params_)\n",
    "print(\"Lowest RMSE found: \", np.sqrt(np.abs(randomized_mse.best_score_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.10 Grid search with XGBoost\n",
    "\n",
    "Now that you've learned how to tune parameters individually with XGBoost, let's take your parameter tuning to the next level by using scikit-learn's GridSearch and RandomizedSearch capabilities with internal cross-validation using the GridSearchCV and RandomizedSearchCV functions. You will use these to find the best model exhaustively from a collection of possible parameter values across multiple parameters simultaneously. Let's get to work, starting with GridSearchCV!\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "1. Create a parameter grid called gbm_param_grid that contains a list of \"colsample_bytree\" values (0.3, 0.7), a list with a single value for \"n_estimators\" (50), and a list of 2 \"max_depth\" (2, 5) values.\n",
    "2. Instantiate an XGBRegressor object called gbm.\n",
    "3. Create a GridSearchCV object called grid_mse, passing in: the parameter grid to param_grid, the XGBRegressor to estimator, \"neg_mean_squared_error\" to scoring, and 4 to cv. Also specify verbose=1 so you can better understand the output.\n",
    "4. Fit the GridSearchCV object to X and y.\n",
    "5. Print the best parameter values and lowest RMSE, using the .best_params_ and .best_score_ attributes, respectively, of grid_mse.\n",
    "\n",
    "**Results**\n",
    "\n",
    "<font color=darkgreen>Excellent work! Next up, RandomizedSearchCV.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 4 candidates, totalling 16 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'colsample_bytree': 0.3, 'max_depth': 5, 'n_estimators': 50}\n",
      "Lowest RMSE found:  28986.18703093561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  16 out of  16 | elapsed:    0.9s finished\n"
     ]
    }
   ],
   "source": [
    "# Create the parameter grid: gbm_param_grid\n",
    "gbm_param_grid = {\n",
    "    'colsample_bytree': [0.3, 0.7],\n",
    "    'n_estimators': [50],\n",
    "    'max_depth': [2, 5]\n",
    "}\n",
    "\n",
    "# Instantiate the regressor: gbm\n",
    "gbm = xgb.XGBRegressor()\n",
    "\n",
    "# Perform grid search: grid_mse\n",
    "grid_mse = GridSearchCV(estimator = gbm,\n",
    "                        param_grid = gbm_param_grid,\n",
    "                        scoring = 'neg_mean_squared_error',\n",
    "                        cv = 4,\n",
    "                        verbose = 1)\n",
    "\n",
    "\n",
    "# Fit grid_mse to the data\n",
    "grid_mse.fit(X, y)\n",
    "\n",
    "# Print the best parameters and lowest RMSE\n",
    "print(\"Best parameters found: \", grid_mse.best_params_)\n",
    "print(\"Lowest RMSE found: \", np.sqrt(np.abs(grid_mse.best_score_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.11 Random search with XGBoost\n",
    "\n",
    "Often, GridSearchCV can be really time consuming, so in practice, you may want to use RandomizedSearchCV instead, as you will do in this exercise. The good news is you only have to make a few modifications to your GridSearchCV code to do RandomizedSearchCV. The key difference is you have to specify a param_distributions parameter instead of a param_grid parameter.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "1. Create a parameter grid called gbm_param_grid that contains a list with a single value for 'n_estimators' (25), and a list of 'max_depth' values between 2 and 11 for 'max_depth' - use range(2, 12) for this.\n",
    "2. Create a RandomizedSearchCV object called randomized_mse, passing in: the parameter grid to param_distributions, the XGBRegressor to estimator, \"neg_mean_squared_error\" to scoring, 5 to n_iter, and 4 to cv. Also specify verbose=1 so you can better understand the output.\n",
    "3. Fit the RandomizedSearchCV object to X and y.\n",
    "\n",
    "**Results**\n",
    "\n",
    "<font color=darkgreen>Superb!</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 5 candidates, totalling 20 fits\n",
      "Best parameters found:  {'n_estimators': 25, 'max_depth': 6}\n",
      "Lowest RMSE found:  31412.365221128253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:    1.6s finished\n"
     ]
    }
   ],
   "source": [
    "# Create the parameter grid: gbm_param_grid \n",
    "gbm_param_grid = {\n",
    "    'n_estimators': [25],\n",
    "    'max_depth': range(2, 12)\n",
    "}\n",
    "\n",
    "# Instantiate the regressor: gbm\n",
    "gbm = xgb.XGBRegressor(n_estimators=10)\n",
    "\n",
    "# Perform random search: grid_mse\n",
    "randomized_mse = RandomizedSearchCV(estimator = gbm,\n",
    "                                    param_distributions = gbm_param_grid,\n",
    "                                    n_iter = 5,\n",
    "                                    scoring = 'neg_mean_squared_error',\n",
    "                                    cv = 4,\n",
    "                                    random_state = SEED,\n",
    "                                    verbose = 1)\n",
    "\n",
    "# Fit randomized_mse to the data\n",
    "randomized_mse.fit(X, y)\n",
    "\n",
    "# Print the best parameters and lowest RMSE\n",
    "print(\"Best parameters found: \", randomized_mse.best_params_)\n",
    "print(\"Lowest RMSE found: \", np.sqrt(np.abs(randomized_mse.best_score_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.12 Limits of grid search and random search\n",
    "\n",
    "1. Limits of grid search and random search\n",
    ">Now that you've done both GridSearch and RandomSearch for hyperparameter tuning on the Ames housing data, let's briefly go over the limits of both of these approaches for hyperparameter tuning.\n",
    "\n",
    "2. Grid search and random search limitations\n",
    ">It should be clear to you that grid search and random search each suffer from distinct limitations. As long as the number of hyperparameters and distinct values per hyperparameter you search over is kept small, grid search will give you an answer in a reasonable amount of time. However, as the number of hyperparameters grows, the time it takes to complete a full grid search increases exponentially. For random search, the problem is a bit different. Since you can specify how many iterations a random search should be run, the time it takes to finish the random search wont explode as you add more and more hyperparameters to search through. The problem really is that as you add new hyperparameters to search over, the size of the hyperparameter space explodes as it did in the grid search case, and so you are left hoping that one of the random parameter configurations that the search chooses is a good one! You can always increase the number of iterations you want the random search to run, but then finding an optimal configuration becomes a combination of waiting randomly finding a good set of hyperparameters. In any case, both approaches have significant limitations.\n",
    "\n",
    "3. Let's practice!\n",
    ">Great, now that you've learned how to tune the most important hyperparameters found in XGBoost, lets move onto the final chapter, where we work through 2 end to end processing pipelines utilizing XGBoost and scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.13 When should you use grid search and random search?\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "Now that you've seen some of the drawbacks of grid search and random search, which of the following most accurately describes why both random search and grid search are non-ideal search hyperparameter tuning strategies in all scenarios?\n",
    "\n",
    "**Possible Answers**\n",
    "\n",
    "1. Grid Search and Random Search both take a very long time to perform, regardless of the number of parameters you want to tune. **Incorrect**. <font color=blue>Nope! Only grid-search search time grows exponentially as you add parameters, random-search does not have to grow in this way</font>.\n",
    "2. Grid Search and Random Search both scale exponentially in the number of hyperparameters you want to tune.\n",
    "3. <font color=red>The search space size can be massive for Grid Search in certain cases, whereas for Random Search the number of hyperparameters has a significant effect on how long it takes to run</font>. **Correct!**\n",
    "4. Grid Search and Random Search require that you have some idea of where the ideal values for hyperparameters reside. **Incorrect**. <font color=blue>Although this is true, it is true of all hyperparameter search strategies, not just grid and random search</font>.\n",
    "\n",
    "**Results**\n",
    "\n",
    "<font color=darkgreen>This is why random search and grid search should not always be used. Nice!</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aditional material\n",
    "\n",
    "- Datacamp course: https://learn.datacamp.com/courses/extreme-gradient-boosting-with-xgboost\n",
    "- Xgboost documentation: https://xgboost.readthedocs.io/en/latest/\n",
    "- sklearn.tree.DecisionTreeClassifier documentation: https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier\n",
    "- metrics: https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
