{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn_pandas import DataFrameMapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define global variables\n",
    "SEED = 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define global configuration\n",
    "np.random.seed(SEED) \n",
    "pd.set_option(\"display.max_columns\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "names = [\"crime\",\"zone\",\"industry\",\"charles\",\"no\",\"rooms\", \"age\", \"distance\",\"radial\",\"tax\",\"pupil\",\"aam\",\"lower\",\"med_price\"]\n",
    "boston_data = pd.read_csv(\"boston.csv\", names=names, skiprows=1)\n",
    "#print(boston_data.head())\n",
    "#print(boston_data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_unproc = pd.read_csv(\"ames_unprocessed_data.csv\")\n",
    "#print(housing_unproc.head())\n",
    "#print(housing_unproc.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_name = ['age', 'bp'   , 'sg' , 'al' , 'su'   , 'rbc', 'pc', 'pcc', 'ba' , 'bgr', \n",
    "                'bu' , 'sc'   , 'sod', 'pot', 'hemo' , 'pcv', 'wc' , 'rc', 'htn', 'dm' ,\n",
    "                'cad', 'appet', 'pe' , 'ane', 'class'] \n",
    "kideney = pd.read_csv(\"chronic_kidney_disease.csv\", names=columns_name, na_values='?')\n",
    "#print(kideney.head())\n",
    "#print(kideney.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Using XGBoost in pipelines\n",
    "\n",
    "Take your XGBoost skills to the next level by incorporating your models into two end-to-end machine learning pipelines. You'll learn how to tune the most important XGBoost hyperparameters efficiently within a pipeline, and get an introduction to some more advanced preprocessing techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Review of pipelines using sklearn\n",
    "\n",
    "1. Review of pipelines using sklearn\n",
    ">Let's begin the final chapter in this course by reviewing how pipelines are used in scikit-learn. Refreshing our memory about how pipelines work will allow us to use XGBoost effectively in pipelines going forward. Before working through an example script using pipelines, lets briefly go over how they work.\n",
    "\n",
    "2. Pipeline review\n",
    ">Pipelines in sklearn are objects that take a list of named tuples as input. The named tuples must always contain a string name as the first element in each tuple and any scikit-learn compatible transformer or estimator object as the second element. Each named tuple in the pipeline is called a step, and the list of transformations that are contained in the list are executed in order once some data is passed through the pipeline. This is done using the standard fit/predict paradigm that is standard in scikit-learn. Finally, where pipelines are really useful is that they can be used as input estimator objects into other scikit-learn objects themselves, the most useful of which are the cross_val_score method, which allows for efficient cross-validation and out of sample metric calculation, and the grid search and random search approaches for tuning hyperparameters.\n",
    "\n",
    "3. Scikit-learn pipeline example\n",
    ">Now that we've talked about how pipelines work, lets seem them in action. In this example, we will use the Boston Housing dataset. As you've seen many times before, we first import all of the functionality we will need for the example. We will use a randomforestregressor model to predict housing prices, and will import pipeline from sklearn's pipeline submodule. In lines 2-4, we load in our data and create our X feature matrix and y target vector. Lines 5-6 are the ones that do the real work here. In line 5, we create our pipeline, which contains a standardscaler transformer followed by our RandomForestRegressor estimator. Line 6 takes the just created pipeline estimator as an input along with our X matrix and y vector and performs 10-fold cross-validation using the pipeline and the data and outputs the neg_mean_squared_error as an evaluation metric once per fold. As a brief aside, neg_mean_squared_error is scikit-learn's API-specific way of calculating the mean squared error in an API-compatible way. Negative mean squared errors don't actualy exist as all squares must be positive when working with real numbers.\n",
    "\n",
    "4. Scikit-learn pipeline example\n",
    ">Thus, in lines 7 and 8 we simply take the absolute value of the scores, take each of their square roots, and compute their mean to get a root mean squared error across all 10 cross-validation folds. We can see that on average our prediction was off by about 4-point-5 units. In the following exercises, because we will be working with the Ames housing dataset, which is more complex than the Boston housing dataset,\n",
    "\n",
    "5. Preprocessing I: LabelEncoder and OneHotEncoder\n",
    ">some additional preprocessing steps will be required. Specifically, we will do the same preprocessing steps in two different ways, only one of which can be done within a pipeline. The first approach involves using the LabelEncoder and OneHotEncoder classes of scikit-learn’s preprocessing submodule one after the other. LabelEncoder simply converts a categorical column of strings into integers that map onto those strings. OneHotEncoder takes a column of integers that are treated as categorical values, and encodes them as dummy variables, which you may already be familiar with. The problem with this 2-step method, however, is that it cannot currently be done within a pipeline. However, not all hope is lost. The second approach,\n",
    "\n",
    "6. Preprocessing II: DictVectorizer\n",
    ">which involves using a dict-vectorizer, can accomplish both steps in one line of code.The DictVectorizer is a class found in scikit-learn’s feature extraction submodule, and is traditionally used in text processing pipelines by converting lists of feature mappings into vectors. Using pandas DataFrames, we don’t initially have such a list, however, if we explicitly convert a DataFrame into a list of dictionary entries, then we have exactly what we need. For more details on these classes, I encourage you to explore the scikit-learn documentation.\n",
    "\n",
    "7. Let's build pipelines!\n",
    ">You will use both approaches in the next few exercises. I hope you have fun building pipelines!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final RMSE: 4.163273962502389\n"
     ]
    }
   ],
   "source": [
    "# Scikit-learn pipeline example\n",
    "#X, y = boston_data.iloc[:,:-1], boston_data.iloc[:,-1]\n",
    "X, y = boston_data.drop('med_price', axis=1), boston_data.med_price\n",
    "\n",
    "rf_pipeline = Pipeline([(\"st_scaler\", StandardScaler()), \n",
    "                        (\"rf_model\", RandomForestRegressor(random_state=SEED))])\n",
    "\n",
    "scores = cross_val_score(rf_pipeline,\n",
    "                         X, y, \n",
    "                         scoring=\"neg_mean_squared_error\", \n",
    "                         cv=10)\n",
    "\n",
    "final_avg_rmse = np.mean(np.sqrt(np.abs(scores)))\n",
    "print(\"Final RMSE:\", final_avg_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Exploratory data analysis\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "Before diving into the nitty gritty of pipelines and preprocessing, let's do some exploratory analysis of the original, unprocessed Ames housing dataset. When you worked with this data in previous chapters, we preprocessed it for you so you could focus on the core XGBoost concepts. In this chapter, you'll do the preprocessing yourself!\n",
    "\n",
    "A smaller version of this original, unprocessed dataset has been pre-loaded into a pandas DataFrame called df. Your task is to explore df in the Shell and pick the option that is incorrect. The larger purpose of this exercise is to understand the kinds of transformations you will need to perform in order to be able to use XGBoost.\n",
    "\n",
    "**Possible Answers**\n",
    "\n",
    "1. The DataFrame has 21 columns and 1460 rows.\n",
    "2. The mean of the LotArea column is 10516.828082.\n",
    "3. The DataFrame has missing values.\n",
    "4. <font color=red>The LotFrontage column has no missing values and its entries are of type float64</font>. **Correct!**\n",
    "5. The standard deviation of SalePrice is 79442.502883.\n",
    "\n",
    "**Results**\n",
    "\n",
    "<font color=darkgreen>Well done! The LotFrontage column actually does have missing values: 259, to be precise. Additionally, notice how columns such as MSZoning, PavedDrive, and HouseStyle are categorical. These need to be encoded numerically before you can use XGBoost. This is what you'll do in the coming exercises.</font><font color=darkgreen></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        MSSubClass  LotFrontage        LotArea  OverallQual  ...  BedroomAbvGr   Fireplaces   GarageArea      SalePrice\n",
      "count  1460.000000  1201.000000    1460.000000  1460.000000  ...   1460.000000  1460.000000  1460.000000    1460.000000\n",
      "mean     56.897260    70.049958   10516.828082     6.099315  ...      2.866438     0.613014   472.980137  180921.195890\n",
      "std      42.300571    24.284752    9981.264932     1.382997  ...      0.815778     0.644666   213.804841   79442.502883\n",
      "min      20.000000    21.000000    1300.000000     1.000000  ...      0.000000     0.000000     0.000000   34900.000000\n",
      "25%      20.000000    59.000000    7553.500000     5.000000  ...      2.000000     0.000000   334.500000  129975.000000\n",
      "50%      50.000000    69.000000    9478.500000     6.000000  ...      3.000000     1.000000   480.000000  163000.000000\n",
      "75%      70.000000    80.000000   11601.500000     7.000000  ...      3.000000     1.000000   576.000000  214000.000000\n",
      "max     190.000000   313.000000  215245.000000    10.000000  ...      8.000000     3.000000  1418.000000  755000.000000\n",
      "\n",
      "[8 rows x 16 columns]\n"
     ]
    }
   ],
   "source": [
    "#print(housing_unproc.head())\n",
    "#print(housing_unproc.info())\n",
    "print(housing_unproc.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Encoding categorical columns I: LabelEncoder\n",
    "\n",
    "Now that you've seen what will need to be done to get the housing data ready for XGBoost, let's go through the process step-by-step.\n",
    "\n",
    "First, you will need to fill in missing values - as you saw previously, the column LotFrontage has many missing values. Then, you will need to encode any categorical columns in the dataset using one-hot encoding so that they are encoded numerically. You can watch this video from Supervised Learning with scikit-learn for a refresher on the idea (https://campus.datacamp.com/courses/supervised-learning-with-scikit-learn/preprocessing-and-pipelines?ex=1).\n",
    "\n",
    "The data has five categorical columns: MSZoning, PavedDrive, Neighborhood, BldgType, and HouseStyle. Scikit-learn has a LabelEncoder function that converts the values in each categorical column into integers. You'll practice using this here.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "1. Import LabelEncoder from sklearn.preprocessing.\n",
    "2. Fill in missing values in the LotFrontage column with 0 using .fillna().\n",
    "3. Create a boolean mask for categorical columns. You can do this by checking for whether df.dtypes equals object.\n",
    "4. Create a LabelEncoder object. You can do this in the same way you instantiate any scikit-learn estimator.\n",
    "5. Encode all of the categorical columns into integers using LabelEncoder(). To do this, use the .fit_transform() method of le in the provided lambda function.\n",
    "\n",
    "**Results**\n",
    "\n",
    "<font color=darkgreen>Well done! Notice how the entries in each categorical column are now encoded numerically. A BldgTpe of 1Fam is encoded as 0, while a HouseStyle of 2Story is encoded as 5.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values with 0\n",
    "housing_unproc.LotFrontage = housing_unproc.LotFrontage.fillna(0)\n",
    "\n",
    "# Copy the dataframe to work with\n",
    "df_sk = housing_unproc.copy(deep = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSZoning         5\n",
      "Neighborhood    25\n",
      "BldgType         5\n",
      "HouseStyle       8\n",
      "PavedDrive       3\n",
      "dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a boolean mask for categorical columns\n",
    "categorical_mask = (df_sk.dtypes == object)\n",
    "\n",
    "# Get list of categorical column names\n",
    "categorical_columns = df_sk.columns[categorical_mask].tolist()\n",
    "\n",
    "# Print count distinct values in each categorical columns\n",
    "print(df_sk[categorical_columns].nunique(axis=0),'\\n')\n",
    "\n",
    "# Print the head of the categorical columns\n",
    "#print(df_sk[categorical_columns].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  MSZoning Neighborhood BldgType HouseStyle PavedDrive\n",
      "0       RL      CollgCr     1Fam     2Story          Y\n",
      "1       RL      Veenker     1Fam     1Story          Y\n",
      "2       RL      CollgCr     1Fam     2Story          Y\n",
      "3       RL      Crawfor     1Fam     2Story          Y\n",
      "4       RL      NoRidge     1Fam     2Story          Y\n",
      "   MSZoning  Neighborhood  BldgType  HouseStyle  PavedDrive\n",
      "0         3             5         0           5           2\n",
      "1         3            24         0           2           2\n",
      "2         3             5         0           5           2\n",
      "3         3             6         0           5           2\n",
      "4         3            15         0           5           2\n"
     ]
    }
   ],
   "source": [
    "# Print the head of unlabeled categorical columns\n",
    "print(df_sk[categorical_columns].head())\n",
    "\n",
    "# Create LabelEncoder object: le\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Apply LabelEncoder to categorical columns\n",
    "df_sk[categorical_columns] = df_sk[categorical_columns].apply(lambda x: le.fit_transform(x))\n",
    "\n",
    "# Print the head of the LabelEncoded categorical columns\n",
    "print(df_sk[categorical_columns].head())\n",
    "\n",
    "# Exploring a little bit more\n",
    "#print('\\nData in Neighborhood:')\n",
    "#print('Before encoding: ', housing_unproc['Neighborhood'].unique())\n",
    "#print('After  encoding: ', df_sk['Neighborhood'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Encoding categorical columns II: OneHotEncoder\n",
    "\n",
    "Okay - so you have your categorical columns encoded numerically. Can you now move onto using pipelines and XGBoost? Not yet! In the categorical columns of this dataset, there is no natural ordering between the entries. As an example: Using LabelEncoder, the CollgCr Neighborhood was encoded as 5, while the Veenker Neighborhood was encoded as 24, and Crawfor as 6. Is Veenker \"greater\" than Crawfor and CollgCr? No - and allowing the model to assume this natural ordering may result in poor performance.\n",
    "\n",
    "As a result, there is another step needed: You have to apply a one-hot encoding to create binary, or \"dummy\" variables. You can do this using scikit-learn's OneHotEncoder.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "1. Import OneHotEncoder from sklearn.preprocessing.\n",
    "2. Instantiate a OneHotEncoder object called ohe. Specify the keyword arguments categorical_features=categorical_mask and sparse=False.\n",
    "3. Using its .fit_transform() method, apply the OneHotEncoder to df and save the result as df_encoded. The output will be a NumPy array.\n",
    "4. Print the first 5 rows of df_encoded, and then the shape of df as well as df_encoded to compare the difference.\n",
    "\n",
    "**Results**\n",
    "\n",
    "<font color=darkgreen>Superb! As you can see, after one hot encoding, which creates binary variables out of the categorical variables, there are now 62 columns.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using <font color=red>sklearn.preprocessing.OneHotEncoder</font> to encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ColumnTransformer] . (1 of 2) Processing OneHotEncoder, total=   0.0s\n",
      "[ColumnTransformer] ..... (2 of 2) Processing remainder, total=   0.0s\n",
      "Before OneHotEncoder: (1460, 21)\n",
      "After OneHotEncoder: (1460, 62)\n",
      "\n",
      "First 2 rows of the resulting dataset:\n",
      "[[0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 1.000e+00 6.000e+01 6.500e+01 8.450e+03\n",
      "  7.000e+00 5.000e+00 2.003e+03 0.000e+00 1.710e+03 1.000e+00 0.000e+00\n",
      "  2.000e+00 1.000e+00 3.000e+00 0.000e+00 5.480e+02 2.085e+05]\n",
      " [0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 1.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 1.000e+00 2.000e+01 8.000e+01 9.600e+03\n",
      "  6.000e+00 8.000e+00 1.976e+03 0.000e+00 1.262e+03 0.000e+00 1.000e+00\n",
      "  2.000e+00 0.000e+00 3.000e+00 1.000e+00 4.600e+02 1.815e+05]] \n",
      "\n",
      "   OneHotEncoder__x0_0  OneHotEncoder__x0_1  OneHotEncoder__x0_2  ...  Fireplaces  GarageArea  SalePrice\n",
      "0                  0.0                  0.0                  0.0  ...         0.0       548.0   208500.0\n",
      "1                  0.0                  0.0                  0.0  ...         1.0       460.0   181500.0\n",
      "2                  0.0                  0.0                  0.0  ...         1.0       608.0   223500.0\n",
      "3                  0.0                  0.0                  0.0  ...         1.0       642.0   140000.0\n",
      "4                  0.0                  0.0                  0.0  ...         1.0       836.0   250000.0\n",
      "\n",
      "[5 rows x 62 columns]\n",
      "['OneHotEncoder__x0_0', 'OneHotEncoder__x0_1', 'OneHotEncoder__x0_2', 'OneHotEncoder__x0_3', 'OneHotEncoder__x0_4', 'OneHotEncoder__x1_0', 'OneHotEncoder__x1_1', 'OneHotEncoder__x1_2', 'OneHotEncoder__x1_3', 'OneHotEncoder__x1_4', 'OneHotEncoder__x1_5', 'OneHotEncoder__x1_6', 'OneHotEncoder__x1_7', 'OneHotEncoder__x1_8', 'OneHotEncoder__x1_9', 'OneHotEncoder__x1_10', 'OneHotEncoder__x1_11', 'OneHotEncoder__x1_12', 'OneHotEncoder__x1_13', 'OneHotEncoder__x1_14', 'OneHotEncoder__x1_15', 'OneHotEncoder__x1_16', 'OneHotEncoder__x1_17', 'OneHotEncoder__x1_18', 'OneHotEncoder__x1_19', 'OneHotEncoder__x1_20', 'OneHotEncoder__x1_21', 'OneHotEncoder__x1_22', 'OneHotEncoder__x1_23', 'OneHotEncoder__x1_24', 'OneHotEncoder__x2_0', 'OneHotEncoder__x2_1', 'OneHotEncoder__x2_2', 'OneHotEncoder__x2_3', 'OneHotEncoder__x2_4', 'OneHotEncoder__x3_0', 'OneHotEncoder__x3_1', 'OneHotEncoder__x3_2', 'OneHotEncoder__x3_3', 'OneHotEncoder__x3_4', 'OneHotEncoder__x3_5', 'OneHotEncoder__x3_6', 'OneHotEncoder__x3_7', 'OneHotEncoder__x4_0', 'OneHotEncoder__x4_1', 'OneHotEncoder__x4_2', 'MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', 'Remodeled', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'Fireplaces', 'GarageArea', 'SalePrice']\n"
     ]
    }
   ],
   "source": [
    "# Print the list of categorical column names\n",
    "#print(df_sk.head(1))\n",
    "#print(categorical_mask)\n",
    "#print(df_sk.columns[categorical_mask].tolist(),'\\n')\n",
    "\n",
    "############################################################\n",
    "## Without drop='first' and after labeling\n",
    "############################################################\n",
    "# Create OneHotEncoder: ohe\n",
    "ohe = ColumnTransformer([(\"OneHotEncoder\", OneHotEncoder(), categorical_mask)], \n",
    "                        remainder = 'passthrough',\n",
    "                        sparse_threshold = 0,\n",
    "                        verbose = True)\n",
    "\n",
    "# Apply OneHotEncoder to categorical columns - output is no longer a dataframe: df_encoded\n",
    "housing_encoded = ohe.fit_transform(df_sk)\n",
    "\n",
    "# Print the shape of the original DataFrame\n",
    "print(\"Before OneHotEncoder:\", df_sk.shape)\n",
    "\n",
    "# Print the shape of the transformed array\n",
    "print(\"After OneHotEncoder:\", housing_encoded.shape)\n",
    "\n",
    "# Print first 5 rows of the resulting dataset - again, this will no longer be a pandas dataframe\n",
    "print('\\nFirst 2 rows of the resulting dataset:')\n",
    "print(housing_encoded[:2, :], '\\n')\n",
    "\n",
    "# Transforming to df\n",
    "df_data = pd.DataFrame(data=housing_encoded, columns=ohe.get_feature_names())\n",
    "print(df_data.head())\n",
    "print(ohe.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ColumnTransformer] . (1 of 2) Processing OneHotEncoder, total=   0.0s\n",
      "[ColumnTransformer] ..... (2 of 2) Processing remainder, total=   0.0s\n",
      "Before OneHotEncoder: (1460, 21)\n",
      "After OneHotEncoder: (1460, 57)\n",
      "\n",
      "First 2 rows of the resulting dataset:\n",
      "[[0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00 6.000e+01\n",
      "  6.500e+01 8.450e+03 7.000e+00 5.000e+00 2.003e+03 0.000e+00 1.710e+03\n",
      "  1.000e+00 0.000e+00 2.000e+00 1.000e+00 3.000e+00 0.000e+00 5.480e+02\n",
      "  2.085e+05]\n",
      " [0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00 2.000e+01\n",
      "  8.000e+01 9.600e+03 6.000e+00 8.000e+00 1.976e+03 0.000e+00 1.262e+03\n",
      "  0.000e+00 1.000e+00 2.000e+00 0.000e+00 3.000e+00 1.000e+00 4.600e+02\n",
      "  1.815e+05]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "############################################################\n",
    "## With drop='first' and not labeling\n",
    "############################################################\n",
    "# Create OneHotEncoder: ohe\n",
    "ohe = ColumnTransformer([(\"OneHotEncoder\", OneHotEncoder(drop='first'), categorical_mask)], \n",
    "                        remainder = 'passthrough',\n",
    "                        sparse_threshold = 0,\n",
    "                        verbose = True)\n",
    "\n",
    "# Apply OneHotEncoder to categorical columns - output is no longer a dataframe: df_encoded\n",
    "housing_encoded_drop = ohe.fit_transform(housing_unproc)\n",
    "\n",
    "# Print the shape of the original DataFrame\n",
    "print(\"Before OneHotEncoder:\", df_sk.shape)\n",
    "\n",
    "# Print the shape of the transformed array\n",
    "print(\"After OneHotEncoder:\", housing_encoded_drop.shape)\n",
    "\n",
    "# Print first 5 rows of the resulting dataset - again, this will no longer be a pandas dataframe\n",
    "print('\\nFirst 2 rows of the resulting dataset:')\n",
    "print(housing_encoded_drop[:2, :], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using <font color=red>pandas.get_dummies</font> to encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Encoded with Pandas Whithout drop parameters\n",
      "   MSSubClass  LotFrontage  LotArea  OverallQual  ...  HouseStyle_SLvl  PavedDrive_N  PavedDrive_P  PavedDrive_Y\n",
      "0          60         65.0     8450            7  ...                0             0             0             1\n",
      "1          20         80.0     9600            6  ...                0             0             0             1\n",
      "2          60         68.0    11250            7  ...                0             0             0             1\n",
      "3          70         60.0     9550            7  ...                0             0             0             1\n",
      "4          60         84.0    14260            8  ...                0             0             0             1\n",
      "\n",
      "[5 rows x 62 columns]\n",
      "Shape: (1460, 62)\n",
      "\n",
      "\n",
      "Encoded with Pandas Whit drop parameters\n",
      "   MSSubClass  LotFrontage  LotArea  OverallQual  ...  HouseStyle_SFoyer  HouseStyle_SLvl  PavedDrive_P  PavedDrive_Y\n",
      "0          60         65.0     8450            7  ...                  0                0             0             1\n",
      "1          20         80.0     9600            6  ...                  0                0             0             1\n",
      "2          60         68.0    11250            7  ...                  0                0             0             1\n",
      "3          70         60.0     9550            7  ...                  0                0             0             1\n",
      "4          60         84.0    14260            8  ...                  0                0             0             1\n",
      "\n",
      "[5 rows x 57 columns]\n",
      "Shape: (1460, 57)\n"
     ]
    }
   ],
   "source": [
    "#print(housing_unproc.head(1))\n",
    "#print(housing_unproc.shape)\n",
    "\n",
    "# Using pandas to encode categorical columns\n",
    "print('\\n\\nEncoded with Pandas Whithout drop parameters')\n",
    "df_pd = housing_unproc.copy(deep = True)\n",
    "df_pd = pd.get_dummies(df_pd)\n",
    "print(df_pd.head())\n",
    "print('Shape:', df_pd.shape)\n",
    "\n",
    "# Using pandas to encode categorical columns\n",
    "print('\\n\\nEncoded with Pandas Whit drop parameters')\n",
    "df_pd = housing_unproc.copy(deep = True)\n",
    "df_pd = pd.get_dummies(df_pd, drop_first=True)\n",
    "print(df_pd.head())\n",
    "print('Shape:', df_pd.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OneHotEncoder vs get_dummies**<br>\n",
    "<Font color=red>For machine learning, you almost definitely want to use sklearn.OneHotEncoder. For other tasks like simple analyses, you might be able to use pd.get_dummies, which is a bit more convenient.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Encoding categorical columns III: DictVectorizer\n",
    "\n",
    "Alright, one final trick before you dive into pipelines. The two step process you just went through - LabelEncoder followed by OneHotEncoder - can be simplified by using a DictVectorizer.\n",
    "\n",
    "Using a DictVectorizer on a DataFrame that has been converted to a dictionary allows you to get label encoding as well as one-hot encoding in one go.\n",
    "\n",
    "Your task is to work through this strategy in this exercise!\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "1. Import DictVectorizer from sklearn.feature_extraction.\n",
    "2. Convert df into a dictionary called df_dict using its .to_dict() method with \"records\" as the argument.\n",
    "3. Instantiate a DictVectorizer object called dv with the keyword argument sparse=False.\n",
    "4. Apply the DictVectorizer on df_dict by using its .fit_transform() method.\n",
    "5. Hit 'Submit Answer' to print the resulting first five rows and the vocabulary.\n",
    "\n",
    "**Results**\n",
    "\n",
    "<font color=darkgreen>Fantastic! Besides simplifying the process into one step, DictVectorizer has useful attributes such as vocabulary_ which maps the names of the features to their indices. With the data preprocessed, it's time to move onto pipelines!</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 2 rows of the resulting dataset:\n",
      "[[3.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00\n",
      "  0.000e+00 0.000e+00 2.000e+00 5.480e+02 1.710e+03 1.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00\n",
      "  8.450e+03 6.500e+01 6.000e+01 0.000e+00 0.000e+00 0.000e+00 1.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 5.000e+00 7.000e+00\n",
      "  0.000e+00 0.000e+00 1.000e+00 0.000e+00 2.085e+05 2.003e+03]\n",
      " [3.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  1.000e+00 1.000e+00 2.000e+00 4.600e+02 1.262e+03 0.000e+00 0.000e+00\n",
      "  0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  9.600e+03 8.000e+01 2.000e+01 0.000e+00 0.000e+00 0.000e+00 1.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00 8.000e+00 6.000e+00\n",
      "  0.000e+00 0.000e+00 1.000e+00 0.000e+00 1.815e+05 1.976e+03]] \n",
      "\n",
      "Vocabulary:\n",
      "{'MSSubClass': 23, 'MSZoning=RL': 27, 'LotFrontage': 22, 'LotArea': 21, 'Neighborhood=CollgCr': 34, 'BldgType=1Fam': 1, 'HouseStyle=2Story': 18, 'OverallQual': 55, 'OverallCond': 54, 'YearBuilt': 61, 'Remodeled': 59, 'GrLivArea': 11, 'BsmtFullBath': 6, 'BsmtHalfBath': 7, 'FullBath': 9, 'HalfBath': 12, 'BedroomAbvGr': 0, 'Fireplaces': 8, 'GarageArea': 10, 'PavedDrive=Y': 58, 'SalePrice': 60, 'Neighborhood=Veenker': 53, 'HouseStyle=1Story': 15, 'Neighborhood=Crawfor': 35, 'Neighborhood=NoRidge': 44, 'Neighborhood=Mitchel': 40, 'HouseStyle=1.5Fin': 13, 'Neighborhood=Somerst': 50, 'Neighborhood=NWAmes': 43, 'MSZoning=RM': 28, 'Neighborhood=OldTown': 46, 'Neighborhood=BrkSide': 32, 'BldgType=2fmCon': 2, 'HouseStyle=1.5Unf': 14, 'Neighborhood=Sawyer': 48, 'Neighborhood=NridgHt': 45, 'Neighborhood=NAmes': 41, 'BldgType=Duplex': 3, 'Neighborhood=SawyerW': 49, 'Neighborhood=IDOTRR': 38, 'PavedDrive=N': 56, 'Neighborhood=MeadowV': 39, 'BldgType=TwnhsE': 5, 'MSZoning=C (all)': 24, 'Neighborhood=Edwards': 36, 'Neighborhood=Timber': 52, 'PavedDrive=P': 57, 'HouseStyle=SFoyer': 19, 'MSZoning=FV': 25, 'Neighborhood=Gilbert': 37, 'HouseStyle=SLvl': 20, 'BldgType=Twnhs': 4, 'Neighborhood=StoneBr': 51, 'HouseStyle=2.5Unf': 17, 'Neighborhood=ClearCr': 33, 'Neighborhood=NPkVill': 42, 'HouseStyle=2.5Fin': 16, 'Neighborhood=Blmngtn': 29, 'Neighborhood=BrDale': 31, 'Neighborhood=SWISU': 47, 'MSZoning=RH': 26, 'Neighborhood=Blueste': 30}\n",
      "\n",
      "Transforming to dataframe:\n",
      "   BedroomAbvGr  BldgType=1Fam  BldgType=2fmCon  BldgType=Duplex  ...  PavedDrive=Y  Remodeled  SalePrice  YearBuilt\n",
      "0           3.0            1.0              0.0              0.0  ...           1.0        0.0   208500.0     2003.0\n",
      "1           3.0            1.0              0.0              0.0  ...           1.0        0.0   181500.0     1976.0\n",
      "2           3.0            1.0              0.0              0.0  ...           1.0        1.0   223500.0     2001.0\n",
      "3           3.0            1.0              0.0              0.0  ...           1.0        1.0   140000.0     1915.0\n",
      "4           4.0            1.0              0.0              0.0  ...           1.0        0.0   250000.0     2000.0\n",
      "\n",
      "[5 rows x 62 columns]\n"
     ]
    }
   ],
   "source": [
    "# Convert df into a dictionary: df_dict\n",
    "df_dict = housing_unproc.to_dict('records')\n",
    "\n",
    "# Create the DictVectorizer object: dv\n",
    "dv = DictVectorizer(sparse = False)\n",
    "\n",
    "# Apply dv on df: df_encoded\n",
    "df_encoded = dv.fit_transform(df_dict)\n",
    "\n",
    "# Print the resulting first two rows\n",
    "print('First 2 rows of the resulting dataset:')\n",
    "print(df_encoded[:2,:], '\\n')\n",
    "\n",
    "# Print the vocabulary\n",
    "print('Vocabulary:')\n",
    "print(dv.vocabulary_)\n",
    "\n",
    "# Transforming to df\n",
    "print('\\nTransforming to dataframe:')\n",
    "df_data = pd.DataFrame(data=df_encoded, columns=dv.feature_names_)\n",
    "print(df_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'BedroomAbvGr': 0, 'BldgType=1Fam': 1, 'BldgType=2fmCon': 2, 'BldgType=Duplex': 3, 'BldgType=Twnhs': 4, 'BldgType=TwnhsE': 5, 'BsmtFullBath': 6, 'BsmtHalfBath': 7, 'Fireplaces': 8, 'FullBath': 9, 'GarageArea': 10, 'GrLivArea': 11, 'HalfBath': 12, 'HouseStyle=1.5Fin': 13, 'HouseStyle=1.5Unf': 14, 'HouseStyle=1Story': 15, 'HouseStyle=2.5Fin': 16, 'HouseStyle=2.5Unf': 17, 'HouseStyle=2Story': 18, 'HouseStyle=SFoyer': 19, 'HouseStyle=SLvl': 20, 'LotArea': 21, 'LotFrontage': 22, 'MSSubClass': 23, 'MSZoning=C (all)': 24, 'MSZoning=FV': 25, 'MSZoning=RH': 26, 'MSZoning=RL': 27, 'MSZoning=RM': 28, 'Neighborhood=Blmngtn': 29, 'Neighborhood=Blueste': 30, 'Neighborhood=BrDale': 31, 'Neighborhood=BrkSide': 32, 'Neighborhood=ClearCr': 33, 'Neighborhood=CollgCr': 34, 'Neighborhood=Crawfor': 35, 'Neighborhood=Edwards': 36, 'Neighborhood=Gilbert': 37, 'Neighborhood=IDOTRR': 38, 'Neighborhood=MeadowV': 39, 'Neighborhood=Mitchel': 40, 'Neighborhood=NAmes': 41, 'Neighborhood=NPkVill': 42, 'Neighborhood=NWAmes': 43, 'Neighborhood=NoRidge': 44, 'Neighborhood=NridgHt': 45, 'Neighborhood=OldTown': 46, 'Neighborhood=SWISU': 47, 'Neighborhood=Sawyer': 48, 'Neighborhood=SawyerW': 49, 'Neighborhood=Somerst': 50, 'Neighborhood=StoneBr': 51, 'Neighborhood=Timber': 52, 'Neighborhood=Veenker': 53, 'OverallCond': 54, 'OverallQual': 55, 'PavedDrive=N': 56, 'PavedDrive=P': 57, 'PavedDrive=Y': 58, 'Remodeled': 59, 'SalePrice': 60, 'YearBuilt': 61}\n"
     ]
    }
   ],
   "source": [
    "cols = dict(sorted(dv.vocabulary_.items(), key=lambda w: w[1]))\n",
    "print(cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 Preprocessing within a pipeline\n",
    "\n",
    "Now that you've seen what steps need to be taken individually to properly process the Ames housing data, let's use the much cleaner and more succinct DictVectorizer approach and put it alongside an XGBoostRegressor inside of a scikit-learn pipeline.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "1. Import DictVectorizer from sklearn.feature_extraction and Pipeline from sklearn.pipeline.\n",
    "2. Fill in any missing values in the LotFrontage column of X with 0.\n",
    "3. Complete the steps of the pipeline with DictVectorizer(sparse=False) for \"ohe_onestep\" and xgb.XGBRegressor() for \"xgb_model\".\n",
    "4. Create the pipeline using Pipeline() and steps.\n",
    "4. Fit the Pipeline. Don't forget to convert X into a format that DictVectorizer understands by calling the to_dict(\"records\") method on X.\n",
    "\n",
    "**Results**\n",
    "\n",
    "<font color=darkgreen>Well done! It's now time to see what it takes to use XGBoost within pipelines.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 28278.025825\n"
     ]
    }
   ],
   "source": [
    "# Create arrays for the features and the target: X, y\n",
    "#X, y = housing_unproc.iloc[:,:-1], housing_unproc.iloc[:,-1]\n",
    "X, y = housing_unproc.drop('SalePrice', axis=1), housing_unproc.SalePrice\n",
    "\n",
    "# Fill LotFrontage missing values with 0\n",
    "X.LotFrontage = X.LotFrontage.fillna(0)\n",
    "\n",
    "# Create the training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)\n",
    "\n",
    "# Setup the pipeline steps: steps\n",
    "steps = [(\"ohe_onestep\", DictVectorizer(sparse = False)),\n",
    "         (\"xgb_model\", xgb.XGBRegressor(seed = SEED))]\n",
    "\n",
    "# Create the pipeline: xgb_pipeline\n",
    "xgb_pipeline = Pipeline(steps)\n",
    "\n",
    "# Fit the pipeline\n",
    "xgb_pipeline.fit(X_train.to_dict('records'), y_train)\n",
    "\n",
    "# Predict the labels of the test set: preds\n",
    "preds = xgb_pipeline.predict(X_test.to_dict('records'))\n",
    "\n",
    "# Compute the rmse: rmse\n",
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7 Incorporating XGBoost into pipelines\n",
    "\n",
    "1. Incorporating xgboost into pipelines\n",
    ">Now that you've had some practice using pipelines in scikit-learn, let's see what it takes to use xgboost within pipelines.\n",
    "\n",
    "2. Scikit-learn pipeline example with XGBoost\n",
    ">This example is very similar to what was shown in the pipeline review that began this chapter. To get XGBoost to work within a pipeline, all that's really required is that you use XGBoost's scikit-learn API within a pipeline object. Let's see what that looks like in practice. As always, we first import everything we need for our purposes. We then proceed to load in the dataset and parse it into the matrix of features X and target vector y. At this point lies the only difference between using a scikit-learn native machine learning model and XGBoost. Specifically, we simply pass in an instance of the XGBoost XGBRegressor object into the pipeline where a normal scikit-learn estimator would be. The rest of the script is exactly what you've seen in the past. You compute the cross-validated negative mse using 10-fold cross-validation and then convert the 10-fold negative MSE into an average RMSE across all 10 folds. As you can see, without any hyperparameter tuning, the XGBoost model had a lower RMSE, of ~4-point-03 units, than the randomforest model we started the chapter with, which had an RMSE around 4-point-5.\n",
    "\n",
    "3. Additional components introduced for pipelines\n",
    ">We wanted you to see how a simple case of pipelining with XGBoost works, however, in the final end-to-end example, we will take a dataset that involves significantly more wrangling before it can be used with XGBoost and put it through a pipeline as well. As a result, we will have to work with a library that is not part of the standard suite of scikit-learn tools, as well as work with parts of pipelines that you may not be familiar with. Sklearn_pandas is a separate library that attempts to bridge the gap between working with pandas and working with scikit-learn, as they don't always work seamlessly together. Specifically, sklearn_pandas has a generic class called DataFrameMapper, that allows for easy conversion between scikit-learn aware objects, or pure numpy arrays, and the DataFrames that are the bread and butter of the pandas library. Additionally, we will use a class called CategoricalImputer that will allow us to impute missing categorical values directly, without having to first convert them to integers, as is the requirement in scikit-learn. We will also use some uncommon aspects of scikit-learn to accomplish our goals. Specifically, we will use the Imputer class from scikit-learn's preprocessing submodule, that allows us to fill in missing numerical values, and the FeatureUnion class found in scikit-learn's pipeline submodule. The FeatureUnion class allows us to combine separate pipeline outputs into a single pipeline output, as for example, we would need to do if we had one set of preprocessing steps we needed to perform on the categorical features of a dataset and a distinct set of preprocessing steps on the numeric features found in a dataset. The point is, we will introduce these topics at once, but don't want you to feel overwhelmed about what they are doing and how they can be used properly.\n",
    "\n",
    "4. Let's practice!\n",
    ">Hopefully, you just saw that its not particularly difficult to incorporate XGBoost into pipelines. Now, its your turn to practice what you just learned!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final XGB RMSE: 4.37678209879882\n"
     ]
    }
   ],
   "source": [
    "# Scikit-learn pipeline example with XGBoost\n",
    "#X, y = boston_data.iloc[:,:-1], boston_data.iloc[:,-1]\n",
    "X, y = boston_data.drop('med_price', axis=1), boston_data.med_price\n",
    "\n",
    "# if you use Pipeline[] instead of Pipeline(), you get the error\n",
    "# TypeError: 'ABCMeta' object is not subscriptable\n",
    "xgb_pipeline = Pipeline([(\"st_scaler\", StandardScaler()),\n",
    "                         (\"xgb_model\",xgb.XGBRegressor(seed=SEED))])\n",
    "\n",
    "scores = cross_val_score(xgb_pipeline, \n",
    "                         X, y,\n",
    "                         scoring=\"neg_mean_squared_error\",\n",
    "                         cv = 10)\n",
    "\n",
    "final_avg_rmse = np.mean(np.sqrt(np.abs(scores)))\n",
    "\n",
    "print(\"Final XGB RMSE:\", final_avg_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.8 Cross-validating your XGBoost model\n",
    "\n",
    "In this exercise, you'll go one step further by using the pipeline you've created to preprocess and cross-validate your model.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "1. Create a pipeline called xgb_pipeline using steps.\n",
    "2. Perform 10-fold cross-validation using cross_val_score(). You'll have to pass in the pipeline, X (as a dictionary, using .to_dict(\"records\")), y, the number of folds you want to use, and scoring (\"neg_mean_squared_error\").\n",
    "3. Print the 10-fold RMSE.\n",
    "\n",
    "**Results**\n",
    "\n",
    "<font color=darkgreen>Great work!</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-fold RMSE:  27683.04157118635\n"
     ]
    }
   ],
   "source": [
    "# Create arrays for the features and the target: X, y\n",
    "#X, y = housing_unproc.iloc[:,:-1], housing_unproc.iloc[:,-1]\n",
    "X, y = housing_unproc.drop('SalePrice', axis=1), housing_unproc.SalePrice\n",
    "\n",
    "# Fill LotFrontage missing values with 0\n",
    "X.LotFrontage = X.LotFrontage.fillna(0)\n",
    "\n",
    "# Setup the pipeline steps: steps\n",
    "steps = [(\"ohe_onestep\", DictVectorizer(sparse=False)),\n",
    "         (\"xgb_model\", xgb.XGBRegressor(max_depth=2, objective=\"reg:squarederror\", seed=SEED))]\n",
    "\n",
    "# Create the pipeline: xgb_pipeline\n",
    "xgb_pipeline = Pipeline(steps)\n",
    "\n",
    "# Cross-validate the model\n",
    "cross_val_scores = cross_val_score(xgb_pipeline,\n",
    "                                   X.to_dict('records'), y,\n",
    "                                   cv = 10,\n",
    "                                   scoring = \"neg_mean_squared_error\")\n",
    "\n",
    "# Print the 10-fold RMSE\n",
    "print(\"10-fold RMSE: \", np.mean(np.sqrt(np.abs(cross_val_scores))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.9 Kidney disease case study I: Categorical Imputer\n",
    "\n",
    "You'll now continue your exploration of using pipelines with a dataset that requires significantly more wrangling. The <a href=https://archive.ics.uci.edu/ml/datasets/chronic_kidney_disease>chronic kidney disease dataset</a> contains both categorical and numeric features, but contains lots of missing values. The goal here is to predict who has chronic kidney disease given various blood indicators as features.\n",
    "\n",
    "As Sergey mentioned in the video, you'll be introduced to a new library, <a href=https://github.com/scikit-learn-contrib/sklearn-pandas>sklearn_pandas</a>, that allows you to chain many more processing steps inside of a pipeline than are currently supported in scikit-learn. Specifically, you'll be able to impute missing categorical values directly using the Categorical_Imputer() class in sklearn_pandas, and the DataFrameMapper() class to apply any arbitrary sklearn-compatible transformer on DataFrame columns, where the resulting output can be either a NumPy array or DataFrame.\n",
    "\n",
    "We've also created a transformer called a Dictifier that encapsulates converting a DataFrame using .to_dict(\"records\") without you having to do it explicitly (and so that it works in a pipeline). Finally, we've also provided the list of feature names in kidney_feature_names, the target name in kidney_target_name, the features in X, and the target in y.\n",
    "\n",
    "In this exercise, your task is to apply the CategoricalImputer to impute all of the categorical columns in the dataset. You can refer to how the numeric imputation mapper was created as a template. Notice the keyword arguments input_df=True and df_out=True? This is so that you can work with DataFrames instead of arrays. By default, the transformers are passed a numpy array of the selected columns as input, and as a result, the output of the DataFrame mapper is also an array. Scikit-learn transformers have historically been designed to work with numpy arrays, not pandas DataFrames, even though their basic indexing interfaces are similar.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "1. Apply the categorical imputer using DataFrameMapper() and CategoricalImputer(). CategoricalImputer() does not need any arguments to be passed in. The columns are contained in categorical_columns. Be sure to specify input_df=True and df_out=True, and use category_feature as your iterator variable in the list comprehension.\n",
    "\n",
    "**Results**\n",
    "\n",
    "<font color=darkgreen>Great work!</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exploring the data:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 400 entries, 0 to 399\n",
      "Data columns (total 25 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   age     391 non-null    float64\n",
      " 1   bp      388 non-null    float64\n",
      " 2   sg      353 non-null    float64\n",
      " 3   al      354 non-null    float64\n",
      " 4   su      351 non-null    float64\n",
      " 5   rbc     248 non-null    object \n",
      " 6   pc      335 non-null    object \n",
      " 7   pcc     396 non-null    object \n",
      " 8   ba      396 non-null    object \n",
      " 9   bgr     356 non-null    float64\n",
      " 10  bu      381 non-null    float64\n",
      " 11  sc      383 non-null    float64\n",
      " 12  sod     313 non-null    float64\n",
      " 13  pot     312 non-null    float64\n",
      " 14  hemo    348 non-null    float64\n",
      " 15  pcv     329 non-null    float64\n",
      " 16  wc      294 non-null    float64\n",
      " 17  rc      269 non-null    float64\n",
      " 18  htn     398 non-null    object \n",
      " 19  dm      398 non-null    object \n",
      " 20  cad     398 non-null    object \n",
      " 21  appet   399 non-null    object \n",
      " 22  pe      399 non-null    object \n",
      " 23  ane     399 non-null    object \n",
      " 24  class   400 non-null    object \n",
      "dtypes: float64(14), object(11)\n",
      "memory usage: 78.2+ KB\n",
      "None\n",
      "\n",
      "Nulls per column in kideney dataset: \n",
      "age        9\n",
      "bp        12\n",
      "sg        47\n",
      "al        46\n",
      "su        49\n",
      "rbc      152\n",
      "pc        65\n",
      "pcc        4\n",
      "ba         4\n",
      "bgr       44\n",
      "bu        19\n",
      "sc        17\n",
      "sod       87\n",
      "pot       88\n",
      "hemo      52\n",
      "pcv       71\n",
      "wc       106\n",
      "rc       131\n",
      "htn        2\n",
      "dm         2\n",
      "cad        2\n",
      "appet      1\n",
      "pe         1\n",
      "ane        1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Exploring the data\n",
    "print('Exploring the data:')\n",
    "print(kideney.info())\n",
    "\n",
    "# Create arrays for the features and the target: X, y\n",
    "df = kideney.copy(deep = True)\n",
    "X, y = df.drop('class', axis=1), df['class']\n",
    "\n",
    "# Apply LabelEncoder to target columns\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "# Check number of nulls in each feature column\n",
    "print('\\nNulls per column in kideney dataset: ')\n",
    "print(X.isnull().sum())\n",
    "\n",
    "# Create a boolean mask for categorical columns\n",
    "categorical_feature_mask = X.dtypes == object\n",
    "\n",
    "# Get list of categorical column names\n",
    "categorical_columns = X.columns[categorical_feature_mask].tolist()\n",
    "\n",
    "# Get list of non-categorical column names\n",
    "non_categorical_columns = X.columns[~categorical_feature_mask].tolist()\n",
    "\n",
    "# Apply numeric imputer\n",
    "numeric_imputation_mapper = DataFrameMapper(\n",
    "    [([numeric_feature], SimpleImputer(strategy=\"median\")) for numeric_feature in non_categorical_columns],\n",
    "    input_df=True, df_out=True\n",
    ")\n",
    "\n",
    "# Apply categorical imputer\n",
    "categorical_imputation_mapper = DataFrameMapper(\n",
    "    [(category_feature, SimpleImputer(strategy='most_frequent')) for category_feature in categorical_columns],\n",
    "    input_df=True, df_out=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.10 Kidney disease case study II: Feature Union\n",
    "\n",
    "Having separately imputed numeric as well as categorical columns, your task is now to use scikit-learn's <a href=http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html>FeatureUnion</a> to concatenate their results, which are contained in two separate transformer objects - numeric_imputation_mapper, and categorical_imputation_mapper, respectively.\n",
    "\n",
    "You may have already encountered FeatureUnion in <a href=https://campus.datacamp.com/courses/machine-learning-with-the-experts-school-budgets/improving-your-model?ex=7>Machine Learning with the Experts: School Budgets</a>. Just like with pipelines, you have to pass it a list of (string, transformer) tuples, where the first half of each tuple is the name of the transformer.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "1. Import FeatureUnion from sklearn.pipeline.\n",
    "2. Combine the results of numeric_imputation_mapper and categorical_imputation_mapper using FeatureUnion(), with the names \"num_mapper\" and \"cat_mapper\" respectively.\n",
    "\n",
    "**Results**\n",
    "\n",
    "<font color=darkgreen>Great work!</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the numeric and categorical transformations\n",
    "numeric_categorical_union = FeatureUnion([\n",
    "    (\"num_mapper\", numeric_imputation_mapper),\n",
    "    (\"cat_mapper\", categorical_imputation_mapper)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.11 Kidney disease case study III: Full pipeline\n",
    "\n",
    "It's time to piece together all of the transforms along with an XGBClassifier to build the full pipeline!\n",
    "\n",
    "Besides the numeric_categorical_union that you created in the previous exercise, there are two other transforms needed: the Dictifier() transform which we created for you, and the DictVectorizer().\n",
    "\n",
    "After creating the pipeline, your task is to cross-validate it to see how well it performs.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "1. Create the pipeline using the numeric_categorical_union, Dictifier(), and DictVectorizer(sort=False) transforms, and xgb.XGBClassifier() estimator with max_depth=3. Name the transforms \"featureunion\", \"dictifier\" \"vectorizer\", and the estimator \"clf\".\n",
    "2. Perform 3-fold cross-validation on the pipeline using cross_val_score(). Pass it the pipeline, pipeline, the features, kidney_data, the outcomes, y. Also set scoring to \"roc_auc\" and cv to 3.\n",
    "\n",
    "**Results**\n",
    "\n",
    "<font color=darkgreen>Great work!</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transformer to convert Pandas DataFrame into Dict (needed for DictVectorizer)\n",
    "class Dictifier(BaseEstimator, TransformerMixin):   \n",
    "    \"\"\"\n",
    "    Encapsulates converting a DataFrame using .to_dict(\"records\") without you having to do it explicitly \n",
    "    (and so that it works in a pipeline). \n",
    "    \"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-fold AUC:  0.9985589978963473\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list for Simple Imputer details\n",
    "transformers = []\n",
    "\n",
    "# Apply numeric imputer\n",
    "transformers.extend(\n",
    "    [([numeric_feature], [SimpleImputer(strategy=\"median\"), StandardScaler()]) for numeric_feature in non_categorical_columns]\n",
    ")\n",
    "\n",
    "# Apply categorical imputer\n",
    "transformers.extend(\n",
    "    [([category_feature], [SimpleImputer(strategy='most_frequent')]) for category_feature in categorical_columns]\n",
    ")\n",
    "# Combine the numeric and categorical transformations\n",
    "numeric_categorical_union = DataFrameMapper(transformers, input_df=True, df_out=True)\n",
    "\n",
    "# Create full pipeline\n",
    "pipeline = Pipeline([(\"featureunion\", numeric_categorical_union),\n",
    "                     (\"dictifier\", Dictifier()),\n",
    "                     (\"vectorizer\", DictVectorizer(sort=False)),\n",
    "                     (\"clf\", xgb.XGBClassifier(objective='binary:logistic', n_estimators=10, \n",
    "                                               use_label_encoder=False, eval_metric='error', \n",
    "                                               max_depth=3, seed=SEED))])\n",
    "\n",
    "# Perform cross-validation\n",
    "cross_val_scores = cross_val_score(pipeline, X, y, cv=3, scoring='roc_auc')\n",
    "\n",
    "# Print avg. AUC\n",
    "print(\"3-fold AUC: \", np.mean(cross_val_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.12 Tuning XGBoost hyperparameters\n",
    "\n",
    "1. Tuning xgboost hyperparameters in a pipeline\n",
    ">We are going to finish off this chapter, and the course, by seeing how automated hyperparameter tuning for an XGBoost model works within a scikit-learn pipeline. Once you have this down, you'll be able to make some of the most powerful well-tuned machine learning models today in an automated, reproducible manner.\n",
    "\n",
    "2. Tuning XGBoost hyperparameters in a pipeline\n",
    ">We will again use the Boston housing dataset to motivate our use of pipelines and hyperparameter tuning. As always, we first import what we will be using. The only difference is now we also import RandomizedSearchCV from the scikit-learn modelselection submodule. We then load in our data and create our feature matrix X and target vector y and also create our pipeline that includes both the standard scaling step and a base xgboostregressor object with all default parameters. At this point, you need to create the grid of parameters over which you will search. In order for the hyperparameters to be passed to the appropriate step, you have to name the parameters in the dictionary with the name of the step being referenced followed by 2 underscore signs and then the name of the hyperparameter you want to iterate over. Since the xgboost step is called xgb_model, all of our hyperparameter keys will start with xgboost_model__. In the example, we will tune subsample, max_depth, and colsample_bytree, and give each parameter a range of possible values. We then pass the pipeline in as an estimator to RandomizedSearchCV and the parameter grid to param_distributions. Everything else is as you've seen before, with appropriate scoring and cross-validation parameters passed in as well. Once that's done all you need to do is fit the randomizedsearch object and pass in the X and y objects we created earlier.\n",
    "\n",
    "3. Tuning XGBoost hyperparameters in a pipeline II\n",
    ">Finally, once you've fit the randomizedsearchcv object, you can inspect what the best score it found was, and convert it to an RMSE. You can also inspect what the best model found was and print it to screen.\n",
    "\n",
    "4. Let's finish this up!\n",
    ">Ok, last coding exercise of the course, let's finish this up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best rmse:  4.825683665057551\n",
      "Best model:  Pipeline(steps=[('st_scaler', StandardScaler()),\n",
      "                ('xgb_model',\n",
      "                 XGBRegressor(base_score=0.5, booster='gbtree',\n",
      "                              colsample_bylevel=1, colsample_bynode=1,\n",
      "                              colsample_bytree=0.9000000000000002, gamma=0,\n",
      "                              gpu_id=-1, importance_type='gain',\n",
      "                              interaction_constraints='',\n",
      "                              learning_rate=0.300000012, max_delta_step=0,\n",
      "                              max_depth=5, min_child_weight=1, missing=nan,\n",
      "                              monotone_constraints='()', n_estimators=100,\n",
      "                              n_jobs=8, num_parallel_tree=1, random_state=123,\n",
      "                              reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "                              seed=123, subsample=0.6500000000000001,\n",
      "                              tree_method='exact', validate_parameters=1,\n",
      "                              verbosity=None))])\n"
     ]
    }
   ],
   "source": [
    "# Scikit-learn pipeline example\n",
    "#X, y = boston_data.iloc[:,:-1], boston_data.iloc[:,-1]\n",
    "X, y = boston_data.drop('med_price', axis=1), boston_data.med_price\n",
    "\n",
    "# Tuning XGBoost hyperparameters in a pipeline\n",
    "xgb_pipeline = Pipeline([(\"st_scaler\", StandardScaler()), \n",
    "                         (\"xgb_model\",xgb.XGBRegressor(seed=SEED))])\n",
    "gbm_param_grid = {\n",
    "    'xgb_model__subsample': np.arange(.05, 1, .05),\n",
    "    'xgb_model__max_depth': np.arange(3,20,1),\n",
    "    'xgb_model__colsample_bytree': np.arange(.1,1.05,.05) \n",
    "}\n",
    "\n",
    "randomized_neg_mse = RandomizedSearchCV(\n",
    "    estimator=xgb_pipeline, \n",
    "    param_distributions=gbm_param_grid, \n",
    "    n_iter=10,\n",
    "    scoring='neg_mean_squared_error', \n",
    "    cv=4,\n",
    "    random_state=SEED\n",
    ")\n",
    "randomized_neg_mse.fit(X, y)\n",
    "\n",
    "print(\"Best rmse: \", np.sqrt(np.abs(randomized_neg_mse.best_score_)))\n",
    "print(\"Best model: \", randomized_neg_mse.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.13 Bringing it all together\n",
    "\n",
    "Alright, it's time to bring together everything you've learned so far! In this final exercise of the course, you will combine your work from the previous exercises into one end-to-end XGBoost pipeline to really cement your understanding of preprocessing and pipelines in XGBoost.\n",
    "\n",
    "Your work from the previous 3 exercises, where you preprocessed the data and set up your pipeline, has been pre-loaded. Your job is to perform a randomized search and identify the best hyperparameters.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "1. Set up the parameter grid to tune 'clf__learning_rate' (from 0.05 to 1 in increments of 0.05), 'clf__max_depth' (from 3 to 10 in increments of 1), and 'clf__n_estimators' (from 50 to 200 in increments of 50).\n",
    "2. Using your pipeline as the estimator, perform 2-fold RandomizedSearchCV with an n_iter of 2. Use \"roc_auc\" as the metric, and set verbose to 1 so the output is more detailed. Store the result in randomized_roc_auc.\n",
    "3. Fit randomized_roc_auc to X and y.\n",
    "4. Compute the best score and best estimator of randomized_roc_auc.\n",
    "\n",
    "**Results**\n",
    "\n",
    "<font color=darkgreen>Amazing work! This type of pipelining is very common in real-world data science and you're well on your way towards mastering it.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "## Step 1, ex.4.9\n",
    "###########################################################\n",
    "# Create arrays for the features and the target: X, y\n",
    "df = kideney.copy(deep = True)\n",
    "X, y = df.drop('class', axis=1), df['class']\n",
    "\n",
    "# Apply LabelEncoder to target columns\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "# Create a boolean mask for categorical columns\n",
    "categorical_feature_mask = X.dtypes == object\n",
    "\n",
    "# Get list of categorical column names\n",
    "categorical_columns = X.columns[categorical_feature_mask].tolist()\n",
    "\n",
    "# Get list of non-categorical column names\n",
    "non_categorical_columns = X.columns[~categorical_feature_mask].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "## Step 2, ex.4.11\n",
    "###########################################################\n",
    "# Initialize an empty list for Simple Imputer details\n",
    "transformers = []\n",
    "\n",
    "# Apply numeric imputer\n",
    "transformers.extend(\n",
    "    [([numeric_feature], [SimpleImputer(strategy=\"median\"), StandardScaler()]) for numeric_feature in non_categorical_columns]\n",
    ")\n",
    "\n",
    "# Apply categorical imputer\n",
    "transformers.extend(\n",
    "    [([category_feature], [SimpleImputer(strategy='most_frequent')]) for category_feature in categorical_columns]\n",
    ")\n",
    "\n",
    "# Combine the numeric and categorical transformations\n",
    "numeric_categorical_union = DataFrameMapper(transformers, input_df=True, df_out=True)\n",
    "\n",
    "# Create full pipeline\n",
    "pipeline = Pipeline([(\"featureunion\", numeric_categorical_union),\n",
    "                     (\"dictifier\", Dictifier()),\n",
    "                     (\"vectorizer\", DictVectorizer(sort=False)),\n",
    "                     (\"clf\", xgb.XGBClassifier(objective='binary:logistic', n_estimators=10, \n",
    "                                               use_label_encoder=False, eval_metric='error', \n",
    "                                               max_depth=3, seed=SEED))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9965333333333333\n",
      "Pipeline(steps=[('featureunion',\n",
      "                 DataFrameMapper(df_out=True, drop_cols=[],\n",
      "                                 features=[(['age'],\n",
      "                                            [SimpleImputer(strategy='median'),\n",
      "                                             StandardScaler()]),\n",
      "                                           (['bp'],\n",
      "                                            [SimpleImputer(strategy='median'),\n",
      "                                             StandardScaler()]),\n",
      "                                           (['sg'],\n",
      "                                            [SimpleImputer(strategy='median'),\n",
      "                                             StandardScaler()]),\n",
      "                                           (['al'],\n",
      "                                            [SimpleImputer(strategy='median'),\n",
      "                                             StandardScaler()]),\n",
      "                                           (['su'],\n",
      "                                            [SimpleImputer...\n",
      "                               interaction_constraints='',\n",
      "                               learning_rate=0.9500000000000001,\n",
      "                               max_delta_step=0, max_depth=4,\n",
      "                               min_child_weight=1, missing=nan,\n",
      "                               monotone_constraints='()', n_estimators=100,\n",
      "                               n_jobs=8, num_parallel_tree=1, random_state=123,\n",
      "                               reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "                               seed=123, subsample=1, tree_method='exact',\n",
      "                               use_label_encoder=False, validate_parameters=1,\n",
      "                               verbosity=None))])\n"
     ]
    }
   ],
   "source": [
    "###########################################################\n",
    "## Step 3, ex.4.13\n",
    "###########################################################\n",
    "# Create the parameter grid\n",
    "gbm_param_grid = {\n",
    "    'clf__learning_rate': np.arange(0.05, 1, 0.05),\n",
    "    'clf__max_depth'    : np.arange(3, 10, 1),\n",
    "    'clf__n_estimators' : np.arange(50, 200, 50)\n",
    "}\n",
    "\n",
    "# Perform RandomizedSearchCV\n",
    "randomized_roc_auc = RandomizedSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_distributions=gbm_param_grid,\n",
    "    n_iter=2,\n",
    "    cv=2,\n",
    "    scoring='roc_auc',\n",
    "    verbose=1,\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "# Fit the estimator\n",
    "randomized_roc_auc.fit(X, y)\n",
    "\n",
    "# Compute metrics\n",
    "print(randomized_roc_auc.best_score_)\n",
    "print(randomized_roc_auc.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.14 Final Thoughts\n",
    "\n",
    "1. Final Thoughts\n",
    ">Congratulations on completing this course. Let's go over everything we've covered in this course, as well as where you can go from here with learning other topics related to XGBoost that we didn't have a chance to cover.\n",
    "\n",
    "2. What We Have Covered And You Have Learned\n",
    ">So, what have we been able to cover in this course? Well, we've learned how to use XGBoost for both classification and regression tasks. We've also covered all the most important hyperparameters that you should tune when creating XGBoost models, so that they are as performant as possible. And we just finished up how to incorporate XGBoost into pipelines, and used some more advanced functions that allow us to seamlessly work with Pandas DataFrames and scikit-learn. That's quite a lot of ground we've covered and you should be proud of what you've been able to accomplish.\n",
    "\n",
    "3. What We Have Not Covered (And How You Can Proceed)\n",
    ">However, although we've covered quite a lot, we didn't cover some other topics that would advance your mastery of XGBoost. Specifically, we never looked into how to use XGBoost for ranking or recommendation problems, which can be done by modifying the loss function you use when constructing your model. We also didn't look into more advanced hyperparameter selection strategies. The most powerful strategy, called Bayesian optimization, has been used with lots of success, and entire companies have been created just for specifically using this method in tuning models (for example, the company sigopt does exactly this). It's a powerful method, but would take an entire other DataCamp course to teach properly! Finally, we haven't talked about ensembling XGBoost with other models. Although XGBoost is itself an ensemble method, nothing stops you from combining the predictions you get from an XGBoost model with other models, as this is usually a very powerful additional way to squeeze the last bit of juice from your data. Learning about all of these additional topics will help you become an even more powerful user of XGBoost. Now that you know your way around the package, there's no reason for you to stop learning how to get even more benefits out of it.\n",
    "\n",
    "4. Congratulations!\n",
    ">I hope you've enjoyed taking this course on XGBoost as I have teaching it. Please let us know if you've enjoyed the course and definitely let me know how I can improve it. It's been a pleasure, and I hope you continue your data science journey from here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aditional material\n",
    "- Datacamp course: https://learn.datacamp.com/courses/extreme-gradient-boosting-with-xgboost\n",
    "- Xgboost documentation: https://xgboost.readthedocs.io/en/latest/\n",
    "- sklearn.tree.DecisionTreeClassifier documentation: https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier\n",
    "- metrics: https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
