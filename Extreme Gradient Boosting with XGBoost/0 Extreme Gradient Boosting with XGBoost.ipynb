{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extreme Gradient Boosting with XGBoost\n",
    "\n",
    "Do you know the basics of supervised learning and want to use state-of-the-art models on real-world datasets? \n",
    "\n",
    "Gradient boosting is currently one of the most popular techniques for efficient modeling of tabular datasets of all sizes. \n",
    "\n",
    "XGboost is a very fast, scalable implementation of gradient boosting, with models using XGBoost regularly winning online data science competitions and being used at scale across different industries. \n",
    "\n",
    "In this course, you'll learn how to use this powerful library alongside pandas and scikit-learn to build and tune supervised learning models. \n",
    "\n",
    "You'll work with real-world datasets to solve classification and regression problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Classification with XGBoost\n",
    "\n",
    "This chapter will introduce you to the fundamental idea behind XGBoostâ€”boosted learners. Once you understand how XGBoost works, you'll apply it to solve a common classification problem found in industry: predicting whether a customer will stop being a customer at some point in the future.\n",
    "\n",
    "    1.1 Welcome to the course!\n",
    "    1.2 Which of these is a classification problem?\n",
    "    1.3 Which of these is a binary classification problem?\n",
    "    1.4 Introducing XGBoost\n",
    "    1.5 XGBoost: Fit/Predict\n",
    "    1.6 What is a decision tree?\n",
    "    1.7 Decision trees\n",
    "    1.8 What is Boosting?\n",
    "    1.9 Measuring accuracy\n",
    "    1.10 Measuring AUC\n",
    "    1.11 When should I use XGBoost?\n",
    "    1.12 Using XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Regression with XGBoost\n",
    "\n",
    "After a brief review of supervised regression, you'll apply XGBoost to the regression task of predicting house prices in Ames, Iowa. You'll learn about the two kinds of base learners that XGboost can use as its weak learners, and review how to evaluate the quality of your regression models.\n",
    "\n",
    "    2.1 Regression review\n",
    "    2.2 Which of these is a regression problem?\n",
    "    2.3 Objective (loss) functions and base learners\n",
    "    2.4 Decision trees as base learners\n",
    "    2.5 Linear base learners\n",
    "    2.6 Evaluating model quality\n",
    "    2.7 Regularization and base learners in XGBoost\n",
    "    2.8 Using regularization in XGBoost\n",
    "    2.9 Visualizing individual XGBoost trees\n",
    "    2.10 Visualizing feature importances: What features are most important in my dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fine-tuning your XGBoost model\n",
    "\n",
    "This chapter will teach you how to make your XGBoost models as performant as possible. You'll learn about the variety of parameters that can be adjusted to alter the behavior of XGBoost and how to tune them efficiently so that you can supercharge the performance of your models.\n",
    "\n",
    "    3.1 Why tune your model?\n",
    "    3.2 When is tuning your model a bad idea?\n",
    "    3.3 Tuning the number of boosting rounds\n",
    "    3.4 Automated boosting round selection using early_stopping\n",
    "    3.5 Overview of XGBoost's hyperparameters\n",
    "    3.6 Tuning eta\n",
    "    3.7 Tuning max_depth\n",
    "    3.8 Tuning colsample_bytree\n",
    "    3.9 Review of grid search and random search\n",
    "    3.10 Grid search with XGBoost\n",
    "    3.11 Random search with XGBoost\n",
    "    3.12 Limits of grid search and random search\n",
    "    3.13 When should you use grid search and random search?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Using XGBoost in pipelines\n",
    "\n",
    "Take your XGBoost skills to the next level by incorporating your models into two end-to-end machine learning pipelines. \n",
    "You'll learn how to tune the most important XGBoost hyperparameters efficiently within a pipeline, and get an introduction to some more advanced preprocessing techniques.\n",
    "\n",
    "    4.1 Review of pipelines using sklearn\n",
    "    4.2 Exploratory data analysis\n",
    "    4.3 Encoding categorical columns I: LabelEncoder\n",
    "    4.4 Encoding categorical columns II: OneHotEncoder\n",
    "    4.5 Encoding categorical columns III: DictVectorizer\n",
    "    4.6 Preprocessing within a pipeline\n",
    "    4.7 Incorporating XGBoost into pipelines\n",
    "    4.8 Cross-validating your XGBoost model\n",
    "    4.9 Kidney disease case study I: Categorical Imputer\n",
    "    4.10 Kidney disease case study II: Feature Union\n",
    "    4.11 Kidney disease case study III: Full pipeline\n",
    "    4.12 Tuning XGBoost hyperparameters\n",
    "    4.13 Bringing it all together\n",
    "    4.14 Final Thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aditional material\n",
    "\n",
    "- Datacamp course: https://learn.datacamp.com/courses/extreme-gradient-boosting-with-xgboost\n",
    "- Xgboost documentation: https://xgboost.readthedocs.io/en/latest/\n",
    "- sklearn.tree.DecisionTreeClassifier documentation: https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier\n",
    "- metrics: https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
