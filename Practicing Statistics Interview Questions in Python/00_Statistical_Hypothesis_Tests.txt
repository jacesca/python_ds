# -*- coding: utf-8 -*-
"""
Created on Fri Mar 20 19:48:06 2020

@author: jacqueline.cortez
Subject: 17 Statistical Hypothesis Tests in Python (Cheat Sheet)
Source: https://machinelearningmastery.com/statistical-hypothesis-tests-in-python-cheat-sheet/
"""

print("****************************************************")
print("** BEGIN                                          **")
print("****************************************************")
topic = "Preparing the environment"; print("** %s\n" % topic)

import numpy                          as np                                   #For making operations in lists
SEED = 123
np.random.seed(SEED)

print("****************************************************")
topic = "1. Normality Tests"; print("** %s\n" % topic)

# This section lists statistical tests that you can use to check if your 
# data has a Gaussian distribution.

#***********************************************************************
#**** Shapiro-Wilk Test
#***********************************************************************
#Tests whether a data sample has a Gaussian distribution.
#
#Assumptions:
#Observations in each sample are independent and identically distributed.
#
#Interpretation:
#H0: the sample has a Gaussian distribution.
#H1: the sample does not have a Gaussian distribution.
#
#Python Code:

from scipy.stats                     import shapiro                           #For Shapiro-Wilk Normality Test. Tests whether a data sample has a Gaussian distribution.
print("SHAPIRO-WILK TEST:")
data = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]
stat, p = shapiro(data)
print('stat=%.3f, p=%.3f' % (stat, p))
if p > 0.05:
	print('Probably Gaussian (p > 0.05).')
else:
	print('Probably not Gaussian (p <= 0.05).')

#More information:
#A Gentle Introduction to Normality Tests in Python -->https://machinelearningmastery.com/a-gentle-introduction-to-normality-tests-in-python/
#scipy.stats.shapiro --> https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.shapiro.html
#Shapiro-Wilk test on Wikipedia --> https://en.wikipedia.org/wiki/Shapiro%E2%80%93Wilk_test


#***********************************************************************
#**** D’Agostino’s K^2 Test
#***********************************************************************
#Tests whether a data sample has a Gaussian distribution.
#
#Assumptions:
#Observations in each sample are independent and identically distributed.
#
#Interpretation
#H0: the sample has a Gaussian distribution.
#H1: the sample does not have a Gaussian distribution.
#
#Python Code:
    
from scipy.stats                     import normaltest                        #For D'Agostino's K^2 Normality Test. Tests whether a data sample has a Gaussian distribution.
print("\nD'AGOSTINO'S K^2 TEST:")
data = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869, 0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]
stat, p = normaltest(data)
print('stat=%.3f, p=%.3f' % (stat, p))
if p > 0.05:
	print('Probably Gaussian (p > 0.05).')
else:
	print('Probably not Gaussian (p <= 0.05).')
"""
Give a warning if the sample has n<20.
C:\Anaconda3\lib\site-packages\scipy\stats\stats.py:1535: 
UserWarning: kurtosistest only valid for n>=20 ... continuing anyway, n=10 "anyway, n=%i" % int(n))
"""
#More information:
#A Gentle Introduction to Normality Tests in Python --> https://machinelearningmastery.com/a-gentle-introduction-to-normality-tests-in-python/
#scipy.stats.normaltest --> https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.normaltest.html
#D’Agostino’s K-squared test on Wikipedia --> https://en.wikipedia.org/wiki/D%27Agostino%27s_K-squared_test


#***********************************************************************
#**** Anderson-Darling Test
#***********************************************************************
#Tests whether a data sample has a Gaussian distribution.
#
#Assumptions:
#Observations in each sample are independent and identically distributed.
#
#Interpretation:
#H0: the sample has a Gaussian distribution.
#H1: the sample does not have a Gaussian distribution.
#
#Python Code

from scipy.stats                     import anderson                          #For Anderson-Darling Normality Test. Tests whether a data sample has a Gaussian distribution.
print("\nANDERSON-DARLING TEST:")
data = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]
result = anderson(data)
print('stat=%.3f' % (result.statistic))
for i in range(len(result.critical_values)):
	sl, cv = result.significance_level[i], result.critical_values[i]
	if result.statistic < cv:
		print('Probably Gaussian at the %.1f%% level (statistic < critical value).' % (sl))
	else:
		print('Probably not Gaussian at the %.1f%% level (statistic >= critical value).' % (sl))

#More Information:
#A Gentle Introduction to Normality Tests in Python --> https://machinelearningmastery.com/a-gentle-introduction-to-normality-tests-in-python/
#scipy.stats.anderson --> https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.anderson.html
#Anderson-Darling test on Wikipedia --> https://en.wikipedia.org/wiki/Anderson%E2%80%93Darling_test

print("\n****************************************************")
topic = "2. Correlation Tests"; print("** %s\n" % topic)
#This section lists statistical tests that you can use to check if two samples are related.

#***********************************************************************
#**** Pearson’s Correlation Coefficient
#***********************************************************************
#Tests whether two samples have a linear relationship.
#
#Assumptions:
#Observations in each sample are independent and identically distributed.
#Observations in each sample are normally distributed.
#Observations in each sample have the same variance.
#
#Interpretation
#H0: the two samples are independent.
#H1: there is a dependency between the samples.
#
#Python Code:

from scipy.stats                     import pearsonr                          #For learning machine. For Pearson's Correlation test. To check if two samples are related.
print("PEARSON'S CORRELATION COEFFICIENT:")
data1 = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]
data2 = [0.353, 3.517, 0.125, -7.545, -0.555, -1.536, 3.350, -1.578, -3.537, -1.579]
stat, p = pearsonr(data1, data2)
print('stat=%.3f, p=%.3f' % (stat, p))
if p > 0.05:
	print('Probably independent (p > 0.05).')
else:
	print('Probably dependent (p <= 0.05).')

#More Information:
#How to Calculate Correlation Between Variables in Python --> https://machinelearningmastery.com/how-to-use-correlation-to-understand-the-relationship-between-variables/
#scipy.stats.pearsonr https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pearsonr.html
#Pearson’s correlation coefficient on Wikipedia --> https://en.wikipedia.org/wiki/Pearson_correlation_coefficient


#***********************************************************************
#**** Spearman’s Rank Correlation
#***********************************************************************
#Tests whether two samples have a monotonic relationship.
#
#Assumptions:
#Observations in each sample are independent and identically distributed.
#Observations in each sample can be ranked.
#
#Interpretation:
#H0: the two samples are independent.
#H1: there is a dependency between the samples.
#
#Python Code:

from scipy.stats                     import spearmanr                         #For Spearman's Rank Correlation Test.  To check if two samples are related.
print("\nSPEARMAN'S RANK CORRELATION:")
data1 = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]
data2 = [0.353, 3.517, 0.125, -7.545, -0.555, -1.536, 3.350, -1.578, -3.537, -1.579]
stat, p = spearmanr(data1, data2)
print('stat=%.3f, p=%.3f' % (stat, p))
if p > 0.05:
	print('Probably independent (p > 0.05).')
else:
	print('Probably dependent (p <= 0.05).')

#More Information:
#How to Calculate Nonparametric Rank Correlation in Python --> https://machinelearningmastery.com/how-to-calculate-nonparametric-rank-correlation-in-python/
#scipy.stats.spearmanr --> https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.spearmanr.html
#Spearman’s rank correlation coefficient on Wikipedia --> https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient
    

#***********************************************************************
#**** Kendall’s Rank Correlation
#***********************************************************************
#Tests whether two samples have a monotonic relationship.
#
#Assumptions:
#Observations in each sample are independent and identically distributed.
#Observations in each sample can be ranked.
#
#Interpretation:
#H0: the two samples are independent.
#H1: there is a dependency between the samples.
#
#Python Code

from scipy.stats                     import kendalltau                        #For Kendall's Rank Correlation Test. To check if two samples are related.
print("\nKENDALL'S RANK CORRELATION:")
data1 = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]
data2 = [0.353, 3.517, 0.125, -7.545, -0.555, -1.536, 3.350, -1.578, -3.537, -1.579]
stat, p = kendalltau(data1, data2)
print('stat=%.3f, p=%.3f' % (stat, p))
if p > 0.05:
	print('Probably independent (p > 0.05).')
else:
	print('Probably dependent (p <= 0.05).')

#More Information:
#How to Calculate Nonparametric Rank Correlation in Python --> https://machinelearningmastery.com/how-to-calculate-nonparametric-rank-correlation-in-python/
#scipy.stats.kendalltau --> https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.kendalltau.html
#Kendall rank correlation coefficient on Wikipedia --> https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient


#***********************************************************************
#**** Chi-Squared Test
#***********************************************************************
#Tests whether two categorical variables are related or independent.
#
#Assumptions:
#Observations used in the calculation of the contingency table are independent.
#25 or more examples in each cell of the contingency table.
#
#Interpretation:
#H0: the two samples are independent.
#H1: there is a dependency between the samples.
#
#Python Code

from scipy.stats                     import chi2_contingency                  #For Chi-Squared Test. Tests whether two categorical variables are related or independent
print("\nCHI-SQUARED TEST:")
table = [[10, 20, 30],[6,  9,  17]]
stat, p, dof, expected = chi2_contingency(table)
print('stat=%.3f, p=%.3f' % (stat, p))
if p > 0.05:
	print('Probably independent (p > 0.05).')
else:
	print('Probably dependent (p <= 0.05).')
"""
Give an error if the sample has negatives values.
C:\Anaconda3\lib\site-packages\scipy\stats\contingency.py, line 245, in chi2_contingency
raise ValueError("All values in `observed` must be nonnegative.")
ValueError: All values in `observed` must be nonnegative.
"""

#More Information:
#A Gentle Introduction to the Chi-Squared Test for Machine Learning --> https://machinelearningmastery.com/chi-squared-test-for-machine-learning/
#scipy.stats.chi2_contingency --> https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chi2_contingency.html
#Chi-Squared test on Wikipedia --> https://en.wikipedia.org/wiki/Chi-squared_test

print("\n****************************************************")
topic = "3. Stationary Tests"; print("** %s\n" % topic)
#This section lists statistical tests that you can use to check if a time series is stationary or not.

#***********************************************************************
#**** Augmented Dickey-Fuller Unit Root Test
#***********************************************************************
#Tests whether a time series has a unit root, e.g. has a trend or more generally is autoregressive.
#
#Assumptions:
#Observations in are temporally ordered.
#
#Interpretation:
#H0: a unit root is present (series is non-stationary).
#H1: a unit root is not present (series is stationary).
#
#Python Code:

from statsmodels.tsa.stattools       import adfuller                          #For Augmented Dickey-Fuller unit root test. Tests whether a time series has a unit root, e.g. has a trend or more generally is autoregressive. Tests that you can use to check if a time series is stationary or not.
print("STATIONARY TEST:")
data = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
stat, p, lags, obs, crit, t = adfuller(data)
print('stat=%.3f, p=%.3f' % (stat, p))
if p > 0.05:
	print('Probably not Stationary (p > 0.05).')
else:
	print('Probably Stationary (p <= 0.05).')

#More Information:
#How to Check if Time Series Data is Stationary with Python --> https://machinelearningmastery.com/time-series-data-stationary-python/
#statsmodels.tsa.stattools.adfuller API. --> https://www.statsmodels.org/dev/generated/statsmodels.tsa.stattools.adfuller.html
#Augmented Dickey–Fuller test, Wikipedia. --> https://en.wikipedia.org/wiki/Augmented_Dickey%E2%80%93Fuller_test


#***********************************************************************
#**** Kwiatkowski-Phillips-Schmidt-Shin
#***********************************************************************
#Tests whether a time series is trend stationary or not.
#
#Assumptions:
#Observations in are temporally ordered.
#
#Interpretation:
#H0: the time series is not trend-stationary.
#H1: the time series is trend-stationary.
#
#Python Code:

from statsmodels.tsa.stattools       import kpss                              #For Kwiatkowski-Phillips-Schmidt-Shin test. Tests whether a time series is trend stationary or not.
print("\nKWIATKOWSKI-PHILLIPS-SCHMIDT-SHIN TEST:")
data = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
stat, p, lags, crit = kpss(data, nlags='legacy')
print('stat=%.3f, p=%.3f' % (stat, p))
if p > 0.05:
	print('Probably not Stationary (p > 0.05).')
else:
	print('Probably Stationary (p <= 0.05).')
"""
Give a warning if we don't use the nlags attribute
C:\Anaconda3\lib\site-packages\statsmodels\tsa\stattools.py:1661: 
FutureWarning: The behavior of using lags=None will change in the next release. 
Currently lags=None is the same as lags='legacy', and so a sample-size lag length is used. 
After the next release, the default will change to be the same as lags='auto' which uses an automatic lag length 
selection method. To silence this warning, either use 'auto' or 'legacy'.

FutureWarning: the 'lags'' keyword is deprecated, use 'nlags' instead.
"""

#More Information:
#statsmodels.tsa.stattools.kpss API. --> https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.kpss.html#statsmodels.tsa.stattools.kpss
#KPSS test, Wikipedia. --> https://en.wikipedia.org/wiki/KPSS_test

print("\n****************************************************")
topic = "4. Parametric Statistical Hypothesis Tests"; print("** %s\n" % topic)
#This section lists statistical tests that you can use to compare data samples.

#***********************************************************************
#**** Student’s t-test
#***********************************************************************
#Tests whether the means of two independent samples are significantly different.
#
#Assumptions:
#Observations in each sample are independent and identically distributed.
#Observations in each sample are normally distributed.
#Observations in each sample have the same variance.
#
#Interpretation:
#H0: the means of the samples are equal.
#H1: the means of the samples are unequal.
#
#Python Code:

from scipy.stats                     import ttest_ind                         #For Student's t-test. Tests whether the means of two independent samples are significantly different.
print("STUDENT’S T-TEST:")
data1 = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]
data2 = [1.142, -0.432, -0.938, -0.729, -0.846, -0.157, 0.500, 1.183, -1.075, -0.169]
stat, p = ttest_ind(data1, data2)
print('stat=%.3f, p=%.3f' % (stat, p))
if p > 0.05:
	print('Probably the same distribution (p > 0.05).')
else:
	print('Probably different distributions (p <= 0.05).')

#More Information:
#How to Calculate Parametric Statistical Hypothesis Tests in Python --> https://machinelearningmastery.com/parametric-statistical-significance-tests-in-python/
#scipy.stats.ttest_ind --> https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html
#Student’s t-test on Wikipedia --> https://en.wikipedia.org/wiki/Student%27s_t-test
    
    
#***********************************************************************
#**** Paired Student’s t-test
#***********************************************************************
#Tests whether the means of two paired samples are significantly different.
#
#Assumptions:
#Observations in each sample are independent and identically distributed.
#Observations in each sample are normally distributed.
#Observations in each sample have the same variance.
#Observations across each sample are paired.
#
#Interpretation:
#H0: the means of the samples are equal.
#H1: the means of the samples are unequal.
#
#Python Code:

from scipy.stats                     import ttest_rel                         #For Paired Student's t-test. Tests whether the means of two paired samples are significantly different.
print("\nPAIRED STUDENT’S T-TEST:")
data1 = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]
data2 = [1.142, -0.432, -0.938, -0.729, -0.846, -0.157, 0.500, 1.183, -1.075, -0.169]
stat, p = ttest_rel(data1, data2)
print('stat=%.3f, p=%.3f' % (stat, p))
if p > 0.05:
	print('Probably the same distribution (p > 0.05).')
else:
	print('Probably different distributions (p <= 0.05).')

#More Information:
#How to Calculate Parametric Statistical Hypothesis Tests in Python
#scipy.stats.ttest_rel
#Student’s t-test on Wikipedia


#***********************************************************************
#**** Analysis of Variance Test (ANOVA)
#***********************************************************************
#Tests whether the means of two or more independent samples are significantly different.
#
#Assumptions:
#Observations in each sample are independent and identically distributed (iid).
#Observations in each sample are normally distributed.
#Observations in each sample have the same variance.
#
#Interpretation:
#H0: the means of the samples are equal.
#H1: one or more of the means of the samples are unequal.
#
#Python Code:

from scipy.stats                     import f_oneway                          #For Analysis of Variance Test. Tests whether the means of two or more independent samples are significantly different.
print("\nANALYSIS OF VARIANCE TEST (ANOVA):")
data1 = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]
data2 = [1.142, -0.432, -0.938, -0.729, -0.846, -0.157, 0.500, 1.183, -1.075, -0.169]
data3 = [-0.208, 0.696, 0.928, -1.148, -0.213, 0.229, 0.137, 0.269, -0.870, -1.204]
stat, p = f_oneway(data1, data2, data3)
print('stat=%.3f, p=%.3f' % (stat, p))
if p > 0.05:
	print('Probably the same distribution (p > 0.05).')
else:
	print('Probably different distributions (p <= 0.05).')

#More Information
#How to Calculate Parametric Statistical Hypothesis Tests in Python --> https://machinelearningmastery.com/parametric-statistical-significance-tests-in-python/
#scipy.stats.f_oneway --> https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.f_oneway.html
#Analysis of variance on Wikipedia -- https://en.wikipedia.org/wiki/Analysis_of_variance


#***********************************************************************
#**** Repeated Measures ANOVA Test
#***********************************************************************
#Tests whether the means of two or more paired samples are significantly different.
#
#Assumptions:
#Observations in each sample are independent and identically distributed.
#Observations in each sample are normally distributed.
#Observations in each sample have the same variance.
#Observations across each sample are paired.
#
#Interpretation:
#H0: the means of the samples are equal.
#H1: one or more of the means of the samples are unequal.
#
#Python Code:
#
#Currently not supported in Python.
#
#More Information:
#How to Calculate Parametric Statistical Hypothesis Tests in Python --> https://machinelearningmastery.com/parametric-statistical-significance-tests-in-python/
#Analysis of variance on Wikipedia --> https://en.wikipedia.org/wiki/Analysis_of_variance

print("\n****************************************************")
topic = "5. Nonparametric Statistical Hypothesis Tests"; print("** %s\n" % topic)

#***********************************************************************
#**** Mann-Whitney U Test
#***********************************************************************
#Tests whether the distributions of two independent samples are equal or not.
#
#Assumptions:
#Observations in each sample are independent and identically distributed (iid).
#Observations in each sample can be ranked.
#
#Interpretation:
#H0: the distributions of both samples are equal.
#H1: the distributions of both samples are not equal.
#
#Python Code:

from scipy.stats                     import mannwhitneyu                      #For Mann-Whitney U Test. Tests whether the distributions of two independent samples are equal or not.
print("MANN-WHITNEY U TEST:")
data1 = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]
data2 = [1.142, -0.432, -0.938, -0.729, -0.846, -0.157, 0.500, 1.183, -1.075, -0.169]
stat, p = mannwhitneyu(data1, data2)
print('stat=%.3f, p=%.3f' % (stat, p))
if p > 0.05:
	print('Probably the same distribution (p > 0.05)')
else:
	print('Probably different distributions (p <= 0.05)')

#More Information:
#How to Calculate Nonparametric Statistical Hypothesis Tests in Python --> https://machinelearningmastery.com/nonparametric-statistical-significance-tests-in-python/
#scipy.stats.mannwhitneyu --> https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.mannwhitneyu.html
#Mann-Whitney U test on Wikipedia --> https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test


#***********************************************************************
#**** Wilcoxon Signed-Rank Test
#***********************************************************************
#Tests whether the distributions of two paired samples are equal or not.
#
#Assumptions:
#Observations in each sample are independent and identically distributed.
#Observations in each sample can be ranked.
#Observations across each sample are paired.
#
#Interpretation:
#H0: the distributions of both samples are equal.
#H1: the distributions of both samples are not equal.
#
#Python Code:

from scipy.stats                     import wilcoxon                          #For Wilcoxon Signed-Rank Test. Tests whether the distributions of two paired samples are equal or not.
print("\nWILCOXON SIGNED-RANK TEST:")
data1 = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]
data2 = [1.142, -0.432, -0.938, -0.729, -0.846, -0.157, 0.500, 1.183, -1.075, -0.169]
stat, p = wilcoxon(data1, data2)
print('stat=%.3f, p=%.3f' % (stat, p))
if p > 0.05:
	print('Probably the same distribution (p > 0.05).')
else:
	print('Probably different distributions (p <= 0.05).')

#More Information:
#How to Calculate Nonparametric Statistical Hypothesis Tests in Python --> https://machinelearningmastery.com/nonparametric-statistical-significance-tests-in-python/
#scipy.stats.wilcoxon --> https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.wilcoxon.html
#Wilcoxon signed-rank test on Wikipedia --> https://en.wikipedia.org/wiki/Wilcoxon_signed-rank_test


#***********************************************************************
#**** Kruskal-Wallis H Test
#***********************************************************************
#Tests whether the distributions of two or more independent samples are equal or not.
#
#Assumptions:
#Observations in each sample are independent and identically distributed.
#Observations in each sample can be ranked.
#
#Interpretation:
#H0: the distributions of all samples are equal.
#H1: the distributions of one or more samples are not equal.
#
#Python Code:

from scipy.stats                     import kruskal                           #For Kruskal-Wallis H Test. Tests whether the distributions of two or more independent samples are equal or not.
print("\nKRUSKAL-WALLIS H TEST:")
data1 = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]
data2 = [1.142, -0.432, -0.938, -0.729, -0.846, -0.157, 0.500, 1.183, -1.075, -0.169]
stat, p = kruskal(data1, data2)
print('stat=%.3f, p=%.3f' % (stat, p))
if p > 0.05:
	print('Probably the same distribution (p > 0.05).')
else:
	print('Probably different distributions (p <= 0.05).')

#More Information:
#How to Calculate Nonparametric Statistical Hypothesis Tests in Python --> https://machinelearningmastery.com/nonparametric-statistical-significance-tests-in-python/
#scipy.stats.kruskal --> https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.kruskal.html
#Kruskal-Wallis one-way analysis of variance on Wikipedia --> https://en.wikipedia.org/wiki/Kruskal%E2%80%93Wallis_one-way_analysis_of_variance


#***********************************************************************
#**** Friedman Test
#***********************************************************************
#Tests whether the distributions of two or more paired samples are equal or not.
#
#Assumptions:
#Observations in each sample are independent and identically distributed.
#Observations in each sample can be ranked.
#Observations across each sample are paired.
#
#Interpretation:
#H0: the distributions of all samples are equal.
#H1: the distributions of one or more samples are not equal.
#
#Python Code:

from scipy.stats                     import friedmanchisquare                 #For Friedman Test. Tests whether the distributions of two or more paired samples are equal or not.
print("\nFRIEDMAN TEST:")
data1 = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]
data2 = [1.142, -0.432, -0.938, -0.729, -0.846, -0.157, 0.500, 1.183, -1.075, -0.169]
data3 = [-0.208, 0.696, 0.928, -1.148, -0.213, 0.229, 0.137, 0.269, -0.870, -1.204]
stat, p = friedmanchisquare(data1, data2, data3)
print('stat=%.3f, p=%.3f' % (stat, p))
if p > 0.05:
	print('Probably the same distribution (p > 0.05).')
else:
	print('Probably different distributions (p <= 0.05).')

#More Information:
#How to Calculate Nonparametric Statistical Hypothesis Tests in Python --> https://machinelearningmastery.com/nonparametric-statistical-significance-tests-in-python/
#scipy.stats.friedmanchisquare --> https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.kruskal.html
#Friedman test on Wikipedia --> https://en.wikipedia.org/wiki/Friedman_test
"""
Further Reading
This section provides more resources on the topic if you are looking to go deeper:

A Gentle Introduction to Normality Tests in Python --> https://machinelearningmastery.com/how-to-use-correlation-to-understand-the-relationship-between-variables/
How to Use Correlation to Understand the Relationship Between Variables --> https://machinelearningmastery.com/how-to-use-correlation-to-understand-the-relationship-between-variables/
How to Use Parametric Statistical Significance Tests in Python --> https://machinelearningmastery.com/parametric-statistical-significance-tests-in-python/
A Gentle Introduction to Statistical Hypothesis Tests --> https://machinelearningmastery.com/statistical-hypothesis-tests/

Summary
In this tutorial, you discovered the key statistical hypothesis tests that you may need to use 
in a machine learning project.

Specifically, you learned:
- The types of tests to use in different circumstances, such as normality checking, relationships between 
  variables, and differences between samples.
- The key assumptions for each test and how to interpret the test result.
- How to implement the test using the Python API.
"""
print("\n****************************************************")
print("** END                                            **")
print("****************************************************")