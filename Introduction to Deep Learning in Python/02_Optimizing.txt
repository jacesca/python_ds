# -*- coding: utf-8 -*-
"""
Created on Sat Aug 24 13:21:34 2019

@author: jacqueline.cortez

Chapter 2. Optimizing a neural network with backward propagation
Introduction:
    Learn how to optimize the predictions generated by your neural networks. You'll use a method called backward 
    propagation, which is one of the most important techniques in deep learning. Understanding how it works will 
    give you a strong foundation to build on in the second half of the course.
"""
import numpy as np                                                                  #For making operations in lists
import matplotlib.pyplot as plt                                                     #For creating charts
import seaborn as sns                                                               #For visualizing data

from sklearn.metrics import mean_squared_error as MSE                               #For learning machine

print("****************************************************")
print("** BEGIN                                          **")
print("****************************************************")
print("** User defined functions \n")

def relu(input):
    '''Define your relu activation function here'''
    output = max(0, input) # Calculate the value for the output of the relu function: output
    return(output) # Return the value just calculated


def onelayer_predict_with_network(input_data_row, weights): # Define predict_with_network()
    """Function to make prediction with one hidden layer"""
    node_0_input = (input_data_row * weights['node_0']).sum() # Calculate node 0 value
    node_0_output = relu(node_0_input)
    
    node_1_input = (input_data_row * weights['node_1']).sum() # Calculate node 1 value
    node_1_output = relu(node_1_input)
    
    hidden_layer_outputs = np.array([node_0_output, node_1_output]) # Put node values into array: hidden_layer_outputs
    
    input_to_final_layer = (hidden_layer_outputs * weights['output']).sum() # Calculate model output
    model_output = relu(input_to_final_layer)
    return(model_output)# Return model output


def pred(input_data, weights):
    """Make predictions base on input and weights"""
    return ((input_data * weights).sum())


def get_slope(input_data, target, weights):
    """Get slope base on input_data, weights and error get from difference between target and predictions"""
    preds = pred(input_data, weights)
    error = preds - target
    slope = 2 * input_data * error
    return slope


def get_mse(input_data, target, weights):
    """Get the mean square error from the getted predictions"""
    preds = pred(input_data, weights)
    return MSE([preds], [target])


print("****************************************************")
print("** Getting the data for this program\n")

print("****************************************************")
tema = "4. Coding how weight changes affect accuracy"; print("** %s\n" % tema)

input_data = np.array([0, 3]) # The data point you will make a prediction for
target_actual = 3 # The actual target value, used to calculate the error

weights_0 = {'node_0': [2, 1], 'node_1': [1, 2], 'output': [1, 1]} # Sample weights
model_output_0 = onelayer_predict_with_network(input_data, weights_0) # Make prediction using original weights
error_0 = model_output_0 - target_actual # Calculate error: error_0

weights_1 = {'node_0': [2, 1], 'node_1': [1, 2], 'output': [-1, 1]} # Create weights that cause the network to make perfect prediction (3): weights_1
model_output_1 = onelayer_predict_with_network(input_data, weights_1) # Make prediction using new weights: model_output_1
error_1 = model_output_1 - target_actual # Calculate error: error_1

print(error_0) # Print error_0 and error_1
print(error_1)

print("****************************************************")
tema = "5. Scaling up to multiple data points"; print("** %s\n" % tema)

input_data = [np.array([0, 3]), np.array([1, 2]), np.array([-1, -2]), np.array([4, 0])]
target_actuals = [1, 3, 5, 7]
weights_0 = {'node_0': np.array([2, 1]), 'node_1': np.array([1, 2]), 'output': np.array([1, 1])}
weights_1 = {'node_0': np.array([2, 1]), 'node_1': np.array([1. , 1.5]), 'output': np.array([1. , 1.5])}

model_output_0 = [] # Create model_output_0 
model_output_1 = [] # Create model_output_1

for row in input_data: # Loop over input_data
    model_output_0.append(onelayer_predict_with_network(row, weights_0)) # Append prediction to model_output_0
    model_output_1.append(onelayer_predict_with_network(row, weights_1)) # Append prediction to model_output_1
    
mse_0 = MSE(target_actuals, model_output_0) # Calculate the mean squared error for model_output_0: mse_0
mse_1 = MSE(target_actuals, model_output_1) # Calculate the mean squared error for model_output_1: mse_1

print("Mean squared error with weights_0: %f" %mse_0) # Print mse_0 and mse_1
print("Mean squared error with weights_1: %f" %mse_1)

print("Root mean squared error with weights_0: %f" %(mse_0**(1/2))) # Print rmse_0 and mse_1
print("Root mean squared error with weights_1: %f" %(mse_1**(1/2)))

print("****************************************************")
tema = "7. Calculating slopes"; print("** %s\n" % tema)

input_data = np.array([1, 2, 3])
weights = np.array([0, 2, 1])
target = 0

preds = (input_data * weights).sum() # Calculate the predictions: preds
error = preds - target# Calculate the error: error
slope = 2 * input_data * error # Calculate the slope: slope

print('inout data: ', input_data)
print('weights: ', weights)
print('target: ', target)
print('predictions: ', preds)
print('slope: ', slope) # Print the slope

print("****************************************************")
tema = "8. Improving model weights"; print("** %s\n" % tema)

learning_rate = 0.01 # Set the learning rate: learning_rate

weights_updated = weights - (slope * learning_rate) # Update the weights: weights_updated
print('updated weights: ', weights_updated)

preds_updated = (input_data * weights_updated).sum() # Get updated predictions: preds_updated
error_updated = preds_updated - target # Calculate updated error: error_updated

print("Initial error: ", error) # Print the original error
print("Error after first rectification: ", error_updated) # Print the updated error

print("****************************************************")
tema = "9. Making multiple updates to weights"; print("** %s\n" % tema)

learning_rate = 0.01
input_data = np.array([1, 2, 3])
weights = np.array([0, 2, 1])
target = 0

n_updates = 20
mse_hist = []

for i in range(n_updates): # Iterate over the number of updates
    slope = get_slope(input_data, target, weights) # Calculate the slope: slope
    weights = weights - (slope * learning_rate) # Update the weights: weights
    mse = get_mse(input_data, target, weights) # Calculate mse with new weights: mse
    mse_hist.append(mse) # Append the mse to mse_hist

sns.set() # Set default Seaborn style
#plt.figure()
plt.plot(mse_hist) # Plot the mse history
plt.xlabel('Iterations')
plt.ylabel('Mean Squared Error')
plt.title('Features Importances')
plt.suptitle(tema)
#plt.subplots_adjust(left=0.35, bottom=None, right=None, top=0.88, wspace=None, hspace=None)
plt.show()
plt.style.use('default')

print("****************************************************")
print("** END                                            **")
print("****************************************************")