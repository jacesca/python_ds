# -*- coding: utf-8 -*-
"""
Created on Wed Jul 15 19:02:47 2020

@author: jacesca
Additional documentation:
    List of tags: https://medium.com/@muddaprince456/categorizing-and-pos-tagging-with-nltk-python-28f2bc9312c3
    Source of the code: https://randlow.github.io/posts/nlp/nlp-ner/
"""

print("****************************************************")
step = "BEGIN"; print("** %s" % step)
print("****************************************************")
step = "Importing libraries.."; print("** %s" % step)
import os
import nltk
#import collections
import matplotlib.pyplot as plt
from collections import defaultdict
from pprint import pprint
from nltk.tokenize import word_tokenize
from nltk.tokenize import sent_tokenize


print("****************************************************")
step = "Preparing the environment...\n"; print("** %s" % step)
suptitle_param = dict(color='darkblue', fontsize=11)
title_param    = {'color': 'darkred', 'fontsize': 14, 'weight': 'bold'}
plot_param     = {'axes.labelsize': 8, 'xtick.labelsize': 8, 'ytick.labelsize': 8, 
                  'legend.fontsize': 8, 'font.size': 8}
figsize        = (12.1, 5.9)

# Global configuration
plt.rcParams.update(**plot_param)


print("****************************************************")
step = "Reading the file...\n"; print("** %s" % step)
currDir = os.getcwd()

fileName = 'aeon.txt'
readFile = currDir + '\\' + fileName


with open(readFile, 'r', encoding='utf-8') as f: 
    article = f.read()
print(article[:100])


print("****************************************************")
step = "To obtain information about tags use nltk.help.upenn_tagset...\n"; print("** %s" % step)
print(nltk.help.upenn_tagset('VBP'))


print("****************************************************")
step = "Step 1. We tokenize the article into sentences using sent_tokenize.\n"; print("** %s" % step)
sentences = sent_tokenize(article)
print(sentences[:1])

print("****************************************************")
step = "Step 2. Using word_tokenize(sentence) will create a list of sentence lists. " + \
       "The sentence list comprises of tokens of each sentence.\n"; print("** %s" % step);
token_sentences = [ word_tokenize(sentence) for sentence in sentences ]
pprint(token_sentences[:1])

print("****************************************************")
step = "Step 3. Using nltk.pos_tag, we POS-tag each token of the sentence using POS " + \
       "(parts-of-speech) tagging so we know what type of category each word is. This " + \
       "produce a list of tuples where (i) token (ii) the POS-tag.\n"; print("** %s" % step);
pos_sentences = [ nltk.pos_tag(sent) for sent in token_sentences ]
pprint(pos_sentences[:1])

print("****************************************************")
step = "Step 4. Using nltk.ne_chunk_sents) on the list of tuples consiting of (i) token " + \
       "(ii) POS-tag of the token, we can obtain the words that are the important named " + \
       "entities of the document. nltk.ne_chunk_sents returns a tree which has leaves " + \
       "and sub-trees to represent.\n"; print("** %s" % step);
chunked_sentences = nltk.ne_chunk_sents(pos_sentences, binary=True)
print(chunked_sentences)

print("****************************************************")
step = "Step 5. 'ne_chunk_sents' will convert the list of list of tuples (i.e., list of " + \
       "sentences with tuples of tokens & POS-tags) that is generated by nltk.pos_tag into " + \
       "a generator function. Using a for loop we can access each sentence in the chunked_sentences, " + \
       "and each token and associated POS-tag of each sentence as shown below. " + \
       "Most importantly, it processes the article into named-entity. Having the binary=True " + \
       "is such that named entities are tagged as NE. If binary=False the classifier adds the " + \
       "category labels such as PERSON, ORGANIZATION, GPE, etc. as shown in the Table at the top."; print("** %s" % step)
chunked_sentences = nltk.ne_chunk_sents(pos_sentences,binary=True)
for i, sent in enumerate(chunked_sentences, start=1):
    if i>2: break; #Only two sentences
    print(f'\nSENTENCE {i}:')
    for chunk in sent:
        print(chunk)

step = "We can see above that any chunks that have a label and is a NE, that is a named-entity, " + \
       "we print it out. We find that only certain chunks in our document are named entities.\n"; print("\n\n** %s" % step)
chunked_sentences = nltk.ne_chunk_sents(pos_sentences,binary=True)
for sent in chunked_sentences:
    for chunk in sent:
        if hasattr(chunk, "label") and chunk.label() == "NE":
            print(chunk)
            

step = "The binary keyword for nltk.ne_chunk_sents.\n" + \
       "When we set binary=False, the classifier adds cateory labels to the named entities. " + \
       "Thus we do not just have ne but we havev more informative labels such as GPE, PERSON, " + \
       "ORGANIZATION, etc.\n"; print("\n\n** %s" % step)
chunked_sentences = nltk.ne_chunk_sents(pos_sentences, binary=False)
for sent in chunked_sentences:
    for chunk in sent:
        if hasattr(chunk,'label'):
            print(chunk)

print("****************************************************")
step = "Printing out percentage of each category\n"; print("** %s" % step);

chunked_sentences = nltk.ne_chunk_sents(pos_sentences, binary=False)
ner_categories = defaultdict(int)
for sent in chunked_sentences:
    for chunk in sent:
        if hasattr(chunk,'label'):
            ner_categories[chunk.label()] += 1           
print(f'{ner_categories}\n\n')

labels = list(ner_categories.keys())
#values = [ner_categories.get(l) for l in labels]
values = list(ner_categories.values())

fig, ax = plt.subplots()
ax.pie(values, labels=labels, autopct='%1.1f%%', startangle=90)
ax.set_title('Categories found in the article', **title_param)
fig.suptitle(step, **suptitle_param)
#plt.subplots_adjust(left=None, bottom=None, right=None, top=.85, wspace=None, hspace=.5) #To set the margins 
plt.show()

print("****************************************************")
step = "END"; print("** %s" % step)
print("****************************************************")
