{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Importing the requires libraries\n",
    "import speech_recognition as sr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. Using the Python SpeechRecognition library\n",
    "\n",
    "Speech recognition is still far from perfect. But the SpeechRecognition library provides an easy way to interact with many speech-to-text APIs. In this section, you'll learn how to use the SpeechRecognition library to easily start converting the spoken language in your audio files to text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02.01 SpeechRecognition Python library\n",
    "\n",
    "See the video.\n",
    "\n",
    "**Examples from the video - Working with with SpeechRecognition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "speech_recognition.AudioData"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_audio_file(wav_file, duration=None, offset=None, noise=0):\n",
    "    # Setup recognizer instance\n",
    "    recognizer = sr.Recognizer()\n",
    "    \n",
    "    # Read in audio file\n",
    "    clean_support_call = sr.AudioFile(wav_file) #type: speech_recognition.AudioFile\n",
    "\n",
    "    # Check type of clean_support_call\n",
    "    #type(clean_support_call) #type: speech_recognition.AudioFile\n",
    "    \n",
    "    # Convert from AudioFile to AudioData\n",
    "    with clean_support_call as source:\n",
    "        # Adjust for ambient noise and record\n",
    "        if noise>0:\n",
    "            recognizer.adjust_for_ambient_noise(source, duration=noise)\n",
    "        \n",
    "        # Record the audio\n",
    "        clean_support_call_audio = recognizer.record(source,\n",
    "                                                     duration=duration, #Listen from the begining to duration value.\n",
    "                                                     offset=offset) #used to skip over a specific seconds at the start.\n",
    "        \n",
    "    return clean_support_call_audio\n",
    "\n",
    "audio_file = get_audio_file('good_morning.wav')\n",
    "type(audio_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transcription(file, language, energy_threshold=300, duration=None, offset=None, show_all=None, noise=0):\n",
    "    # get the audio data\n",
    "    audio_data = get_audio_file(file, duration=duration, offset=offset, noise=0)\n",
    "\n",
    "    # Create an instance of Recognizer\n",
    "    recognizer = sr.Recognizer()\n",
    "    \n",
    "    # Set the energy threshold\n",
    "    recognizer.energy_threshold = energy_threshold\n",
    "    \n",
    "    # Transcribe speech using Goole web API\n",
    "    return recognizer.recognize_google(audio_data=audio_data, language=language, show_all=show_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language en-US:  good morning\n",
      "Language es-US:  Buenos días\n",
      "Language it-IT:  Dolcevita\n",
      "Language fr-CA:  je peux parler français\n"
     ]
    }
   ],
   "source": [
    "files = ['good_morning_jacesca.wav', 'buenos_dias.wav', 'it_example.wav', 'fr_example.wav']\n",
    "langs = ['en-US', 'es-US', 'it-IT', 'fr-CA']\n",
    "\n",
    "for file, lang in zip(files, langs):\n",
    "    print(f\"Language {lang}: \", get_transcription(file, lang))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02.02 Pick the wrong speech_recognition API\n",
    "\n",
    "**Instructions:**<br>\n",
    "Which of the following is **not a speech recognition API** within the <code>speech_recognition</code> library?\n",
    "\n",
    "An instance of the <code>Recognizer</code> class has been created and saved to <code>recognizer</code>. You can try calling the API on recognizer to see what happens.\n",
    "\n",
    "**Possible Answers:**\n",
    "1. recognize_google()\n",
    "2. recognize_bing()\n",
    "3. recognize_wit()\n",
    "4. __what_does_this_say()__\n",
    "\n",
    "**Results:**<br>\n",
    "<font color=darkgreen>Excellent! All of the Recognizer class API calls begin with recognize_.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02.03 Using the SpeechRecognition library\n",
    "\n",
    "To save typing <code>speech_recognition</code> every time, we'll import it as <code>sr</code>.\n",
    "\n",
    "We'll also setup an instance of the <code>Recognizer</code> class to use later.\n",
    "\n",
    "The <code>energy_threshold</code> is a number between 0 and 4000 for how much the <code>Recognizer</code> class should listen to an audio file.\n",
    "\n",
    "<code>energy_threshold</code> will dynamically adjust whilst the recognizer class listens to audio.\n",
    "\n",
    "**Instructions**<br>\n",
    "1. Import the speech_recognition library as sr.\n",
    "2. Setup an instance of the Recognizer class and save it to recognizer.\n",
    "3. Set the recognizer.energy_threshold to 300.\n",
    "\n",
    "**Results**<br>\n",
    "<font color=darkgreen>The most important step done! Now you're ready to start accessing the speech_recognition library and use the Recognizer class!</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the Recognizer class\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "# Set the energy threshold\n",
    "recognizer.energy_threshold = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02.04 Using the Recognizer class\n",
    "\n",
    "Now you've created an instance of the <code>Recognizer</code> class we'll use the <code>recognize_google()</code> method on it to access the Google web speech API and turn spoken language into text.\n",
    "\n",
    "<code>recognize_google()</code> requires an argument <code>audio_data</code> otherwise it will return an error.\n",
    "\n",
    "US English is the default language. If your audio file isn't in US English, you can change the <code>language</code> with the language argument. A list of language codes can be seen here: https://cloud.google.com/speech-to-text/docs/languages\n",
    "\n",
    "An audio file containing English speech has been imported as <code>clean_support_call_audio</code>. You can listen to the audio file here. https://assets.datacamp.com/production/repositories/4637/datasets/393a2f76d057c906de27ec57ea655cb1dc999fce/clean-support-call.wav SpeechRecognition has also been imported as <code>sr</code>.\n",
    "\n",
    "To avoid hitting the API request limit of Google's web API, we've mocked the <code>Recognizer</code> class to work with our audio files. This means some functionality will be limited.\n",
    "\n",
    "**Instructions**<br>\n",
    "1. Call the recognize_google() method on recognizer and pass it clean_support_call_audio.\n",
    "2. Set the language argument to \"en-US\".\n",
    "\n",
    "**Results**<br>\n",
    "<font color=darkgreen>Massive effort! You just transcribed your first piece of audio using speech_recognition's Recognizer class! Well, we've set it a mock version of Recognizer so we don't hit the API max requests limit. Notice how the 'hello' wasn't seperate from the rest of the text. As powerful as recognize_google() is, it doesn't have sentence separation.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language en-US:  hello I'd like to get some help setting up my account please\n"
     ]
    }
   ],
   "source": [
    "lang, file = 'en-US', 'clean-support-call.wav'\n",
    "print(f\"Language {lang}: \", get_transcription(file, lang))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02.05 Reading audio files with SpeechRecognition\n",
    "\n",
    "See the video.\n",
    "\n",
    "**Examples from the video - Duration and Offset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 2 seconds:  hello I'd like to\n",
      "After 2 seconds:  help setting up my account please\n"
     ]
    }
   ],
   "source": [
    "# Get first 2-seconds of clean support call\n",
    "print(f\"First 2 seconds: \", get_transcription(file, lang, duration=2.0))\n",
    "\n",
    "# Skip first 2-seconds of clean support call\n",
    "print(f\"After 2 seconds: \", get_transcription(file, lang, offset=2.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02.06 From AudioFile to AudioData\n",
    "\n",
    "As you saw earlier, there are some transformation steps we have to take to make our audio data useful. The same goes for SpeechRecognition.\n",
    "\n",
    "In this exercise, we'll import the <code>clean_support_call.wav</code> audio file (https://assets.datacamp.com/production/repositories/4637/datasets/393a2f76d057c906de27ec57ea655cb1dc999fce/clean-support-call.wav) and get it ready to be recognized.\n",
    "\n",
    "We first read our audio file using the <code>AudioFile</code> class. But the <code>recognize_google()</code> method requires an input of type <code>AudioData</code>.\n",
    "\n",
    "To convert our <code>AudioFile</code> to <code>AudioData</code>, we'll use the <code>Recognizer</code> class's method <code>record()</code> along with a context manager. The <code>record()</code> method takes an <code>AudioFile</code> as input and converts it to <code>AudioData</code>, ready to be used with <code>recognize_google()</code>.\n",
    "\n",
    "SpeechRecognition has already been imported as <code>sr</code>.\n",
    "\n",
    "**Instructions**<br>\n",
    "1. Pass the AudioFile class clean_support_call.wav.\n",
    "2. Use the context manager to open and read clean_support_call as source.\n",
    "3. Record source and run the code.\n",
    "\n",
    "**Results**<br>\n",
    "<font color=darkgreen>Nice! You've gone end to end with SpeechRecognition, you've imported an audio file, converted it to the right data type and transcribed it using Google's free web API! Now let's see a few more capabilities of the record() method.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language en-US:  hello I'd like to get some help setting up my account please\n"
     ]
    }
   ],
   "source": [
    "lang, file = 'en-US', 'clean-support-call.wav'\n",
    "print(f\"Language {lang}: \", get_transcription(file, lang))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02.07 Recording the audio we need\n",
    "\n",
    "Sometimes you may not want the entire audio file you're working with. The <code>duration</code> and <code>offset</code> parameters of the <code>record()</code> method can help with this.\n",
    "\n",
    "After exploring your dataset, you find there's one file, imported as <code>nothing_at_end</code> which has __30-seconds of silence at the end__ (https://assets.datacamp.com/production/repositories/4637/datasets/ca799cf2a7b093c06e1a5ae1dd96a49d48d65efa/30-seconds-of-nothing-16k.wav) and a support call file, imported as <code>out_of_warranty</code> has __3-seconds of static at the front__ (https://assets.datacamp.com/production/repositories/4637/datasets/dbc47d8210fdf8de42b0da73d1c2ba92e883b2d2/static-out-of-warranty.wav).\n",
    "\n",
    "Setting <code>duration</code> and <code>offset</code> means the <code>record()</code> method will record up to <code>duration</code> audio starting at <code>offset</code>. They're both measured in seconds.\n",
    "\n",
    "**Instructions**<br>\n",
    "1. Let's get the first 10-seconds of nothing_at_end_audio. To do this, you can set duration to 10.\n",
    "2. Let's remove the first 3-seconds of static of static_at_start by setting offset to 3.\n",
    "\n",
    "**Results**<br>\n",
    "<font color=darkgreen>That's much better! Speech recognition can be resource intensive, so in practice, you'll want to explore your audio files to make you're not wasting any compute power trying to transcribe static or silence.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration = 10 seconds:  this ODI fall has 30 seconds of nothing at the end of it\n",
      "Offset   =  3 seconds:  hello I'd like to get some help with my device please I think it's out of warranty I bought it about 2 years ago\n"
     ]
    }
   ],
   "source": [
    "# Get first 2-seconds of clean support call\n",
    "lang, file = 'en-US', '30-seconds-of-nothing-16k.wav'\n",
    "print(f\"Duration = 10 seconds: \", get_transcription(file, lang, duration=10))\n",
    "\n",
    "# Skip first 2-seconds of clean support call\n",
    "lang, file = 'en-US', 'static-out-of-warranty.wav'\n",
    "print(f\"Offset   =  3 seconds: \", get_transcription(file, lang, offset=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02.08 Dealing with different kinds of audio\n",
    "\n",
    "See the video.\n",
    "\n",
    "**Examples from the video - What language?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language en-US:  ohayo gozaimasu\n",
      "Language ja-JP:  おはようございます\n",
      "Language en-US:  is that it doesn't recognize different speakers invoices it will just return it all as one block of text\n"
     ]
    }
   ],
   "source": [
    "# Pass the audio to recognize_google\n",
    "files = ['good-morning-japanense.wav', 'good-morning-japanense.wav', 'multiple-speakers-16k.wav']\n",
    "langs = ['en-US', 'ja-JP', 'en-US']\n",
    "\n",
    "for file, lang in zip(files, langs):\n",
    "    print(f\"Language {lang}: \", get_transcription(file, lang))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Examples from the video - Showing all**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language en-US:  {'alternative': [{'transcript': 'ohayo gozaimasu', 'confidence': 0.65199554}, {'transcript': 'ohayogozaimas'}], 'final': True}\n",
      "Language en-US:  []\n"
     ]
    }
   ],
   "source": [
    "files = ['good-morning-japanense.wav', 'leopard.wav']\n",
    "langs = ['en-US', 'en-US']\n",
    "\n",
    "for file, lang in zip(files, langs):\n",
    "    print(f\"Language {lang}: \", get_transcription(file, lang, show_all=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Examples from the video - Multiple speakers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text from speaker 0:  one of the limitation of the speech recognition Library\n",
      "Text from speaker 1:  is that it doesn't recognize different speakers invoices\n",
      "Text from speaker 2:  it will just return it all as one block a text\n"
     ]
    }
   ],
   "source": [
    "files = ['speaker_0.wav', 'speaker_1.wav', 'speaker_2.wav']\n",
    "langs = ['en-US', 'en-US', 'en-US']\n",
    "\n",
    "for i, (f, l) in enumerate(zip(files, langs)):\n",
    "    print(f\"Text from speaker {i}: \", get_transcription(f, l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Examples from the video - Noisy audio**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language en-US:  hello I'd like to get to help setting up my account\n"
     ]
    }
   ],
   "source": [
    "# Import audio file with background nosie\n",
    "lang, file = 'en-US', '2-noisy-support-call.wav'\n",
    "print(f\"Language {lang}: \", get_transcription(file, lang, noise=0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02.09 Different kinds of audio\n",
    "\n",
    "Now you've seen an example of how the <code>Recognizer</code> class works. Let's try a few more. How about speech from a different language?\n",
    "\n",
    "What do you think will happen when we call the <code>recognize_google()</code> function on a Japanese version (https://assets.datacamp.com/production/repositories/4637/datasets/cd9b801670d0664275cdbd3a24b6b70a8c2e5222/good-morning-japanense.wav) of good_morning.wav (<code>japanese_audio</code>)?\n",
    "\n",
    "The default language is <code>\"en-US\"</code>, are the results the same with the <code>\"ja\"</code> tag?\n",
    "\n",
    "How about non-speech audio? Like this leopard roaring (https://assets.datacamp.com/production/repositories/4637/datasets/5720832b2735089d8e735cac3e0b0ad9b5114864/leopard.wav) (<code>leopard_audio</code>).\n",
    "\n",
    "Or speech where the sounds may not be real words, such as a baby talking (https://campus.datacamp.com/courses/spoken-language-processing-in-python/using-the-python-speechrecognition-library?ex=9) (<code>charlie_audio</code>)?\n",
    "\n",
    "To familiarize more with the <code>Recognizer</code> class, we'll look at an example of each of these.\n",
    "\n",
    "**Instructions**<br>\n",
    "1. Pass the Japanese version of good morning (japanese_audio) to recognize_google() using \"en-US\" as the language.\n",
    "2. Pass the same Japanese audio (japanese_audio) using \"ja\" as the language parameter. Do you see a difference?\n",
    "3. What about about non-speech audio? Pass leopard_audio to recognize_google() with show_all as True.\n",
    "4. What if your speech files have non-audible human sounds? Pass charlie_audio to recognize_google() to find out.\n",
    "\n",
    "**Results**<br>\n",
    "<font color=darkgreen>What an effort! You've seen how the recognize_google() deals with different kinds of audio. It's worth noting the recognize_google() function is only going to return words, as in, it didn't return the baby saying 'ahhh!' because it doesn't recognize it as a word. Speech recognition has come a long way but it's far from perfect. Let's push on!</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language en-US:  {'alternative': [{'transcript': 'ohayo gozaimasu', 'confidence': 0.65199554}, {'transcript': 'ohayogozaimas'}], 'final': True}\n",
      "Language ja-JP:  {'alternative': [{'transcript': 'おはようございます', 'confidence': 0.89717746}, {'transcript': 'あおはようございます'}, {'transcript': 'おはようございまーす'}, {'transcript': 'あおはようございまーす'}], 'final': True}\n",
      "Language en-US:  []\n",
      "Language en-US:  {'alternative': [{'transcript': 'charlie bit me', 'confidence': 0.59338063}, {'transcript': 'Charlie Batman'}, {'transcript': 'Batman'}, {'transcript': 'surely Batman'}, {'transcript': 'Shirley Batman'}], 'final': True}\n"
     ]
    }
   ],
   "source": [
    "# Pass the audio to recognize_google\n",
    "files = ['good-morning-japanense.wav', 'good-morning-japanense.wav', 'leopard.wav', 'charlie-bit-me-5.wav']\n",
    "langs = ['en-US', 'ja-JP', 'en-US', 'en-US']\n",
    "\n",
    "for file, lang in zip(files, langs):\n",
    "    print(f\"Language {lang}: \", get_transcription(file, lang, show_all=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02.10 Multiple Speakers 1\n",
    "\n",
    "If your goal is to transcribe conversations, there will be more than one speaker. However, as you'll see, the <code>recognize_google()</code> function will only transcribe speech into a single block of text.\n",
    "\n",
    "You can hear in __this audio file__ (https://assets.datacamp.com/production/repositories/4637/datasets/925c8c31d6e4af9c291c692f13e4f41c7b5e86b2/multiple-speakers-16k.wav) there are three different speakers.\n",
    "\n",
    "But if you transcribe it on its own, <code>recognize_google()</code> returns a single block of text. Which is still useful but it doesn't let you know which speaker said what.\n",
    "\n",
    "We'll see an alternative to this in the next exercise.\n",
    "\n",
    "The multiple speakers audio file has been imported and converted to <code>AudioData</code> as <code>multiple_speakers</code>.\n",
    "\n",
    "**Instructions**<br>\n",
    "1. Create an instance of Recognizer.\n",
    "2. Recognize the multiple_speakers variable using the recognize_google() function.\n",
    "3. Set the language to US English (\"en-US\").\n",
    "\n",
    "**Results**<br>\n",
    "<font color=darkgreen>Boom! You did it. But see how all of the speakers speech came out in one big block of text? In the next exercise we'll see a way of working around this.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language en-US:  is that it doesn't recognize different speakers invoices it will just return it all as one block of text\n"
     ]
    }
   ],
   "source": [
    "lang, file = 'en-US', 'multiple-speakers-16k.wav'\n",
    "print(f\"Language {lang}: \", get_transcription(file, lang, noise=0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02.11 Multiple Speakers \n",
    "\n",
    "Deciphering between multiple speakers in one audio file is called speaker diarization. However, you've seen the free function we've been using, <code>recognize_google()</code> doesn't have the ability to transcribe different speakers.\n",
    "\n",
    "One way around this, without using one of the paid speech to text services, is to ensure your audio files are single speaker.\n",
    "\n",
    "This means if you were working with phone call data, you would make sure the caller and receiver are recorded separately. Then you could transcribe each file individually.\n",
    "\n",
    "In this exercise, we'll transcribe each of the speakers in our multiple speakers audio file individually.\n",
    "\n",
    "**Instructions**<br>\n",
    "1. Pass speakers to the enumerate() function to loop through the different speakers.\n",
    "2. Call record() on recognizer to convert the AudioFiles into AudioData.\n",
    "3. Use recognize_google() to transcribe each of the speaker_audio objects.\n",
    "\n",
    "**Results**<br>\n",
    "<font color=darkgreen>Nice work! Something to remember is I had to manually split the audio file into different speakers. You can see this solution still isn't perfect but it's easier to deal with than having a single block of text. You could think about automating this process in the future by having a model split the audio when it detects different speakers. For now, let's look into what happens when you've got noisy audio!</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text from speaker 0:  one of the limitation of the speech recognition Library\n",
      "Text from speaker 1:  is that it doesn't recognize different speakers invoices\n",
      "Text from speaker 2:  it will just return it all as one block a text\n"
     ]
    }
   ],
   "source": [
    "files = ['speaker_0.wav', 'speaker_1.wav', 'speaker_2.wav']\n",
    "lang = 'en-US'\n",
    "\n",
    "for i, audio in enumerate(files):\n",
    "    print(f\"Text from speaker {i}: \", get_transcription(audio, lang))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02.12 Working with noisy audio\n",
    "\n",
    "In this exercise, we'll start by transcribing a clean speech sample to text and then see what happens when we add some background noise.\n",
    "\n",
    "A clean audio sample has been imported as <code>clean_support_call</code>.\n",
    "\n",
    "Play clean support call (https://assets.datacamp.com/production/repositories/4637/datasets/393a2f76d057c906de27ec57ea655cb1dc999fce/clean-support-call.wav).\n",
    "\n",
    "We'll then do the same with the noisy audio file saved as <code>noisy_support_call</code>. It has the same speech as <code>clean_support_call</code> but with additional background noise.\n",
    "\n",
    "Play noisy support call (https://assets.datacamp.com/production/repositories/4637/datasets/f3edd5024944eac2f424b592840475890c86d405/2-noisy-support-call.wav).\n",
    "\n",
    "To try and negate the background noise, we'll take advantage of <code>Recognizer</code>'s <code>adjust_for_ambient_noise()</code> function.\n",
    "\n",
    "**Instructions**<br>\n",
    "1. Let's transcribe some clean audio. Read in clean_support_call as the source and call recognize_google() on the file.\n",
    "2. Let's do the same as before but with a noisy audio file saved as noisy_support_call and show_all parameter as True.\n",
    "3. Set the duration parameter of adjust_for_ambient_noise() to 1 (second) so recognizer adjusts for background noise.\n",
    "4. A duration of 1 was too long and it cut off some of the audio. Try setting duration to 0.5.\n",
    "\n",
    "**Results**<br>\n",
    "<font color=darkgreen>Well, the results still weren't perfect. This should be expected with some audio files though, sometimes the background noise is too much. If your audio files have a large amount of background noise, you may need to preprocess them with an audio tool such as Audacity before using them with speech_recognition.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ex.0:  hello I'd like to get some help setting up my account please\n",
      "Ex.1:  {'alternative': [{'transcript': \"hello I'd like to get to help setting up my account\", 'confidence': 0.89344984}, {'transcript': \"hello I'd like to get some help setting up my account\"}, {'transcript': \"hello I'd like to get to help thinning out my account\"}, {'transcript': \"hello I'd like to get to help setting up my account.\"}, {'transcript': \"hello I'd like to get to help setting up my calendar\"}], 'final': True}\n",
      "Ex.2:  {'alternative': [{'transcript': \"hello I'd like to get to help setting up my account\", 'confidence': 0.89344972}, {'transcript': \"hello I'd like to get some help setting up my account\"}, {'transcript': \"hello I'd like to get to help thinning out my account\"}, {'transcript': \"hello I'd like to get to help setting up my account.\"}, {'transcript': \"hello I'd like to get to help setting up my calendar\"}], 'final': True}\n",
      "Ex.3:  {'alternative': [{'transcript': \"hello I'd like to get to help setting up my account\", 'confidence': 0.89344984}, {'transcript': \"hello I'd like to get some help setting up my account\"}, {'transcript': \"hello I'd like to get to help thinning out my account\"}, {'transcript': \"hello I'd like to get to help setting up my account.\"}, {'transcript': \"hello I'd like to get to help setting up my calendar\"}], 'final': True}\n"
     ]
    }
   ],
   "source": [
    "# Pass the audio to recognize_google\n",
    "files = ['clean-support-call.wav', '2-noisy-support-call.wav', '2-noisy-support-call.wav', '2-noisy-support-call.wav']\n",
    "show_all = [False, True, True, True]\n",
    "noise = [0, 0, 1, 0.5]\n",
    "lang = 'en-US'\n",
    "\n",
    "for i, (f, s, n) in enumerate(zip(files, show_all, noise)):\n",
    "    print(f\"Ex.{i}: \", get_transcription(f, lang, show_all=s, noise=n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aditional material\n",
    "\n",
    "- **Online Voice Recorder & Audio Cutter**: https://voice-recorder-online.com/\n",
    "- **For more details on available language models in \"speech_recognition\" python module**: https://cloud.google.com/speech-to-text/docs/languages\n",
    "- **Datacamp course**: https://learn.datacamp.com/courses/spoken-language-processing-in-python"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
