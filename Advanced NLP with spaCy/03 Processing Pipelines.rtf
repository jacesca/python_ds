{\rtf1\ansi\ansicpg1252\deff0\nouicompat\deflang2058{\fonttbl{\f0\fnil\fcharset0 Calibri;}{\f1\fnil\fcharset0 Courier New;}{\f2\fnil Calibri;}{\f3\fnil Courier New;}{\f4\fnil\fcharset2 Symbol;}}
{\colortbl ;\red255\green0\blue0;\red0\green77\blue187;\red0\green176\blue80;\red0\green0\blue255;}
{\*\generator Riched20 10.0.18362}\viewkind4\uc1 
\pard\sl240\slmult1\tx568\cf1\f0\fs22\lang10 __________________________________________________________________________________\par

\pard\sl240\slmult1\qj\b\fs32 03. Processing Pipelines\par
\cf0\b0\fs22 This chapter will show you to everything you need to know about spaCy's processing pipeline. You'll learn what goes on under the hood when you process a text, how to write your own components and add them to the pipeline, and how to use custom attributes to add your own meta data to the documents, spans and tokens.\cf1\b\fs20\par

\pard\sl240\slmult1\tx568\b0\fs22 __________________________________________________________________________________\par

\pard\sl240\slmult1\qj\cf2\b 03.01 Processing pipelines\par

\pard\li720\sl240\slmult1\qj\cf0\b0\fs20\lang10 See the vvideo.\fs22\par

\pard\sl240\slmult1\tx568\cf1 __________________________________________________________________________________\par

\pard\sl240\slmult1\qj\cf2\b 03.02 What happens when you call nlp?\par

\pard\li720\sl240\slmult1\qj\cf0\b0\lang10 ___________________________________________________________________________\par

\pard\li720\sl240\slmult1\b INSTRUCTIONS:\par

\pard\li710\sl240\slmult1\qj\b0\fs20 What does spaCy do when you call nlp on a string of text? The IPython shell has a pre-loaded nlp object that logs what's going on under the hood. Try processing a text with it!.\par
\par

\pard\li1440\sl240\slmult1\f1\fs16 doc = nlp("This is a sentence.")\par

\pard\li720\sl240\slmult1\qj\f0\fs22 ___________________________________________________________________________\par

\pard\li720\sl240\slmult1\b ANSWER:\par

\pard\li1440\sl240\slmult1\f1\fs20 POSIBLLE ANSWERS\par

\pard{\pntext\f4\'B7\tab}{\*\pn\pnlvlblt\pnf4\pnindent0{\pntxtb\'B7}}\fi-360\li2160\sl240\slmult1\qj\b0\f0 Run the tagger, parser and entity recognizer and then the tokenizer.\par
{\pntext\f4\'B7\tab}\cf1 Tokenize the text and apply each pipeline component in order.\par
{\pntext\f4\'B7\tab}\cf0 Connect to the spaCy server to compute the result and return it.\par
{\pntext\f4\'B7\tab}Initialize the language, add the pipeline and load in the binary model weights\par

\pard\li1440\sl240\slmult1\b\f1 CORRECT ANSWER\par

\pard{\pntext\f4\'B7\tab}{\*\pn\pnlvlblt\pnf4\pnindent0{\pntxtb\'B7}}\fi-360\li2160\sl240\slmult1\qj\cf1\b0\f0 Tokenize the text and apply each pipeline component in order.\cf0\f1\fs16\par

\pard\li720\sl240\slmult1\qj\f0\fs22 ___________________________________________________________________________\par

\pard\li720\sl240\slmult1\b IPYTHON SHELL:\par
\b0\f1\fs14 In [2]: nlp\par
Out[2]: <function __main__.nlp>\par
\par
In [3]: import inspect\par
\par
In [4]: inspect.getsource(nlp)\par
Out[4]: '\par

\pard\li720\sl240\slmult1 def nlp(text):\par
    print("Using model '\{\}' (language '\{\}' and pipeline \{\})".format(model, orig_nlp.lang, \par
                                                                    orig_nlp.pipe_names))\par
    print("Tokenizing text: \{\}".format(text))\par
    doc = orig_nlp.make_doc(text)\par
    for name, proc in orig_nlp.pipeline:\par
       print("Calling pipeline component '\{\}' on Doc".format(name))\par
       doc = proc(doc)\par
    print('Returning processed Doc')\par
    return doc\par
'\par
\par
In [5]: model\par
Out[5]: 'en_core_web_sm'\par
\par
In [6]: doc = nlp("This is a sentence")\par
Using model 'en_core_web_sm' (language 'en' and pipeline ['tagger', 'parser', 'ner'])\par
Tokenizing text: This is a sentence\par
Calling pipeline component 'tagger' on Doc\par
Calling pipeline component 'parser' on Doc\par
Calling pipeline component 'ner' on Doc\par
Returning processed Doc\par
\par
In [7]: doc.text\par
Out[7]: 'This is a sentence'\par

\pard\li720\sl240\slmult1\qj\f0\fs22 ___________________________________________________________________________\par
\cf3\i\fs20 Correct! The tokenizer turns a string of text into a Doc object. spaCy then applies every component in the pipeline on document, in order.\cf0\i0\f1\fs16\par

\pard\sl240\slmult1\tx568\cf1\f0\fs22 __________________________________________________________________________________\par

\pard\sl240\slmult1\qj\cf2\b 03.03 Inspecting the pipeline\par

\pard\li720\sl240\slmult1\qj\cf0\b0\fs20\lang10 Let's inspect the small English model's pipeline!\fs22\par
___________________________________________________________________________\par

\pard\li720\sl240\slmult1\b INSTRUCTIONS:\par

\pard{\pntext\f4\'B7\tab}{\*\pn\pnlvlblt\pnf4\pnindent0{\pntxtb\'B7}}\fi-360\li1080\sl240\slmult1\qj\b0\fs20 Load the en_core_web_sm model and create the nlp object.\par
{\pntext\f4\'B7\tab}Print the names of the pipeline components using nlp.pipe_names.\par
{\pntext\f4\'B7\tab}Print the full pipeline of (name, component) tuples using nlp.pipeline.\par

\pard\li720\sl240\slmult1\qj\fs22 ___________________________________________________________________________\par

\pard\li720\sl240\slmult1\b ANSWER:\par
\b0\f1\fs16 # Load the en_core_web_sm model\par
nlp = spacy.load('en_core_web_sm')\par
\par
# Print the names of the pipeline components\par
print(nlp.pipe_names)\par
\par
# Print the full pipeline of (name, component) tuples\par
print(nlp.pipeline)\par

\pard\li720\sl240\slmult1\qj\f0\fs22 ___________________________________________________________________________\par

\pard\li720\sl240\slmult1\b RESULT:\par
\b0\f1\fs16 ['tagger', 'parser', 'ner']\par
[('tagger', <spacy.pipeline.pipes.Tagger object at 0x7eff9c2c3e10>), ('parser', <spacy.pipeline.pipes.DependencyParser object at 0x7eff9c176648>), ('ner', <spacy.pipeline.pipes.EntityRecognizer object at 0x7eff9c1766a8>)]\b\par

\pard\li720\sl240\slmult1\qj\b0\f0\fs22 ___________________________________________________________________________\par
\cf3\i\fs20 Well done! Whenever you're unsure about the current pipeline, you can inspect it by printing nlp.pipe_names or nlp.pipeline.\cf0\i0\f1\fs16\par

\pard\sl240\slmult1\tx568\cf1\f0\fs22 __________________________________________________________________________________\par

\pard\sl240\slmult1\qj\cf2\b 03.04 Custom pipeline components\par

\pard\li720\sl240\slmult1\qj\cf0\b0\fs20\lang10 See the video.\fs22\par

\pard\sl240\slmult1\tx568\cf1 __________________________________________________________________________________\par

\pard\sl240\slmult1\qj\cf2\b 03.05 Use cases for custom components\par

\pard\li720\sl240\slmult1\qj\cf0\b0\lang10 ___________________________________________________________________________\par

\pard\li720\sl240\slmult1\b INSTRUCTIONS:\par

\pard\li720\sl240\slmult1\qj\b0\fs20 Which of these problems can be solved by custom pipeline components? Choose all that apply!\par

\pard 
{\pntext\f0 1.\tab}{\*\pn\pnlvlbody\pnf0\pnindent0\pnstart1\pndec{\pntxta.}}
\fi-360\li1080\sl240\slmult1\qj updating the pre-trained models and improving their predictions\par
{\pntext\f0 2.\tab}\cf1 computing your own values based on tokens and their attributes\par
{\pntext\f0 3.\tab}adding named entities, for example based on a dictionary\par
{\pntext\f0 4.\tab}\cf0 implementing support for an additional language\par

\pard\li720\sl240\slmult1\qj\fs22 ___________________________________________________________________________\par

\pard\li720\sl240\slmult1\b ANSWER:\par

\pard\li1440\sl240\slmult1\f1\fs20 POSIBLLE ANSWERS\par

\pard{\pntext\f4\'B7\tab}{\*\pn\pnlvlblt\pnf4\pnindent0{\pntxtb\'B7}}\fi-360\li2160\sl240\slmult1\qj\b0\f0 1 and 2.\par
{\pntext\f4\'B7\tab}1 and 3.\par
{\pntext\f4\'B7\tab}1 and 4.\par
{\pntext\f4\'B7\tab}\cf1 2 and 3.\par
{\pntext\f4\'B7\tab}\cf0 2 and 4.\par
{\pntext\f4\'B7\tab}3 and 4.\par

\pard\li1440\sl240\slmult1\b\f1 CORRECT ANSWER\par

\pard{\pntext\f4\'B7\tab}{\*\pn\pnlvlblt\pnf4\pnindent0{\pntxtb\'B7}}\fi-360\li2160\sl240\slmult1\qj\cf1\b0\f0 2 and 3.\par

\pard\li720\sl240\slmult1\qj\cf0\fs22 ___________________________________________________________________________\par
\cf3\i\fs20 Custom components are great for adding custom values to documents, tokens and spans, and customizing the doc.ents.\cf0\i0\f1\fs16\par

\pard\sl240\slmult1\tx568\cf1\f0\fs22 __________________________________________________________________________________\par

\pard\sl240\slmult1\qj\cf2\b 03.06 Simple components\par

\pard\li720\sl240\slmult1\qj\cf0\b0\fs20\lang10 The example shows a custom component that prints the character length of a document. Can you complete it? spacy has already been imported for you.\fs22\par
___________________________________________________________________________\par

\pard\li720\sl240\slmult1\b INSTRUCTIONS:\par

\pard{\pntext\f4\'B7\tab}{\*\pn\pnlvlblt\pnf4\pnindent0{\pntxtb\'B7}}\fi-360\li1080\sl240\slmult1\qj\b0\fs20 Complete the component function with the doc's length.\par
{\pntext\f4\'B7\tab}Add the length_component to the existing pipeline as the first component.\par
{\pntext\f4\'B7\tab}Try out the new pipeline and process any text with the nlp object \f2\endash  for example "This is a sentence.".\f0\par

\pard\li720\sl240\slmult1\qj\fs22 ___________________________________________________________________________\par

\pard\li720\sl240\slmult1\b ANSWER:\par
\b0\f1\fs16 # Define the custom component\par
def length_component(doc):\par
    # Get the doc's length\par
    doc_length = len(doc)\par
    print("This document is \{\} tokens long.".format(doc_length))\par
    # Return the doc\par
    return doc\par
\par
# Add the component first in the pipeline and print the pipe names\par
nlp.add_pipe(length_component, first=True)\par
print(nlp.pipe_names)\par
\par
# Process a text\par
doc = nlp("This is a sentence.")\par

\pard\li720\sl240\slmult1\qj\f0\fs22 ___________________________________________________________________________\par

\pard\li720\sl240\slmult1\b RESULT:\par
\b0\f1\fs16 ['length_component', 'tagger', 'parser', 'ner']\par
\par
This document is 5 tokens long.\b\par

\pard\li720\sl240\slmult1\qj\b0\f0\fs22 ___________________________________________________________________________\par
\cf3\i\fs20 Perfect! Now let's take a look at a slightly more complex component!\cf0\i0\f1\fs16\par

\pard\sl240\slmult1\tx568\cf1\f0\fs22 __________________________________________________________________________________\par

\pard\sl240\slmult1\qj\cf2\b 03.07 Complex components\par

\pard\li720\sl240\slmult1\qj\cf0\b0\fs20\lang10 In this exercise, you'll be writing a custom component that uses the PhraseMatcher to find animal names in the document and adds the matched spans to the doc.ents.\par
A PhraseMatcher with the animal patterns has already been created as the variable matcher. The small English model is available as the variable nlp. The Span object has already been imported for you.\par
\fs22\par

\pard\li720\sl240\slmult1\qc\f1\fs16 animal_patterns = [Golden Retriever, cat, turtle, Rattus norvegicus]\par

\pard\li720\sl240\slmult1\qj\f0\fs22 ___________________________________________________________________________\par

\pard\li720\sl240\slmult1\b INSTRUCTIONS:\par

\pard{\pntext\f4\'B7\tab}{\*\pn\pnlvlblt\pnf4\pnindent0{\pntxtb\'B7}}\fi-360\li1080\sl240\slmult1\qj\b0\fs20 Define the custom component and apply the matcher to the doc.\par
{\pntext\f4\'B7\tab}Create a Span for each match, assign the label ID for 'ANIMAL' and overwrite the doc.ents with the new spans.\par

\pard\sl240\slmult1\qj\par

\pard{\pntext\f4\'B7\tab}{\*\pn\pnlvlblt\pnf4\pnindent0{\pntxtb\'B7}}\fi-360\li1080\sl240\slmult1\qj Add the new component to the pipeline after the 'ner' component.\par

\pard\sl240\slmult1\qj\par

\pard{\pntext\f4\'B7\tab}{\*\pn\pnlvlblt\pnf4\pnindent0{\pntxtb\'B7}}\fi-360\li1080\sl240\slmult1\qj Process the text and print the entity text and entity label for the entities in doc.ents.\par

\pard\li720\sl240\slmult1\qj\fs22 ___________________________________________________________________________\par

\pard\li720\sl240\slmult1\b ANSWER:\par
\b0\f1\fs16 # Define the custom component\par
def animal_component(doc):\par
    # Apply the matcher to the doc\par
    matches = matcher(doc)\par
    # Create a Span for each match and assign the label 'ANIMAL'\par
    spans = [Span(doc, start, end, label='ANIMAL')\par
             for match_id, start, end in matches]\par
    # Overwrite the doc.ents with the matched spans\par
    doc.ents = spans\par
    return doc\par
\par
# Add the component to the pipeline after the 'ner' component \par
nlp.add_pipe(animal_component, after='ner')\par
print(nlp.pipe_names)\par
\par
# Process the text and print the text and label for the doc.ents\par
doc = nlp("I have a cat and a Golden Retriever")\par
print([(ent.text, ent.label_) for ent in doc.ents])\par

\pard\li720\sl240\slmult1\qj\f0\fs22 ___________________________________________________________________________\par

\pard\li720\sl240\slmult1\b RESULT:\par
\b0\f1\fs16\par
['tagger', 'parser', 'ner', 'animal_component']\par
\par
[('cat', 'ANIMAL'), ('Golden Retriever', 'ANIMAL')]\b\par

\pard\li720\sl240\slmult1\qj\b0\f0\fs22 ___________________________________________________________________________\par
\cf3\i\fs20 Good job! You've built your first pipeline component for rule-based entity matching.\cf0\i0\f1\fs16\par

\pard\sl240\slmult1\tx568\cf1\f0\fs22 __________________________________________________________________________________\par

\pard\sl240\slmult1\qj\cf2\b 03.08 Extension attributes\par

\pard\li720\sl240\slmult1\qj\cf0\b0\fs20\lang10 See the video.\fs22\par

\pard\sl240\slmult1\tx568\cf1 __________________________________________________________________________________\par

\pard\sl240\slmult1\qj\cf2\b 03.09 Setting extension attributes (1)\par

\pard\li720\sl240\slmult1\qj\cf0\b0\fs20\lang10 Let's practice setting some extension attributes. The nlp object has already been created for you and the Doc, Token and Span classes are already imported.\par
Remember that if you run your code more than once, you might see an error message that the extension already exists. That's because DataCamp will re-run your code in the same session. To solve this, you can set force=True on set_extension, or reload to start a new Python session. None of this will affect the answer you submit.\fs22\par
___________________________________________________________________________\par

\pard\li720\sl240\slmult1\b INSTRUCTIONS:\par

\pard{\pntext\f4\'B7\tab}{\*\pn\pnlvlblt\pnf4\pnindent0{\pntxtb\'B7}}\fi-360\li1080\sl240\slmult1\qj\b0\fs20 Use Token.set_extension to register is_country (default False).\par
{\pntext\f4\'B7\tab}Update it for "Spain" and print it for all tokens.\par

\pard\sl240\slmult1\qj\par

\pard{\pntext\f4\'B7\tab}{\*\pn\pnlvlblt\pnf4\pnindent0{\pntxtb\'B7}}\fi-360\li1080\sl240\slmult1\qj Use Token.set_extension to register 'reversed' (getter function get_reversed).\par
{\pntext\f4\'B7\tab}Print its value for each token.\par

\pard\li720\sl240\slmult1\qj\fs22 ___________________________________________________________________________\par

\pard\li720\sl240\slmult1\b ANSWER:\par
\b0\f1\fs16 # Register the Token extension attribute 'is_country' with the default value False\par
Token.set_extension('is_country', default=False, force=True)\par
\par
# Process the text and set the is_country attribute to True for the token "Spain"\par
doc = nlp("I live in Spain.")\par
doc[3]._.is_country = True\par
\par
# Print the token text and the is_country attribute for all tokens\par
print([(token.text, token._.is_country) for token in doc])\par
\par
\par
# Define the getter function that takes a token and returns its reversed text\par
def get_reversed(token):\par
    return token.text[::-1]\par
  \par
# Register the Token property extension 'reversed' with the getter get_reversed\par
Token.set_extension('reversed', getter=get_reversed, force=True)\par
\par
# Process the text and print the reversed attribute for each token\par
doc = nlp("All generalizations are false, including this one.")\par
for token in doc:\par
    print('reversed:', token._.reversed)\par

\pard\li720\sl240\slmult1\qj\f0\fs22 ___________________________________________________________________________\par

\pard\li720\sl240\slmult1\b RESULT:\par
\b0\f1\fs16 [(I, False), (live, False), (in, False), (Spain, True), (., False)]\par
\par
reversed: llA\par
reversed: snoitazilareneg\par
reversed: era\par
reversed: eslaf\par
reversed: ,\par
reversed: gnidulcni\par
reversed: siht\par
reversed: eno\par
reversed: .\b\par

\pard\li720\sl240\slmult1\qj\b0\f0\fs22 ___________________________________________________________________________\par
\cf3\i\fs20 Great work!\cf0\i0\f1\fs16\par

\pard\sl240\slmult1\tx568\cf1\f0\fs22 __________________________________________________________________________________\par

\pard\sl240\slmult1\qj\cf2\b 03.10 Setting extension attributes (2)\par

\pard\li720\sl240\slmult1\qj\cf0\b0\fs20\lang10 Let's try setting some more complex attributes using getters and method extensions. The nlp object has already been created for you and the Doc, Token and Span classes are already imported.\par
Remember that if you run your code more than once, you might see an error message that the extension already exists. That's because DataCamp will re-run your code in the same session. To solve this, you can set force=True on set_extension, or reload to start a new Python session. None of this will affect the answer you submit.\fs22\par
___________________________________________________________________________\par

\pard\li720\sl240\slmult1\b INSTRUCTIONS:\par

\pard{\pntext\f4\'B7\tab}{\*\pn\pnlvlblt\pnf4\pnindent0{\pntxtb\'B7}}\fi-360\li1080\sl240\slmult1\qj\b0\fs20 Complete the has_number function .\par
{\pntext\f4\'B7\tab}Use Doc.set_extension to register 'has_number' (getter get_has_number) and print its value.\par

\pard\sl240\slmult1\qj\par

\pard{\pntext\f4\'B7\tab}{\*\pn\pnlvlblt\pnf4\pnindent0{\pntxtb\'B7}}\fi-360\li1080\sl240\slmult1\qj Use Span.set_extension to register 'to_html' (method to_html).\par
{\pntext\f4\'B7\tab}Call it on doc[0:2] with the tag 'strong'.\par

\pard\li720\sl240\slmult1\qj\fs22 ___________________________________________________________________________\par

\pard\li720\sl240\slmult1\b ANSWER:\par
\b0\f1\fs16 # Define the getter function\par
def get_has_number(doc):\par
    # Return if any of the tokens in the doc return True for token.like_num\par
    return any(token.like_num for token in doc)\par
\par
# Register the Doc property extension 'has_number' with the getter get_has_number\par
Doc.set_extension('has_number', getter=get_has_number, force=True)\par
\par
# Process the text and check the custom has_number attribute \par
doc = nlp("The museum closed for five years in 2012.")\par
print('has_number:', doc._.has_number)\par
\par
\par
\par
# Define the method\par
def to_html(span, tag):\par
    # Wrap the span text in a HTML tag and return it\par
    return '<\{tag\}>\{text\}</\{tag\}>'.format(tag=tag, text=span.text)\par
\par
# Register the Span property extension 'to_html' with the method to_html\par
Span.set_extension('to_html', method=to_html, force=True)\par
\par
# Process the text and call the to_html method on the span with the tag name 'strong'\par
doc = nlp("Hello world, this is a sentence.")\par
span = doc[0:2]\par
print(span._.to_html('strong'))\par

\pard\li720\sl240\slmult1\qj\f0\fs22 ___________________________________________________________________________\par

\pard\li720\sl240\slmult1\b RESULT:\par
\b0\f1\fs16 has_number: True\par
\par
<strong>Hello world</strong>\b\par

\pard\li720\sl240\slmult1\qj\b0\f0\fs22 ___________________________________________________________________________\par
\cf3\i\fs20 Great work! In the next exercise, you'll get to combine custom attributes with custom pipeline components.\cf0\i0\f1\fs16\par

\pard\sl240\slmult1\tx568\cf1\f0\fs22 __________________________________________________________________________________\par

\pard\sl240\slmult1\qj\cf2\b 03.11 Entities and extensions\par

\pard\li720\sl240\slmult1\qj\cf0\b0\fs20\lang10 In this exercise, you'll combine custom extension attributes with the model's predictions and create an attribute getter that returns a Wikipedia search URL if the span is a person, organization, or location.\par
The Span class is already imported and the nlp object has been created for you.\fs22\par
___________________________________________________________________________\par

\pard\li720\sl240\slmult1\b INSTRUCTIONS:\par

\pard{\pntext\f4\'B7\tab}{\*\pn\pnlvlblt\pnf4\pnindent0{\pntxtb\'B7}}\fi-360\li1080\sl240\slmult1\qj\b0\fs20 Complete the get_wikipedia_url getter so it only returns the URL if the span's label is in the list of labels.\par
{\pntext\f4\'B7\tab}Set the Span extension 'wikipedia_url' using the getter get_wikipedia_url.\par
{\pntext\f4\'B7\tab}Iterate over the entities in the doc and output their Wikipedia URL.\par

\pard\li720\sl240\slmult1\qj\fs22 ___________________________________________________________________________\par

\pard\li720\sl240\slmult1\b ANSWER:\par
\b0\f1\fs16 def get_wikipedia_url(span):\par
    # Get a Wikipedia URL if the span has one of the labels\par
    if span.label_ in ('PERSON', 'ORG', 'GPE', 'LOCATION'):\par
        entity_text = span.text.replace(' ', '_')\par
        return "{{\field{\*\fldinst{HYPERLINK https://en.wikipedia.org/w/index.php?search= }}{\fldrslt{https://en.wikipedia.org/w/index.php?search=\ul0\cf0}}}}\f1\fs16 " + entity_text\par
\par
# Set the Span extension wikipedia_url using get getter get_wikipedia_url\par
Span.set_extension('wikipedia_url', getter=get_wikipedia_url, force=True)\par
\par
doc = nlp("In over fifty years from his very first recordings right through to his last album, David Bowie was at the vanguard of contemporary culture.")\par
for ent in doc.ents:\par
    # Print the text and Wikipedia URL of the entity\par
    print(ent.text, ent.label_, ent._.wikipedia_url)\par

\pard\li720\sl240\slmult1\qj\f0\fs22 ___________________________________________________________________________\par

\pard\li720\sl240\slmult1\b RESULT:\par
\b0\f1\fs16 over fifty years DATE None\par
first ORDINAL None\par
David Bowie PERSON {{\field{\*\fldinst{HYPERLINK https://en.wikipedia.org/w/index.php?search=David_Bowie }}{\fldrslt{https://en.wikipedia.org/w/index.php?search=David_Bowie\ul0\cf0}}}}\b\f1\fs16\par

\pard\li720\sl240\slmult1\qj\b0\f0\fs22 ___________________________________________________________________________\par
\cf3\i\fs20 Nice! You now have a pipeline component that uses named entities predicted by the model to generate Wikipedia URLs and adds them as a custom attribute. Try opening the link in your browser to see what happens!\cf0\i0\f1\fs16\par

\pard\sl240\slmult1\tx568\cf1\f0\fs22 __________________________________________________________________________________\par

\pard\sl240\slmult1\qj\cf2\b 03.12 Components with extensions\par

\pard\li720\sl240\slmult1\qj\cf0\b0\fs20\lang10 Extension attributes are especially powerful if they're combined with custom pipeline components. In this exercise, you'll write a pipeline component that finds country names and a custom extension attribute that returns a country's capital, if available.\par
The nlp object has already been created and the Span class is already imported. A phrase matcher with all countries is available as the variable matcher. A dictionary of countries mapped to their capital cities is available as the variable capitals.\fs22\par
___________________________________________________________________________\par

\pard\li720\sl240\slmult1\b INSTRUCTIONS:\par

\pard{\pntext\f4\'B7\tab}{\*\pn\pnlvlblt\pnf4\pnindent0{\pntxtb\'B7}}\fi-360\li1080\sl240\slmult1\qj\b0\fs20 Complete the countries_component and create a Span with the label 'GPE' (geopolitical entity) for all matches.\par
{\pntext\f4\'B7\tab}Add the component to the pipeline.\par

\pard\sl240\slmult1\qj\par

\pard{\pntext\f4\'B7\tab}{\*\pn\pnlvlblt\pnf4\pnindent0{\pntxtb\'B7}}\fi-360\li1080\sl240\slmult1\qj Register the Span extension attribute 'capital' with the getter get_capital.\par

\pard\sl240\slmult1\qj\par

\pard{\pntext\f4\'B7\tab}{\*\pn\pnlvlblt\pnf4\pnindent0{\pntxtb\'B7}}\fi-360\li1080\sl240\slmult1\qj Process the text and print the entity text, entity label and entity capital for each entity span in doc.ents.\par

\pard\li720\sl240\slmult1\qj\fs22 ___________________________________________________________________________\par

\pard\li720\sl240\slmult1\b ANSWER:\par
\b0\f1\fs16 def countries_component(doc):\par
    # Create an entity Span with the label 'GPE' for all matches\par
    doc.ents = [Span(doc, start, end, label='GPE')\par
                for match_id, start, end in matcher(doc)]\par
    return doc\par
\par
# Add the component to the pipeline\par
nlp.add_pipe(countries_component, last=True)\par
print(nlp.pipe_names)\par
\par
\par
# Getter that looks up the span text in the dictionary of country capitals\par
get_capital = lambda span: capitals.get(span.text)\par
\par
# Register the Span extension attribute 'capital' with the getter get_capital \par
Span.set_extension('capital', getter=get_capital)\par
\par
\par
# Process the text and print the entity text, label and capital attributes\par
doc = nlp("Czech Republic may help Slovakia protect its airspace")\par
print([(ent.text, ent.label_, ent._.capital) for ent in doc.ents])\par

\pard\li720\sl240\slmult1\qj\f0\fs22 ___________________________________________________________________________\par

\pard\li720\sl240\slmult1\b RESULT:\par
\b0\f1\fs16 ['countries_component']\par
\par
[('Czech Republic', 'GPE', 'Prague'), ('Slovakia', 'GPE', 'Bratislava')]\b\par

\pard\li720\sl240\slmult1\qj\b0\f0\fs22 ___________________________________________________________________________\par
\cf3\i\fs20 Well done! This is a great example of how you can add structured data to your spaCy pipeline.\cf0\i0\f1\fs16\par

\pard\sl240\slmult1\tx568\cf1\f0\fs22 __________________________________________________________________________________\par

\pard\sl240\slmult1\qj\cf2\b 03.13 Scaling and performance\par

\pard\li720\sl240\slmult1\qj\cf0\b0\fs20\lang10 See the video.\fs22\par

\pard\sl240\slmult1\tx568\cf1 __________________________________________________________________________________\par

\pard\sl240\slmult1\qj\cf2\b 03.14 Processing streams\par

\pard\li720\sl240\slmult1\qj\cf0\b0\fs20\lang10 In this exercise, you'll be using nlp.pipe for more efficient text processing. The nlp object has already been created for you. A list of tweets about a popular American fast food chain are available as the variable TEXTS.\par
\par

\pard\li720\sl240\slmult1\f1\fs15 In [1]: TEXTS\par
Out[1]: \par
['McDonalds is my favorite restaurant.',\par
 'Here I thought @McDonalds only had precooked burgers but it seems they only have not cooked ones?? I have no time to get sick..',\par
 'People really still eat McDonalds :(',\par
 'The McDonalds in Spain has chicken wings. My heart is so happy ',\par
 '@McDonalds Please bring back the most delicious fast food sandwich of all times!!....The Arch Deluxe :P',\par
 'please hurry and open. I WANT A #McRib SANDWICH SO BAD! :D',\par
 'This morning i made a terrible decision by gettin mcdonalds and now my stomach is payin for it']\par
\par
In [2]: len(TEXTS)\par
Out[2]: 7\f0\fs22\par

\pard\li720\sl240\slmult1\qj ___________________________________________________________________________\par

\pard\li720\sl240\slmult1\b INSTRUCTIONS:\par

\pard{\pntext\f4\'B7\tab}{\*\pn\pnlvlblt\pnf4\pnindent0{\pntxtb\'B7}}\fi-360\li1080\sl240\slmult1\qj\b0\fs20 Rewrite the example to use nlp.pipe. Instead of iterating over the texts and processing them, iterate over the doc objects yielded by nlp.pipe.\par
{\pntext\f4\'B7\tab}Rewrite the example to use nlp.pipe. Don't forget to call list() around the result to turn it into a list.\par
{\pntext\f4\'B7\tab}Rewrite the example to use nlp.pipe. Don't forget to call list() around the result to turn it into a list.\par

\pard\li720\sl240\slmult1\qj\fs22 ___________________________________________________________________________\par

\pard\li720\sl240\slmult1\b ANSWER:\par
\b0\f1\fs16 # Process the texts and print the adjectives\par
for text in TEXTS:\par
    doc = nlp(text)\par
    print([token.text for token in doc if token.pos_ == 'ADJ'])\par
\par
# Process the texts and print the adjectives\par
for doc in nlp.pipe(TEXTS):\par
    print([token.text for token in doc if token.pos_ == 'ADJ'])\par
---------------------------------------------------------------------------\par
# Process the texts and print the entities\par
docs = [nlp(text) for text in TEXTS]\par
entities = [doc.ents for doc in docs]\par
print(*entities)\par
\par
# Process the texts and print the entities\par
docs = list(nlp.pipe(TEXTS))\par
entities = [doc.ents for doc in docs]\par
print(*entities)\par
---------------------------------------------------------------------------\par
people = ['David Bowie', 'Angela Merkel', 'Lady Gaga']\par
\par
# Create a list of patterns for the PhraseMatcher\par
patterns = [nlp(person) for person in people]\par
\par
\par
people = ['David Bowie', 'Angela Merkel', 'Lady Gaga']\par
\par
# Create a list of patterns for the PhraseMatcher\par
patterns = list(nlp.pipe(people))\par

\pard\li720\sl240\slmult1\qj\f0\fs22 ___________________________________________________________________________\par

\pard\li720\sl240\slmult1\b RESULT:\par
\b0\f1\fs16 ['favorite']\par
['sick']\par
[]\par
['happy']\par
['delicious', 'fast']\par
[]\par
['terrible', 'gettin', 'payin']\par
\par
\par
['favorite']\par
['sick']\par
[]\par
['happy']\par
['delicious', 'fast']\par
[]\par
['terrible', 'gettin', 'payin']\par
---------------------------------------------------------------------------\par
(McDonalds,) (@McDonalds,) (McDonalds,) (McDonalds, Spain) (The Arch Deluxe,) (WANT, McRib) (This morning,)\par
\par
(McDonalds,) (@McDonalds,) (McDonalds,) (McDonalds, Spain) (The Arch Deluxe,) (WANT, McRib) (This morning,)\b\par

\pard\li720\sl240\slmult1\qj\b0\f0\fs22 ___________________________________________________________________________\par
\cf3\i\fs20 Good job! Let's move on to a practical example that uses nlp.pipe to process documents with additional meta data.\cf0\i0\f1\fs16\par

\pard\sl240\slmult1\tx568\cf1\f0\fs22 __________________________________________________________________________________\par

\pard\sl240\slmult1\qj\cf2\b 03.15 Processing data with context\par

\pard\li720\sl240\slmult1\qj\cf0\b0\fs20\lang10 In this exercise, you'll be using custom attributes to add author and book meta information to quotes.\par
A list of (text, context) examples is available as the variable DATA. The texts are quotes from famous books, and the contexts dictionaries with the keys 'author' and 'book'. The nlp object has already been created for you.\par
\par

\pard\li720\sl240\slmult1\f1\fs14 In [1]: DATA\par
Out[1]: \par

\pard\li1440\sl240\slmult1 [('One morning, when Gregor Samsa woke from troubled dreams, \par
   he found himself transformed in his bed into a horrible vermin.',\par

\pard\li720\sl240\slmult1   \tab   \{'author': 'Franz Kafka', 'book': 'Metamorphosis'\}),\par
 \tab  ("I know not all that may be coming, but be it what it will, I'll go to it laughing.",\par
  \tab   \{'author': 'Herman Melville', 'book': 'Moby-Dick or, The Whale'\}),\par
 \tab  ('It was the best of times, it was the worst of times.',\par
\tab   \{'author': 'Charles Dickens', 'book': 'A Tale of Two Cities'\}),\par
\tab  ('The only people for me are the mad ones, the ones who are mad to live, mad to talk, \par
\tab    mad to be saved, desirous of everything at the same time, \par
\tab    the ones who never yawn or say a commonplace thing, \par
\tab    but burn, burn, burn like fabulous yellow roman candles exploding \par
\tab    like spiders across the stars.',\par
\tab   \{'author': 'Jack Kerouac', 'book': 'On the Road'\}),\par
\tab  ('It was a bright cold day in April, and the clocks were striking thirteen.',\par
\tab   \{'author': 'George Orwell', 'book': '1984'\}),\par
\tab  ('Nowadays people know the price of everything and the value of nothing.',\par
\tab   \{'author': 'Oscar Wilde', 'book': 'The Picture Of Dorian Gray'\})]\f0\fs22\par

\pard\li720\sl240\slmult1\qj ___________________________________________________________________________\par

\pard\li720\sl240\slmult1\b INSTRUCTIONS:\par

\pard{\pntext\f4\'B7\tab}{\*\pn\pnlvlblt\pnf4\pnindent0{\pntxtb\'B7}}\fi-360\li1080\sl240\slmult1\qj\b0\fs20 Import the Doc class and use the set_extension method to register the custom attributes 'author' and 'book', which default to None.\par
{\pntext\f4\'B7\tab}Process the (text, context) tuples in DATA using nlp.pipe with as_tuples=True.\par
{\pntext\f4\'B7\tab}Overwrite the doc._.book and doc._.author with the respective info passed in as the context.\par

\pard\li720\sl240\slmult1\qj\fs22 ___________________________________________________________________________\par

\pard\li720\sl240\slmult1\b ANSWER:\par
\b0\f1\fs16 # Import the Doc class\par
from spacy.tokens import Doc\par
\par
# Register the Doc extension 'author' (default None)\par
Doc.set_extension('author', default=None)\par
\par
# Register the Doc extension 'book' (default None)\par
Doc.set_extension('book', default=None)\par
\par
for doc, context in nlp.pipe(DATA, as_tuples=True):\par
    # Set the doc._.book and doc._.author attributes from the context\par
    doc._.book = context['book']\par
    doc._.author = context['author']\par
    \par
    # Print the text and custom attribute data\par
    print(doc.text, '\\n', "\f3\emdash  '\{\}' by \{\}".format(doc._.book, doc._.author), '\\n')\f1\par

\pard\li720\sl240\slmult1\qj\f0\fs22 ___________________________________________________________________________\par

\pard\li720\sl240\slmult1\b RESULT:\par
\b0\f1\fs16 One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin. \par
 \f3\emdash  'Metamorphosis' by Franz Kafka \par
\par
I know not all that may be coming, but be it what it will, I'll go to it laughing. \par
 \emdash  'Moby-Dick or, The Whale' by Herman Melville \par
\par
It was the best of times, it was the worst of times. \par
 \emdash  'A Tale of Two Cities' by Charles Dickens \par
\par
The only people for me are the mad ones, the ones who are mad to live, mad to talk, mad to be saved, desirous of everything at the same time, the ones who never yawn or say a commonplace thing, but burn, burn, burn like fabulous yellow roman candles exploding like spiders across the stars. \par
 \emdash  'On the Road' by Jack Kerouac \par
\par
It was a bright cold day in April, and the clocks were striking thirteen. \par
 \emdash  '1984' by George Orwell \par
\par
Nowadays people know the price of everything and the value of nothing. \par
 \emdash  'The Picture Of Dorian Gray' by Oscar Wilde\b\f1\par

\pard\li720\sl240\slmult1\qj\b0\f0\fs22 ___________________________________________________________________________\par
\cf3\i\fs20 Well done! The same technique is useful for a variety of tasks. For example, you could pass in page or paragraph numbers to relate the processed Doc back to the position in a larger document. Or you could pass in other structured data like IDs referring to a knowledge base.\cf0\i0\f1\fs16\par

\pard\sl240\slmult1\tx568\cf1\f0\fs22 __________________________________________________________________________________\par

\pard\sl240\slmult1\qj\cf2\b 03.16 Selective processing\lang10\par

\pard\li720\sl240\slmult1\qj\cf0\b0\fs20 In this exercise, you'll use the nlp.make_doc and nlp.disable_pipes methods to only run selected components when processing a text. The small English model is already loaded in as the nlp object.\fs22\par
___________________________________________________________________________\par

\pard\li720\sl240\slmult1\b INSTRUCTIONS:\par

\pard{\pntext\f4\'B7\tab}{\*\pn\pnlvlblt\pnf4\pnindent0{\pntxtb\'B7}}\fi-360\li1080\sl240\slmult1\qj\b0\fs20 Rewrite the code to only tokenize the text using nlp.make_doc.\par

\pard\sl240\slmult1\qj\par

\pard{\pntext\f4\'B7\tab}{\*\pn\pnlvlblt\pnf4\pnindent0{\pntxtb\'B7}}\fi-360\li1080\sl240\slmult1\qj Disable the tagger and parser using the nlp.disable_pipes method.\par
{\pntext\f4\'B7\tab}Process the text and print all entities in the doc.\par

\pard\li720\sl240\slmult1\qj\fs22 ___________________________________________________________________________\par

\pard\li720\sl240\slmult1\b ANSWER:\par
\b0\f1\fs16 text = "Chick-fil-A is an American fast food restaurant chain headquartered in the city of College Park, Georgia, specializing in chicken sandwiches."\par
\par
# Only tokenize the text\par
doc = nlp(text)\par
\par
print([token.text for token in doc])\par
\par
\par
text = "Chick-fil-A is an American fast food restaurant chain headquartered in the city of College Park, Georgia, specializing in chicken sandwiches."\par
\par
# Only tokenize the text\par
doc = nlp.make_doc(text)\par
\par
print([token.text for token in doc])\par
---------------------------------------------------------------------------\par
text = "Chick-fil-A is an American fast food restaurant chain headquartered in the city of College Park, Georgia, specializing in chicken sandwiches."\par
\par
# Disable the tagger and parser\par
with nlp.disable_pipes('tagger', 'parser'):\par
    # Process the text\par
    doc = nlp(text)\par
    # Print the entities in the doc\par
    print([(ent.text, ent.label_) for ent in doc.ents])\par

\pard\li720\sl240\slmult1\qj\f0\fs22 ___________________________________________________________________________\par

\pard\li720\sl240\slmult1\b RESULT:\par
\b0\f1\fs16 ['Chick', '-', 'fil', '-', 'A', 'is', 'an', 'American', 'fast', 'food', 'restaurant', 'chain', 'headquartered', 'in', 'the', 'city', 'of', 'College', 'Park', ',', 'Georgia', ',', 'specializing', 'in', 'chicken', 'sandwiches', '.']\par
\par
['Chick', '-', 'fil', '-', 'A', 'is', 'an', 'American', 'fast', 'food', 'restaurant', 'chain', 'headquartered', 'in', 'the', 'city', 'of', 'College', 'Park', ',', 'Georgia', ',', 'specializing', 'in', 'chicken', 'sandwiches', '.']\b\par
\b0 ---------------------------------------------------------------------------\par
[('American', 'NORP'), ('College Park', 'GPE'), ('Georgia', 'GPE')]\par

\pard\li720\sl240\slmult1\qj\f0\fs22 ___________________________________________________________________________\par
\cf3\i\fs20 Perfect! Now that you've practiced the performance tips and tricks, you're ready for the next chapter and training spaCy's neural network models.\cf0\i0\f1\fs16\par

\pard\sl240\slmult1\tx568\cf1\f0\fs22 __________________________________________________________________________________\par

\pard\sl240\slmult1\qj\b Source:\par

\pard\sl240\slmult1 {\cf0\b0{\field{\*\fldinst{HYPERLINK https://learn.datacamp.com/courses/advanced-nlp-with-spacy }}{\fldrslt{https://learn.datacamp.com/courses/advanced-nlp-with-spacy\ul0\cf0}}}}\cf0\b0\f0\fs22\lang2058\par

\pard\sl240\slmult1\tx568\cf1\lang10 __________________________________________________________________________________\par
\cf0\b\par
}
 