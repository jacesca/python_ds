{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import expectexception  #%%expect_exception TypeError\n",
    "import pytest\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Test Organization and Execution\n",
    "\n",
    "In any data science project, you quickly reach a point when it becomes impossible to organize and manage unit tests. In this chapter, we will learn about how to structure your test suite well, how to effortlessly execute any subset of tests and how to mark problematic tests so that your test suite always stays green. The last lesson will even enable you to add the trust-inspiring build status and code coverage badges to your own project. Complete this chapter and become a unit testing wizard!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=darkred>3.1 How to organize a growing set of tests?</font>\n",
    "\n",
    "1. How to organize a growing set of tests?\n",
    ">Congratulations on finishing Chapter 2!\n",
    "\n",
    "2. What you've done so far\n",
    ">You wrote about 16 unit tests for the functions row_to_list(), convert_to_int(), get_data_as_numpy_array() and split_into_training_and_testing_sets() in Chapter 1 and 2. Well done!\n",
    "\n",
    "3. What you've done so far\n",
    ">As the number of\n",
    "\n",
    "4. What you've done so far\n",
    ">unit tests\n",
    "\n",
    "5. What you've done so far\n",
    ">keep growing\n",
    "\n",
    "6. Need a strategy to organize tests\n",
    ">we would need a strategy to keep all these tests organized. Otherwise, we risk our unit tests looking like\n",
    "\n",
    "7. Need a strategy to organize tests\n",
    ">this clothes cabinet. But don't worry, this lesson is going to provide us with a strategy.\n",
    "\n",
    "8. Project structure\n",
    ">Assume that the four functions that you tested are present in the following project structure. There's a top level directory called src, which holds all application code.\n",
    "\n",
    "9. Project structure\n",
    ">Inside, there's the data package. This package deals with functions that preprocess data.\n",
    "\n",
    "10. Project structure\n",
    ">It has a Python module called preprocessing_helpers.py, containing the functions row_to_list() and convert_to_int().\n",
    "\n",
    "11. Project structure\n",
    ">Then there's the features package, which deals with extracting features from the preprocessed data.\n",
    "\n",
    "12. Project structure\n",
    ">It has a module called as_numpy.py, containing the function get_data_as_numpy_array().\n",
    "\n",
    "13. Project structure\n",
    ">Finally, there's the models package, which deals with training and testing the linear regression model.\n",
    "\n",
    "14. Project structure\n",
    ">It has a module called train.py. So far, it only contains one function split_into_training_and_testing_sets().\n",
    "\n",
    "15. The tests folder\n",
    ">The developers of pytest recommend that we create a directory called tests at the same level as src. This directory is also called the test suite.\n",
    "\n",
    "16. The tests folder mirrors the application folder\n",
    ">Inside this folder, we simply mirror the inner structure of src and create empty packages called data, features and models respectively.\n",
    "\n",
    "17. Python module and test module correspondence\n",
    ">The general rule is that for each python module my_module.py, there should be a corresponding test module called test_my_module.py. For example, for the module preprocessing_helpers.py, we create a test module called test_preprocessing_helpers.py. Since preprocessing_helpers.py belongs to the data package, we put the corresponding test module in the mirrored package inside the tests directory. The mirroring in the directory structure and test module names ensure that if we know where to find application code, we can follow the same route inside the test directory to access corresponding tests. The test module test_preprocessing_helpers.py should contain tests for row_to_list() and convert_to_int().\n",
    "\n",
    "18. Structuring tests inside test modules\n",
    ">We could just put the tests sequentially like this, but this is an organizational nightmare, because there's no way to tell where the tests for one function ends and another function begins.\n",
    "\n",
    "19. Test class\n",
    ">pytest solves this problem using a construct called the test class.\n",
    "\n",
    "20. Test class is a container for a single unit's tests\n",
    ">A test class is just a simple container for tests of a specific function.\n",
    "\n",
    "21. Test class: theoretical structure\n",
    ">To declare a test class, we start with the class keyword\n",
    "\n",
    "22. Test class: theoretical structure\n",
    ">and follow it up with the name of the class. The name of the class should be in CamelCase, and should always start with “Test”. The best way to name a test class is to follow the “Test” with the name of the function, for example, TestRowToList.\n",
    "\n",
    "23. Test class: theoretical structure\n",
    ">A test class takes one argument, and this argument is always called object. To know more about this argument, check out the DataCamp course on object-oriented Python, but we don't really need to for testing purposes, as we will never use this argument anywhere else. Now put all tests for the function under the test class as follows.\n",
    "\n",
    "24. Test class: theoretical structure\n",
    ">Note that, this time, all tests should receive a single argument called self. This also comes from object-oriented Python.\n",
    "\n",
    "25. Clean separation\n",
    ">For the other function convert_to_int(), we create another test class TestConvertToInt, and put the tests for convert_to_int() inside that class. Then the tests for the two functions are nicely separated.\n",
    "\n",
    "26. Final test directory structure\n",
    ">This procedure is then repeated for test_as_numpy.py, which would hold the test class TestGetDataAsNumpyArray and test_train.py, which would hold the test class TestSplitIntoTrainingAndTestingSets.\n",
    "\n",
    "27. Test directory is well organized!\n",
    ">Now our tests or, should we say clothes, are well organized.\n",
    "\n",
    "28. IPython console's working directory is tests\n",
    ">Before we move on to the exercises, note that the IPython console's working directory will be the tests directory from now on.\n",
    "\n",
    "29. IPython console's working directory is tests\n",
    "\n",
    "30. Let's practice structuring tests!\n",
    ">Let's try all this out in the exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaces\\Documents\\Data Science\\Python\\Cursos\\___Unit Testing for Data Science in Python\\test_practices\n",
      "============================= test session starts =============================\n",
      "platform win32 -- Python 3.8.8, pytest-6.2.4, py-1.10.0, pluggy-0.13.1\n",
      "Matplotlib: 3.3.4\n",
      "Freetype: 2.10.4\n",
      "rootdir: C:\\Users\\jaces\\Documents\\Data Science\\Python\\Cursos\\___Unit Testing for Data Science in Python\\test_practices\n",
      "plugins: cov-2.12.1, mock-3.6.1, mpl-0.13\n",
      "collected 31 items\n",
      "\n",
      "test_TestSplitIntoTrainingAndTestingSets.py .                            [  3%]\n",
      "test_convert_to_int.py .                                                 [  6%]\n",
      "test_convert_to_int_TDD.py ......                                        [ 25%]\n",
      "test_convert_to_int_fail.py F                                            [ 29%]\n",
      "test_for_missing_area_fail.py F                                          [ 32%]\n",
      "test_get_data_as_numpy_array_fail.py .                                   [ 35%]\n",
      "test_mystery_function.py .                                               [ 38%]\n",
      "test_row_to_list.py .......                                              [ 61%]\n",
      "test_row_to_list_boundary_values.py ...                                  [ 70%]\n",
      "test_row_to_list_fail.py .F.                                             [ 80%]\n",
      "test_row_to_list_normal_values.py ..                                     [ 87%]\n",
      "test_row_to_list_special_values.py ..                                    [ 93%]\n",
      "test_split_into_training_and_testing_sets.py .                           [ 96%]\n",
      "test_valueerror_on_one_dimensional_argument.py .                         [100%]\n",
      "\n",
      "================================== FAILURES ===================================\n",
      "________________________ test_on_string_with_one_comma ________________________\n",
      "\n",
      "    def test_on_string_with_one_comma():\n",
      "        test_argument = \"2,081\"\n",
      "        expected = 2081\n",
      "        actual = convert_to_int(test_argument)\n",
      "        # Format the string with the actual return value\n",
      "        message = \"convert_to_int('2,081') should return the int 2081, but it actually returned {0}\".format(actual)\n",
      "        # Write the assert statement which prints message on failure\n",
      ">       assert actual == expected, message\n",
      "E       AssertionError: convert_to_int('2,081') should return the int 2081, but it actually returned None\n",
      "E       assert None == 2081\n",
      "\n",
      "test_convert_to_int_fail.py:18: AssertionError\n",
      "____________________________ test_for_missing_area ____________________________\n",
      "\n",
      "    def test_for_missing_area():\n",
      "        val = '\\t293,410\\n'\n",
      "        actual = row_to_list(val)\n",
      "        expected = None\n",
      "        message = \"row_to_list({0}) returned {1} instead of {2}\".format(repr(val), actual, expected)\n",
      ">       assert actual is expected, message\n",
      "E       AssertionError: row_to_list('\\t293,410\\n') returned ['293,410'] instead of None\n",
      "E       assert ['293,410'] is None\n",
      "\n",
      "test_for_missing_area_fail.py:18: AssertionError\n",
      "____________________________ test_for_missing_area ____________________________\n",
      "\n",
      "    def test_for_missing_area():\n",
      ">       assert row_to_list('\\t293,410\\n') is none\n",
      "E       NameError: name 'none' is not defined\n",
      "\n",
      "test_row_to_list_fail.py:12: NameError\n",
      "=========================== short test summary info ===========================\n",
      "FAILED test_convert_to_int_fail.py::test_on_string_with_one_comma - Assertion...\n",
      "FAILED test_for_missing_area_fail.py::test_for_missing_area - AssertionError:...\n",
      "FAILED test_row_to_list_fail.py::test_for_missing_area - NameError: name 'non...\n",
      "======================== 3 failed, 28 passed in 0.78s =========================\n",
      "C:\\Users\\jaces\\Documents\\Data Science\\Python\\Cursos\\___Unit Testing for Data Science in Python\n"
     ]
    }
   ],
   "source": [
    "%cd test_practices\n",
    "!pytest\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=darkred>3.2 Place test modules at the correct location</font>\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "A data science project without visualization is like pizza without cheese, right? But this has been fixed by creating a package called visualization under the top level application directory src.\n",
    "\n",
    "\n",
    "<code>\n",
    "src/                                    # All application code lives here\n",
    "|-- visualization/                      # Package for visualization\n",
    "    |-- __init__.py\n",
    "    |-- plots.py                        # Module for plotting\n",
    "</code>\n",
    "\n",
    "In the package, there is a Python module plots.py, which contain functions related to plotting. These functions should be tested in a test module test_plots.py.\n",
    "\n",
    "According to pytest guidelines, where should you place this test module within the project structure?\n",
    "\n",
    "**Possible Answers**\n",
    "- src/visualization/test_plots.py.\n",
    "- src/visualization/tests/test_plots.py.\n",
    "- tests/test_plots.py.\n",
    "- <font color=red>tests/visualization/test_plots.py.</font>\n",
    "\n",
    "**Results**\n",
    "\n",
    "<font color=darkgreen>Wow, you have become good at organizing tests! Placing it in this location gives us two advantages: easier navigation within the tests folder and the possibility of having identically named test modules distinguished by the parent mirror package.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=darkred>3.3 Create a test class</font>\n",
    "\n",
    "Test classes are containers inside test modules. They help separate tests for different functions within the test module, and serve as a structuring tool in the pytest framework.\n",
    "\n",
    "Test classes are written in CamelCase e.g. TestMyFunction as opposed to tests, which are written using underscores e.g. test_something().\n",
    "\n",
    "You met the function split_into_training_and_testing_sets() in Chapter 2, and wrote some tests for it. One of these tests was called test_on_one_row() and it checked if the function raises a ValueError when passed a NumPy array with only one row.\n",
    "\n",
    "In this exercise you are going to create a test class for this function. This test class will hold the test test_on_one_row().\n",
    "\n",
    "**Instructions**\n",
    "- Declare the test class for the function split_into_training_and_testing_sets(), making sure to give it a name that follows the standard naming convention.\n",
    "- Fill in the mandatory argument in the test test_on_one_row().\n",
    "\n",
    "**Results**\n",
    "\n",
    "<font color=darkgreen>Wow, well done! Using test classes, you can now cleanly separate tests for different functions in your test modules. If you don't know object-oriented Python, the arguments object and self might make little sense to you. That is all right, since you don't need to use them extensively for the material of this course. Just make sure that you put the arguments in the right place, and everything will work like magic! If you are curious to learn more about them, check out the Datacamp course on <a href=https://www.datacamp.com/courses/object-oriented-programming-in-python>object-oriented Python</a>.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaces\\Documents\\Data Science\\Python\\Cursos\\___Unit Testing for Data Science in Python\\test_practices\n",
      "============================= test session starts =============================\n",
      "platform win32 -- Python 3.8.8, pytest-6.2.4, py-1.10.0, pluggy-0.13.1\n",
      "Matplotlib: 3.3.4\n",
      "Freetype: 2.10.4\n",
      "rootdir: C:\\Users\\jaces\\Documents\\Data Science\\Python\\Cursos\\___Unit Testing for Data Science in Python\\test_practices\n",
      "plugins: cov-2.12.1, mock-3.6.1, mpl-0.13\n",
      "collected 1 item\n",
      "\n",
      "test_TestSplitIntoTrainingAndTestingSets.py .                            [100%]\n",
      "\n",
      "============================== 1 passed in 0.60s ==============================\n",
      "C:\\Users\\jaces\\Documents\\Data Science\\Python\\Cursos\\___Unit Testing for Data Science in Python\n"
     ]
    }
   ],
   "source": [
    "%cd test_practices\n",
    "with open(\"test_TestSplitIntoTrainingAndTestingSets.py\", \"w\") as text_file:\n",
    "    text_file.write(\"\"\"\n",
    "import pytest\n",
    "import numpy as np\n",
    "\n",
    "from models.train import split_into_training_and_testing_sets\n",
    "\n",
    "# Declare the test class\n",
    "class TestSplitIntoTrainingAndTestingSets(object):\n",
    "    # Fill in with the correct mandatory argument\n",
    "    def test_on_one_row(self):\n",
    "        test_argument = np.array([[1382.0, 390167.0]])\n",
    "        with pytest.raises(ValueError) as exc_info:\n",
    "            split_into_training_and_testing_sets(test_argument)\n",
    "        expected_error_msg = \"Argument data_array must have at least 2 rows, it actually has just 1\"\n",
    "        assert exc_info.match(expected_error_msg)\n",
    "    \"\"\")\n",
    "    \n",
    "!pytest test_TestSplitIntoTrainingAndTestingSets.py\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APPLYING BEST PRACTICE --> USE THE TEST DIRECTORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaces\\Documents\\Data Science\\Python\\Cursos\\___Unit Testing for Data Science in Python\\univariate-linear-regression\\src\\test\n",
      "============================= test session starts =============================\n",
      "platform win32 -- Python 3.8.8, pytest-6.2.4, py-1.10.0, pluggy-0.13.1\n",
      "Matplotlib: 3.3.4\n",
      "Freetype: 2.10.4\n",
      "rootdir: C:\\Users\\jaces\\Documents\\Data Science\\Python\\Cursos\\___Unit Testing for Data Science in Python\\univariate-linear-regression\n",
      "plugins: cov-2.12.1, mock-3.6.1, mpl-0.13\n",
      "collected 3 items\n",
      "\n",
      "models\\test_train.py ...                                                 [100%]\n",
      "\n",
      "============================== 3 passed in 0.59s ==============================\n",
      "C:\\Users\\jaces\\Documents\\Data Science\\Python\\Cursos\\___Unit Testing for Data Science in Python\n"
     ]
    }
   ],
   "source": [
    "%cd univariate-linear-regression/src/test\n",
    "with open(\"models/test_train.py\", \"w\") as text_file:\n",
    "    text_file.write(\"\"\"\n",
    "import pytest\n",
    "import numpy as np\n",
    "\n",
    "from models.train import split_into_training_and_testing_sets\n",
    "\n",
    "# Declare the test class\n",
    "class TestSplitIntoTrainingAndTestingSets(object):\n",
    "    def test_on_six_rows(self):\n",
    "        example_argument = np.array([[2081.0, 314942.0], [1059.0, 186606.0],\n",
    "                                     [1148.0, 206186.0], [1506.0, 248419.0],\n",
    "                                     [1210.0, 214114.0], [1697.0, 277794.0]]\n",
    "                                    )\n",
    "        # Fill in with training array's expected number of rows\n",
    "        expected_training_array_num_rows = int(example_argument.shape[0]*0.75)\n",
    "    \n",
    "        # Fill in with testing array's expected number of rows\n",
    "        expected_testing_array_num_rows = example_argument.shape[0] - expected_training_array_num_rows\n",
    "    \n",
    "        # Call the function to test\n",
    "        actual = split_into_training_and_testing_sets(example_argument)\n",
    "    \n",
    "        # Write the assert statement checking training array's number of rows\n",
    "        assert actual[0].shape[0] == expected_training_array_num_rows, \\\n",
    "            \"The actual number of rows in the training array is not {}\".format(expected_training_array_num_rows)\n",
    "    \n",
    "        # Write the assert statement checking testing array's number of rows\n",
    "        assert actual[1].shape[0] == expected_testing_array_num_rows, \\\n",
    "            \"The actual number of rows in the testing array is not {}\".format(expected_testing_array_num_rows)\n",
    "\n",
    "    \n",
    "    def test_on_one_row(self):\n",
    "        test_argument = np.array([[1382.0, 390167.0]])\n",
    "        with pytest.raises(ValueError) as exc_info:\n",
    "            split_into_training_and_testing_sets(test_argument)\n",
    "        expected_error_msg = \"Argument data_array must have at least 2 rows, it actually has just 1\"\n",
    "        assert exc_info.match(expected_error_msg)\n",
    "    \n",
    "    \n",
    "    def test_valueerror_on_one_dimensional_argument(self):\n",
    "        example_argument = np.array([2081, 314942, 1059, 186606, 1148, 206186])\n",
    "    \n",
    "        with pytest.raises(ValueError) as exception_info:\n",
    "            # store the exception\n",
    "            split_into_training_and_testing_sets(example_argument)\n",
    "    \n",
    "        # Check if ValueError contains correct message\n",
    "        assert exception_info.match(\"Argument data_array must be two dimensional. Got 1 dimensional array instead!\")\n",
    "    \"\"\")\n",
    "    \n",
    "!pytest models/test_train.py\n",
    "%cd ../../.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=darkred>3.4 Mastering test execution</font>\n",
    "\n",
    "1. Mastering test execution\n",
    ">In the last lesson, we learned how to organize tests.\n",
    "\n",
    "2. Test organization\n",
    ">The centerpiece was the tests folder, which holds all tests for the project.\n",
    "\n",
    "3. Test organization\n",
    ">The folder contains mirror packages, each of which contain a test module.\n",
    "\n",
    "4. Test organization\n",
    ">The test modules contain many test classes.\n",
    "\n",
    "5. Test organization\n",
    ">A test class is just a container for unit tests for a particular function.\n",
    "\n",
    "6. Running all tests\n",
    ">pytest provides an easy way to run all tests contained in the tests folder.\n",
    "\n",
    "7. Running all tests\n",
    ">We simply change to the tests directory and run the command pytest. This command automatically discovers tests by recursing into the subtree of the working directory. It identifies all files with names starting with “test_” as test modules. Within test modules, it identifies classes with names starting with “Test” as test classes. Within each test class, it identifies all functions with names starting with “test_” as unit tests. It collects these unit tests and runs them all.\n",
    "\n",
    "8. Running all tests\n",
    ">Here is the result of the command. You wrote 16 tests so far, so the command ran all these 16 tests. 15 passed and 1 failed.\n",
    "\n",
    "9. Typical scenario: CI server\n",
    ">A typical scenario to run this command is in a CI server after a commit is pushed to the code base.\n",
    "\n",
    "10. Binary question: do all unit tests pass?\n",
    ">In this case, we are only interested in the binary question: do all unit tests pass after including the commit?\n",
    "\n",
    "11. The -x flag: stop after first failure\n",
    ">In this case, adding the -x flag to the pytest command can save time and resources. This flag makes pytest stop after the first failing test, because a failing test already answers the binary question. In the report, we see that only 9 tests ran this time since execution stopped after the failing test test_on_one_tab_with_missing_value().\n",
    "\n",
    "12. Running tests in a test module\n",
    ">Very often, we would only want to run a subset of tests. For example, we might want to just run tests contained in a particular test module, say, test_preprocessing_helpers.py. You already know how to do that since you did this several times in the exercises.\n",
    "\n",
    "13. Running tests in a test module\n",
    ">Just type pytest followed by the path to the test module. This only runs the 13 tests contained in test_preprocessing_helpers.py, as we can see in the test result report.\n",
    "\n",
    "14. Running only a particular test class\n",
    ">At other times, we want to be more specific. For example, when we are working on a particular function, say, row_to_list(), we only care about the test class TestRowToList.\n",
    "\n",
    "15. Node ID\n",
    ">During automatic test discovery, pytest assigns a node ID to every test class and unit test that it encounters. The node ID of a test class is the path to the test module followed by the name of the test class, separated by two colons. The node ID of a unit test follows the same format, with the unit test name added to the end using another double colon separator.\n",
    "\n",
    "16. Running tests using node ID\n",
    ">When we run the command pytest followed by the node ID of the test class TestRowToList, for example, it only runs the 7 tests contained in TestRowToList.\n",
    "\n",
    "17. Running tests using node ID\n",
    ">When we run the command with the node ID of the unit test test_on_one_tab_with_missing_value(), it only runs a single test.\n",
    "\n",
    "18. Running tests using keyword expressions\n",
    ">A faster and flexible way to do this is by using keyword expressions.\n",
    "\n",
    "19. The -k option\n",
    ">To run tests using keyword expressions, use the -k option. This option takes a quoted string containing a pattern as the value.\n",
    "\n",
    "20. The -k option\n",
    ">For example, we can specify a test class such as TestSplitIntoTrainingAndTestingSets as the pattern, and this will run only the 2 tests within that test class. We can also enter only part of the test class name, as long as that is unique. This saves a lot of typing and has the same outcome.\n",
    "\n",
    "21. Supports Python logical operators\n",
    ">We can even use Python logical operators in the pattern to do more complex subsetting. For example, the following command will execute all tests in TestSplitIntoTrainingAndTestingSets except the unit test test_on_one_row(), which only leaves one test to run.\n",
    "\n",
    "22. Let's run some tests!\n",
    ">Let's run some tests using these command line tricks in the exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATING ONE TEST FILE MORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaces\\Documents\\Data Science\\Python\\Cursos\\___Unit Testing for Data Science in Python\\univariate-linear-regression\\src\\test\n",
      "============================= test session starts =============================\n",
      "platform win32 -- Python 3.8.8, pytest-6.2.4, py-1.10.0, pluggy-0.13.1\n",
      "Matplotlib: 3.3.4\n",
      "Freetype: 2.10.4\n",
      "rootdir: C:\\Users\\jaces\\Documents\\Data Science\\Python\\Cursos\\___Unit Testing for Data Science in Python\\univariate-linear-regression\n",
      "plugins: cov-2.12.1, mock-3.6.1, mpl-0.13\n",
      "collected 13 items\n",
      "\n",
      "data\\test_preprocessing_helpers.py .............                         [100%]\n",
      "\n",
      "============================= 13 passed in 0.23s ==============================\n",
      "C:\\Users\\jaces\\Documents\\Data Science\\Python\\Cursos\\___Unit Testing for Data Science in Python\n"
     ]
    }
   ],
   "source": [
    "%cd univariate-linear-regression/src/test\n",
    "with open(\"data/test_preprocessing_helpers.py\", \"w\") as text_file:\n",
    "    text_file.write(\"\"\"\n",
    "import pytest\n",
    "from data.preprocessing_helpers import convert_to_int\n",
    "from data.preprocessing_helpers import row_to_list\n",
    "\n",
    "class TestConvertToInt(object):\n",
    "    def test_with_no_comma(self):\n",
    "        actual = convert_to_int(\"756\")\n",
    "        assert actual == 756, \"Expected: 756, Actual: {0}\".format(actual)\n",
    "    \n",
    "    def test_with_one_comma(self):\n",
    "        actual = convert_to_int(\"2,081\")\n",
    "        assert actual == 2081, \"Expected: 2081, Actual: {0}\".format(actual)\n",
    "    \n",
    "    def test_with_two_commas(self):\n",
    "        actual = convert_to_int(\"1,034,891\")\n",
    "        assert actual == 1034891, \"Expected: 2081, Actual: {0}\".format(actual)\n",
    "    \n",
    "    def test_on_string_with_missing_comma(self):\n",
    "        actual = convert_to_int(\"178100,301\")\n",
    "        assert actual is None, \"Expected: None, Actual: {0}\".format(actual)\n",
    "    \n",
    "    def test_on_string_with_incorrectly_placed_comma(self):\n",
    "        actual = convert_to_int(\"12,72,891\")\n",
    "        assert actual is None, \"Expected: None, Actual: {0}\".format(actual)\n",
    "    \n",
    "    def test_on_float_valued_string(self):\n",
    "        actual = convert_to_int(\"23,816.92\")\n",
    "        assert actual is None, \"Expected: None, Actual: {0}\".format(actual)\n",
    "\n",
    "\n",
    "class TestRowToList(object):\n",
    "    def test_on_normal_argument_1(self):\n",
    "        actual = row_to_list(\"123\\\\t4,567\\\\n\")\n",
    "        expected = [\"123\", \"4,567\"]\n",
    "        assert actual == expected, \"Expected: {0}, Actual: {1}\".format(expected, actual)\n",
    "    \n",
    "    def test_on_normal_argument_2(self):\n",
    "        actual = row_to_list(\"1,059\\\\t186,606\\\\n\")\n",
    "        expected = [\"1,059\", \"186,606\"]\n",
    "        assert actual == expected, \"Expected: {0}, Actual: {1}\".format(actual, expected)\n",
    "\n",
    "    def test_on_no_tab_with_missing_value(self):      # (0, 1) case\n",
    "        actual = row_to_list('\\\\n')\n",
    "        assert actual is None, \"Expected: None, Actual: {0}\".format(actual)\n",
    "    \n",
    "    def test_on_two_tabs_with_missing_value(self):    # (2, 1) case\n",
    "        actual = row_to_list(\"123\\\\t\\\\t89\\\\n\")\n",
    "        assert actual is None, \"Expected: None, Actual: {0}\".format(actual)\n",
    "\n",
    "    def test_on_no_tab_no_missing_value(self):        # (0, 0) boundary value\n",
    "        actual = row_to_list('123\\\\n')\n",
    "        assert actual is None, 'Expected: None, Actual: {0}'.format(actual)\n",
    "    \n",
    "    def test_on_two_tabs_no_missing_value(self):      # (2, 0) boundary value\n",
    "        actual = row_to_list('123\\\\t4,567\\\\t89\\\\n')\n",
    "        assert actual is None, 'Expected: None, Actual: {0}'.format(actual)\n",
    "    \n",
    "    def test_on_one_tab_with_missing_value(self):     # (1, 1) boundary value\n",
    "        actual = row_to_list('\\\\t4,567\\\\n')\n",
    "        assert actual is None, 'Expected: None, Actual: {0}'.format(actual)\n",
    "    \"\"\")\n",
    "    \n",
    "!pytest data/test_preprocessing_helpers.py\n",
    "%cd ../../.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RUNNING EVERYTHING IN THE TEST DIRECTORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaces\\Documents\\Data Science\\Python\\Cursos\\___Unit Testing for Data Science in Python\\univariate-linear-regression\\src\\test\n",
      "============================= test session starts =============================\n",
      "platform win32 -- Python 3.8.8, pytest-6.2.4, py-1.10.0, pluggy-0.13.1\n",
      "Matplotlib: 3.3.4\n",
      "Freetype: 2.10.4\n",
      "rootdir: C:\\Users\\jaces\\Documents\\Data Science\\Python\\Cursos\\___Unit Testing for Data Science in Python\\univariate-linear-regression\n",
      "plugins: cov-2.12.1, mock-3.6.1, mpl-0.13\n",
      "collected 24 items\n",
      "\n",
      "data\\test_preprocessing_helpers.py .............                         [ 54%]\n",
      "data\\test_preprocessing_helpers_conditional_avoid_fail.py s              [ 58%]\n",
      "features\\test_as_numpy.py .                                              [ 62%]\n",
      "features\\test_as_numpy_avoid_fail.py xs                                  [ 70%]\n",
      "features\\test_as_numpy_avoid_fail2.py s                                  [ 75%]\n",
      "models\\test_train.py ...                                                 [ 87%]\n",
      "models\\test_train_avoid_fail.py xx                                       [ 95%]\n",
      "models\\test_train_not_implemented_avoid_fail.py x                        [100%]\n",
      "\n",
      "================== 17 passed, 3 skipped, 4 xfailed in 0.73s ===================\n",
      "C:\\Users\\jaces\\Documents\\Data Science\\Python\\Cursos\\___Unit Testing for Data Science in Python\n"
     ]
    }
   ],
   "source": [
    "# Running all tests in the test directory inside our project\n",
    "%cd univariate-linear-regression/src/test\n",
    "!pytest\n",
    "%cd ../../.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAKING PYTEST STOP WHEN FIND A FAILURE (THIS IS ANOTHER TEST DIRECTORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaces\\Documents\\Data Science\\Python\\Cursos\\___Unit Testing for Data Science in Python\\test_practices\n",
      "============================= test session starts =============================\n",
      "platform win32 -- Python 3.8.8, pytest-6.2.4, py-1.10.0, pluggy-0.13.1\n",
      "Matplotlib: 3.3.4\n",
      "Freetype: 2.10.4\n",
      "rootdir: C:\\Users\\jaces\\Documents\\Data Science\\Python\\Cursos\\___Unit Testing for Data Science in Python\\test_practices\n",
      "plugins: cov-2.12.1, mock-3.6.1, mpl-0.13\n",
      "collected 31 items\n",
      "\n",
      "test_TestSplitIntoTrainingAndTestingSets.py .                            [  3%]\n",
      "test_convert_to_int.py .                                                 [  6%]\n",
      "test_convert_to_int_TDD.py ......                                        [ 25%]\n",
      "test_convert_to_int_fail.py F\n",
      "\n",
      "================================== FAILURES ===================================\n",
      "________________________ test_on_string_with_one_comma ________________________\n",
      "\n",
      "    def test_on_string_with_one_comma():\n",
      "        test_argument = \"2,081\"\n",
      "        expected = 2081\n",
      "        actual = convert_to_int(test_argument)\n",
      "        # Format the string with the actual return value\n",
      "        message = \"convert_to_int('2,081') should return the int 2081, but it actually returned {0}\".format(actual)\n",
      "        # Write the assert statement which prints message on failure\n",
      ">       assert actual == expected, message\n",
      "E       AssertionError: convert_to_int('2,081') should return the int 2081, but it actually returned None\n",
      "E       assert None == 2081\n",
      "\n",
      "test_convert_to_int_fail.py:18: AssertionError\n",
      "=========================== short test summary info ===========================\n",
      "FAILED test_convert_to_int_fail.py::test_on_string_with_one_comma - Assertion...\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!! stopping after 1 failures !!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "========================= 1 failed, 8 passed in 0.78s =========================\n",
      "C:\\Users\\jaces\\Documents\\Data Science\\Python\\Cursos\\___Unit Testing for Data Science in Python\n"
     ]
    }
   ],
   "source": [
    "# The -x flag: stop after first failure\n",
    "%cd test_practices\n",
    "!pytest -x\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RUNNING TESTS USING NODE ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts =============================\n",
      "platform win32 -- Python 3.8.8, pytest-6.2.4, py-1.10.0, pluggy-0.13.1\n",
      "Matplotlib: 3.3.4\n",
      "Freetype: 2.10.4\n",
      "rootdir: C:\\Users\\jaces\\Documents\\Data Science\\Python\\Cursos\\___Unit Testing for Data Science in Python\\univariate-linear-regression\n",
      "plugins: cov-2.12.1, mock-3.6.1, mpl-0.13\n",
      "collected 7 items\n",
      "\n",
      "univariate-linear-regression\\src\\test\\data\\test_preprocessing_helpers.py . [ 14%]\n",
      "......                                                                   [100%]\n",
      "\n",
      "============================== 7 passed in 0.18s ==============================\n"
     ]
    }
   ],
   "source": [
    "# Run the test class TestRowToList\n",
    "!pytest univariate-linear-regression/src/test/data/test_preprocessing_helpers.py::TestRowToList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaces\\Documents\\Data Science\\Python\\Cursos\\___Unit Testing for Data Science in Python\\univariate-linear-regression\\src\\test\n",
      "============================= test session starts =============================\n",
      "platform win32 -- Python 3.8.8, pytest-6.2.4, py-1.10.0, pluggy-0.13.1\n",
      "Matplotlib: 3.3.4\n",
      "Freetype: 2.10.4\n",
      "rootdir: C:\\Users\\jaces\\Documents\\Data Science\\Python\\Cursos\\___Unit Testing for Data Science in Python\\univariate-linear-regression\n",
      "plugins: cov-2.12.1, mock-3.6.1, mpl-0.13\n",
      "collected 1 item\n",
      "\n",
      "data\\test_preprocessing_helpers.py .                                     [100%]\n",
      "\n",
      "============================== 1 passed in 0.20s ==============================\n",
      "C:\\Users\\jaces\\Documents\\Data Science\\Python\\Cursos\\___Unit Testing for Data Science in Python\n"
     ]
    }
   ],
   "source": [
    "%cd univariate-linear-regression/src/test\n",
    "\n",
    "# Run the unit test test_on_one_tab_with_missing_value()\n",
    "!pytest data/test_preprocessing_helpers.py::TestRowToList::test_on_one_tab_with_missing_value\n",
    "\n",
    "%cd ../../.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RUNNING TEST BASE ON PATTERN SEARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaces\\Documents\\Data Science\\Python\\Cursos\\___Unit Testing for Data Science in Python\\univariate-linear-regression\\src\\test\n",
      "============================= test session starts =============================\n",
      "platform win32 -- Python 3.8.8, pytest-6.2.4, py-1.10.0, pluggy-0.13.1\n",
      "Matplotlib: 3.3.4\n",
      "Freetype: 2.10.4\n",
      "rootdir: C:\\Users\\jaces\\Documents\\Data Science\\Python\\Cursos\\___Unit Testing for Data Science in Python\\univariate-linear-regression\n",
      "plugins: cov-2.12.1, mock-3.6.1, mpl-0.13\n",
      "collected 24 items / 21 deselected / 3 selected\n",
      "\n",
      "models\\test_train.py ...                                                 [100%]\n",
      "\n",
      "====================== 3 passed, 21 deselected in 0.65s =======================\n",
      "C:\\Users\\jaces\\Documents\\Data Science\\Python\\Cursos\\___Unit Testing for Data Science in Python\n"
     ]
    }
   ],
   "source": [
    "%cd univariate-linear-regression/src/test\n",
    "\n",
    "# Run the unit test test_on_one_tab_with_missing_value()\n",
    "!pytest -k \"TestSplitIntoTrainingAndTestingSets\"\n",
    "\n",
    "%cd ../../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaces\\Documents\\Data Science\\Python\\Cursos\\___Unit Testing for Data Science in Python\\univariate-linear-regression\\src\\test\n",
      "============================= test session starts =============================\n",
      "platform win32 -- Python 3.8.8, pytest-6.2.4, py-1.10.0, pluggy-0.13.1\n",
      "Matplotlib: 3.3.4\n",
      "Freetype: 2.10.4\n",
      "rootdir: C:\\Users\\jaces\\Documents\\Data Science\\Python\\Cursos\\___Unit Testing for Data Science in Python\\univariate-linear-regression\n",
      "plugins: cov-2.12.1, mock-3.6.1, mpl-0.13\n",
      "collected 24 items / 22 deselected / 2 selected\n",
      "\n",
      "models\\test_train.py ..                                                  [100%]\n",
      "\n",
      "====================== 2 passed, 22 deselected in 0.64s =======================\n",
      "C:\\Users\\jaces\\Documents\\Data Science\\Python\\Cursos\\___Unit Testing for Data Science in Python\n"
     ]
    }
   ],
   "source": [
    "%cd univariate-linear-regression/src/test\n",
    "\n",
    "# Supports Python logical operators\n",
    "!pytest -k \"TestSplit and not test_on_one_row\"\n",
    "\n",
    "%cd ../../.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=darkred>3.5 One command to run them all</font>\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "One of your colleagues pushed some changes to the functions row_to_list(), convert_to_int(), get_data_as_numpy_array() and split_into_training_and_testing_sets(). That means that you have to run all the tests again to figure out if something got broken as a result.\n",
    "\n",
    "The current working directory in the IPython console is the tests directory, which contains all the tests in the same layout as described in the video. You can, at any time, run the tests in the IPython console using the appropriate command.\n",
    "\n",
    "**Results**\n",
    "\n",
    "- In the IPython console, what is the correct command for running all tests contained in the tests folder?\n",
    "    - <font color=red>!pytest</font>\n",
    "    - !pytest -x\n",
    "    - !pytest tests\n",
    "    - pytest\n",
    "    \n",
    "    \n",
    "- When you run all tests with the command !pytest, how many of them pass and how may fail?\n",
    "    - Passing: 10, Failing: 6\n",
    "    - <font color=red>Passing: 15, Failing: 1</font> (On iterative ex. in datacamp)\n",
    "    - Passing 16, Failing: 0\n",
    "    \n",
    "    \n",
    "- Assuming that you simply want to answer the binary question \"Are all tests passing\" without wasting time and resources, what is the correct command to run all tests till the first failure is encountered?\n",
    "    - !pytest -k\n",
    "    - !pytest\n",
    "    - <font color=red>!pytest -x</font>\n",
    "    \n",
    "    \n",
    "- When you ran the tests using the !pytest -x command, how many tests ran in total before test execution stopped because of the first failing test?\n",
    "    - 16\n",
    "    - <font color=red>15</font> (On iterative ex. in datacamp)\n",
    "    - 7\n",
    "\n",
    "<font color=darkgreen>Well done! In real life, the !pytest or !pytest -x command is often used in CI servers. It can also be useful if there is a major update to the code base, which changes many application modules at the same time. Running all tests is the only way to check if anything was broken due to the update.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaces\\Documents\\Data Science\\Python\\Cursos\\___Unit Testing for Data Science in Python\\univariate-linear-regression\\src\\test\n",
      "============================= test session starts =============================\n",
      "platform win32 -- Python 3.8.8, pytest-6.2.4, py-1.10.0, pluggy-0.13.1\n",
      "Matplotlib: 3.3.4\n",
      "Freetype: 2.10.4\n",
      "rootdir: C:\\Users\\jaces\\Documents\\Data Science\\Python\\Cursos\\___Unit Testing for Data Science in Python\\univariate-linear-regression\n",
      "plugins: cov-2.12.1, mock-3.6.1, mpl-0.13\n",
      "collected 24 items\n",
      "\n",
      "data\\test_preprocessing_helpers.py .............                         [ 54%]\n",
      "data\\test_preprocessing_helpers_conditional_avoid_fail.py s              [ 58%]\n",
      "features\\test_as_numpy.py .                                              [ 62%]\n",
      "features\\test_as_numpy_avoid_fail.py xs                                  [ 70%]\n",
      "features\\test_as_numpy_avoid_fail2.py s                                  [ 75%]\n",
      "models\\test_train.py ...                                                 [ 87%]\n",
      "models\\test_train_avoid_fail.py xx                                       [ 95%]\n",
      "models\\test_train_not_implemented_avoid_fail.py x                        [100%]\n",
      "\n",
      "================== 17 passed, 3 skipped, 4 xfailed in 0.73s ===================\n",
      "C:\\Users\\jaces\\Documents\\Data Science\\Python\\Cursos\\___Unit Testing for Data Science in Python\n"
     ]
    }
   ],
   "source": [
    "%cd univariate-linear-regression/src/test\n",
    "with open(\"features/test_as_numpy.py\", \"w\") as text_file:\n",
    "    text_file.write(\"\"\"\n",
    "import pytest\n",
    "\n",
    "from features.as_numpy import get_data_as_numpy_array\n",
    "import numpy as np\n",
    "\n",
    "class TestGetDataAsNumpyArray(object):\n",
    "    def test_on_clean_file(self):\n",
    "        expected = np.array([[2081.0, 314942.0],\n",
    "                             [1059.0, 186606.0],\n",
    "                             [1148.0, 206186.0]\n",
    "                            ])\n",
    "        actual = get_data_as_numpy_array(\"example_clean_data2.txt\", num_columns=2)\n",
    "        message = \"Expected return value: {0}, Actual return value: {1}\".format(expected, actual)\n",
    "        assert (actual == expected).all()\n",
    "    \"\"\")\n",
    "\n",
    "!pytest\n",
    "\n",
    "#os.remove('features/test_as_numpy.py')\n",
    "%cd ../../.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=darkred>3.6 Running test classes</font>\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "When you ran the !pytest command in the last exercise, the test test_on_six_rows() failed. This is a test for the function split_into_training_and_testing_sets(). This means that this function is broken.\n",
    "\n",
    "Short recap in case you forgot: this function takes a NumPy array containing housing area and prices as argument. The function randomly splits the argument array into training and testing arrays in the ratio 3:1, and returns the resulting arrays in a tuple.\n",
    "\n",
    "A quick look revealed that during the code update, someone inadvertently changed the split from 3:1 to 9:1. This has to be changed back and the unit tests for the function, which now lives in the test class TestSplitIntoTrainingAndTestingSets, needs to be run again. Are you up to the challenge?\n",
    "\n",
    "**Results**\n",
    "- Fill in with a float between 0 and 1 so that num_training is approximately of the number of rows in data_array.\n",
    "\n",
    "\n",
    "- Now let's see if that modification fixed the broken function. The current working directory in the IPython console is the tests folder that contains all tests. The test class TestSplitIntoTrainingAndTestingSets resides in the test module tests/models/test_train.py. What is the correct command to run all the tests in this test class using node IDs?\n",
    "    - !pytest models::test_train.py::TestSplitIntoTrainingAndTestingSets\n",
    "    - !pytest -k \"TestSplitIntoTrainingAndTestingSets\"\n",
    "    - !pytest models/test_train.py/TestSplitIntoTrainingAndTestingSets\n",
    "    - <font color=red>!pytest models/test_train.py::TestSplitIntoTrainingAndTestingSets</font>\n",
    "\n",
    "\n",
    "- What is the correct command to run only the previously failing test test_on_six_rows() using node IDs?\n",
    "    - <font color=red>!pytest models/test_train.py::TestSplitIntoTrainingAndTestingSets::test_on_six_rows</font>\n",
    "    - !pytest models/test_train.py::test_on_six_rows\n",
    "    - !pytest test_on_six_rows\n",
    "\n",
    "\n",
    "- What is the correct command to run the tests in TestSplitIntoTrainingAndTestingSets using keyword expressions?\n",
    "    - !pytest models/test_train.py::TestSplitIntoTrainingAndTestingSets\n",
    "    - !pytest -x \"TestSplitIntoTrainingAndTestingSets\"\n",
    "    - <font color=red>!pytest -k \"SplitInto\"</font>\n",
    "    - !pytest -k \"Test\"\n",
    "\n",
    "\n",
    "<font color=darkgreen>That's correct! The -k flag is really useful, because it helps you select tests and test classes by typing only a unique part of its name. This saves a lot of typing, and you must admit that TestSplitIntoTrainingAndTestingSets is a horrendously long name! In your projects, you will often run tests with the node IDs and the -k flag because you are often not interested in running all tests, but only a subset depending on the functions you are currently working on.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaces\\Documents\\Data Science\\Python\\Cursos\\___Unit Testing for Data Science in Python\\univariate-linear-regression\\src\\test\n",
      "============================= test session starts =============================\n",
      "platform win32 -- Python 3.8.8, pytest-6.2.4, py-1.10.0, pluggy-0.13.1\n",
      "Matplotlib: 3.3.4\n",
      "Freetype: 2.10.4\n",
      "rootdir: C:\\Users\\jaces\\Documents\\Data Science\\Python\\Cursos\\___Unit Testing for Data Science in Python\\univariate-linear-regression\n",
      "plugins: cov-2.12.1, mock-3.6.1, mpl-0.13\n",
      "collected 3 items\n",
      "\n",
      "models\\test_train.py ...                                                 [100%]\n",
      "\n",
      "============================== 3 passed in 0.50s ==============================\n",
      "C:\\Users\\jaces\\Documents\\Data Science\\Python\\Cursos\\___Unit Testing for Data Science in Python\n"
     ]
    }
   ],
   "source": [
    "%cd univariate-linear-regression/src/test\n",
    "!pytest models/test_train.py::TestSplitIntoTrainingAndTestingSets\n",
    "%cd ../../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaces\\Documents\\Data Science\\Python\\Cursos\\___Unit Testing for Data Science in Python\\univariate-linear-regression\\src\\test\n",
      "============================= test session starts =============================\n",
      "platform win32 -- Python 3.8.8, pytest-6.2.4, py-1.10.0, pluggy-0.13.1\n",
      "Matplotlib: 3.3.4\n",
      "Freetype: 2.10.4\n",
      "rootdir: C:\\Users\\jaces\\Documents\\Data Science\\Python\\Cursos\\___Unit Testing for Data Science in Python\\univariate-linear-regression\n",
      "plugins: cov-2.12.1, mock-3.6.1, mpl-0.13\n",
      "collected 1 item\n",
      "\n",
      "models\\test_train.py .                                                   [100%]\n",
      "\n",
      "============================== 1 passed in 0.51s ==============================\n",
      "C:\\Users\\jaces\\Documents\\Data Science\\Python\\Cursos\\___Unit Testing for Data Science in Python\n"
     ]
    }
   ],
   "source": [
    "%cd univariate-linear-regression/src/test\n",
    "!pytest models/test_train.py::TestSplitIntoTrainingAndTestingSets::test_on_six_rows\n",
    "%cd ../../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaces\\Documents\\Data Science\\Python\\Cursos\\___Unit Testing for Data Science in Python\\univariate-linear-regression\\src\\test\n",
      "============================= test session starts =============================\n",
      "platform win32 -- Python 3.8.8, pytest-6.2.4, py-1.10.0, pluggy-0.13.1\n",
      "Matplotlib: 3.3.4\n",
      "Freetype: 2.10.4\n",
      "rootdir: C:\\Users\\jaces\\Documents\\Data Science\\Python\\Cursos\\___Unit Testing for Data Science in Python\\univariate-linear-regression\n",
      "plugins: cov-2.12.1, mock-3.6.1, mpl-0.13\n",
      "collected 24 items / 21 deselected / 3 selected\n",
      "\n",
      "models\\test_train.py ...                                                 [100%]\n",
      "\n",
      "====================== 3 passed, 21 deselected in 0.53s =======================\n",
      "C:\\Users\\jaces\\Documents\\Data Science\\Python\\Cursos\\___Unit Testing for Data Science in Python\n"
     ]
    }
   ],
   "source": [
    "%cd univariate-linear-regression/src/test\n",
    "!pytest -k \"SplitInto\"\n",
    "%cd ../../.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=darkred>3.7 Expected failures and conditional skipping</font>\n",
    "\n",
    "1. Expected failures and conditional skipping\n",
    ">In the last lesson, we learned the magic command pytest that runs all tests.\n",
    "\n",
    "2. Test suite is green when all tests pass\n",
    ">If all tests pass, then our test suite is green. We can relax on the beach and drink a cocktail.\n",
    "\n",
    "3. Test suite is red when any test fails\n",
    ">If any test fails, then our test suite is red. This means we better work and fix it, otherwise our users will be very angry. This is good in theory, but, sometimes, the red light can be a false alarm that will ruin our beach vacations! An example will make things clear.\n",
    "\n",
    "4. Implementing a function using TDD\n",
    ">Let's say we are implementing this new function train_model(), which returns the best fit line on the training data. Since we are gonna use TDD, the first step is to write tests, so we create a test class TestTrainModel and add a test to it.\n",
    "\n",
    "5. The test fails, of course!\n",
    ">If we run pytest, this test will fail because the function train_model() is not yet implemented. And this is just a result of using TDD, it does not indicate a problem with the code base.\n",
    "\n",
    "6. False alarm\n",
    ">But the CI server does not know this and will set off a false alarm when that test fails. It would be nice to have a way to tell pytest that we expect this test to fail.\n",
    "\n",
    "7. xfail: marking tests as \"expected to fail\"\n",
    ">We do that by using the xfail decorator. The decorator goes on top of a test, and it starts with the character @.\n",
    "\n",
    "8. xfail: marking tests as \"expected to fail\"\n",
    ">This is followed by the name of the decorator pytest.mark.xfail. After adding the decorator, if we run pytest again, we see that one test is xfailed. But there are no reported errors,\n",
    "\n",
    "9. Test suite stays green\n",
    ">which means that the test suite remains green.\n",
    "\n",
    "10. Expected failures, but conditionally\n",
    ">At other times, we might know that the test fails only under certain conditions, and we don't want to be warned about them. Common situations are when some function won't work under a particular Python version or a particular platform. As an example, we have deliberately added the unicode() function in the failure message for the test test_with_no_comma() that we wrote earlier. This only works on Python 2.7 or lower.\n",
    "\n",
    "11. Test suite goes red on Python 3\n",
    ">If we run pytest using Python 3, the test suite will go red.\n",
    "\n",
    "12. skipif: skip tests conditionally\n",
    ">To tell pytest to skip running this test on Python versions higher than 2.7, we need the skipif decorator. The syntax is similar to xfail. The name of the decorator is pytest.mark.skipif.\n",
    "\n",
    "13. skipif: skip tests conditionally\n",
    ">It takes a single boolean expression as an argument. If the boolean expression is True, then the test will be skipped.\n",
    "\n",
    "14. skipif when Python version is higher than 2.7\n",
    ">To construct the boolean expression, import the built in module sys and use the attribute sys.version_info. This attribute can be compared against a tuple containing the major and minor Python version, in this case, 2 and 7.\n",
    "\n",
    "15. The reason argument\n",
    ">We must also add the required reason argument, which states why the test is skipped.\n",
    "\n",
    "16. 1 skipped, 1 xfailed\n",
    ">Running pytest again confirms that one test was xfailed and another one was skipped.\n",
    "\n",
    "17. Test suite stays green\n",
    ">The test suite remains green. Perfect again!\n",
    "\n",
    "18. Showing reason in the test result report\n",
    ">We can make the reason for skipping show in the report. For that, we can use the -r option.\n",
    "\n",
    "19. The -r option\n",
    ">The -r option can be followed by any number of characters.\n",
    "\n",
    "20. Showing reason for skipping\n",
    ">If we add the character s, it will show us tests that were skipped in the short test summary section near the end.\n",
    "\n",
    "21. Optional reason argument to xfail\n",
    ">The xfail decorator also takes an optional reason argument.\n",
    "\n",
    "22. Optional reason argument to xfail\n",
    ">For the test that we marked with xfail, we will add the reason “Using TDD, train_model() is not implemented”.\n",
    "\n",
    "23. Showing reason for xfail\n",
    ">If we add the character x to the -r option, it will only show us tests that are xfailed along with the reason in the test summary info.\n",
    "\n",
    "24. Showing reason for both skipped and xfail\n",
    ">We can show reasons for both by using the combination sx.\n",
    "\n",
    "25. Skipping/xfailing entire test classes\n",
    ">If we are skipping and xfailing multiple tests, note that these decorators can be applied to entire test classes as well.\n",
    "\n",
    "26. Let's practice xfailing and skipping!\n",
    ">Let's practice xfailing and skipping in the exercises!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaces\\Documents\\Data Science\\Python\\Cursos\\___Unit Testing for Data Science in Python\\univariate-linear-regression\\src\\test\n",
      "============================= test session starts =============================\n",
      "platform win32 -- Python 3.8.8, pytest-6.2.4, py-1.10.0, pluggy-0.13.1\n",
      "Matplotlib: 3.3.4\n",
      "Freetype: 2.10.4\n",
      "rootdir: C:\\Users\\jaces\\Documents\\Data Science\\Python\\Cursos\\___Unit Testing for Data Science in Python\\univariate-linear-regression\n",
      "plugins: cov-2.12.1, mock-3.6.1, mpl-0.13\n",
      "collected 1 item\n",
      "\n",
      "models\\test_train_not_implemented_avoid_fail.py F                        [100%]\n",
      "\n",
      "================================== FAILURES ===================================\n",
      "______________ TestTrainModelNotImplemented.test_on_linear_data _______________\n",
      "\n",
      "self = <test_train_not_implemented_avoid_fail.TestTrainModelNotImplemented object at 0x00000231CA497790>\n",
      "\n",
      "    def test_on_linear_data(self):\n",
      "        example_argument = np.array([[2081.0, 314942.0], [1059.0, 186606.0], [1697.0, 277794.0]])\n",
      "        expected_value   = True\n",
      ">       actual_value     = train.train_model_not_implemented(example_argument)\n",
      "E       AttributeError: module 'models.train' has no attribute 'train_model_not_implemented'\n",
      "\n",
      "models\\test_train_not_implemented_avoid_fail.py:11: AttributeError\n",
      "=========================== short test summary info ===========================\n",
      "FAILED models\\test_train_not_implemented_avoid_fail.py::TestTrainModelNotImplemented::test_on_linear_data\n",
      "============================== 1 failed in 0.60s ==============================\n",
      "C:\\Users\\jaces\\Documents\\Data Science\\Python\\Cursos\\___Unit Testing for Data Science in Python\n"
     ]
    }
   ],
   "source": [
    "%cd univariate-linear-regression/src/test\n",
    "with open(\"models/test_train_not_implemented_avoid_fail.py\", \"w\") as text_file:\n",
    "    text_file.write(\"\"\"\n",
    "import pytest\n",
    "import numpy as np\n",
    "\n",
    "from models import train\n",
    "\n",
    "class TestTrainModelNotImplemented(object):\n",
    "    def test_on_linear_data(self):\n",
    "        example_argument = np.array([[2081.0, 314942.0], [1059.0, 186606.0], [1697.0, 277794.0]])\n",
    "        expected_value   = True\n",
    "        actual_value     = train.train_model_not_implemented(example_argument)\n",
    "        message          = 'This function is not implemented yet'\n",
    "        assert expected_value == actual_value, message\n",
    "    \"\"\")\n",
    "    \n",
    "!pytest models/test_train_not_implemented_avoid_fail.py\n",
    "%cd ../../.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=blue>xfail: marking tests as \"expected to fail\"</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaces\\Documents\\Data Science\\Python\\Cursos\\___Unit Testing for Data Science in Python\\univariate-linear-regression\\src\\test\n",
      "============================= test session starts =============================\n",
      "platform win32 -- Python 3.8.8, pytest-6.2.4, py-1.10.0, pluggy-0.13.1\n",
      "Matplotlib: 3.3.4\n",
      "Freetype: 2.10.4\n",
      "rootdir: C:\\Users\\jaces\\Documents\\Data Science\\Python\\Cursos\\___Unit Testing for Data Science in Python\\univariate-linear-regression\n",
      "plugins: cov-2.12.1, mock-3.6.1, mpl-0.13\n",
      "collected 1 item\n",
      "\n",
      "models\\test_train_not_implemented_avoid_fail.py x                        [100%]\n",
      "\n",
      "============================= 1 xfailed in 0.61s ==============================\n",
      "C:\\Users\\jaces\\Documents\\Data Science\\Python\\Cursos\\___Unit Testing for Data Science in Python\n"
     ]
    }
   ],
   "source": [
    "%cd univariate-linear-regression/src/test\n",
    "with open(\"models/test_train_not_implemented_avoid_fail.py\", \"w\") as text_file:\n",
    "    text_file.write(\"\"\"\n",
    "import pytest\n",
    "import numpy as np\n",
    "\n",
    "from models import train\n",
    "\n",
    "class TestTrainModelNotImplemented(object):\n",
    "    @pytest.mark.xfail(reason=\"Using TDD, train.train_model_not_implemented() is not implemented.\")\n",
    "    def test_on_linear_data(self):\n",
    "        example_argument = np.array([[2081.0, 314942.0], [1059.0, 186606.0], [1697.0, 277794.0]])\n",
    "        expected_value   = True\n",
    "        actual_value     = train.train_model_not_implemented(example_argument)\n",
    "        message          = 'This function is not implemented yet'\n",
    "        assert expected_value == actual_value, message\n",
    "    \"\"\")\n",
    "    \n",
    "!pytest models/test_train_not_implemented_avoid_fail.py\n",
    "%cd ../../.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=blue>Expected failures, but conditionally</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaces\\Documents\\Data Science\\Python\\Cursos\\___Unit Testing for Data Science in Python\\univariate-linear-regression\\src\\test\n",
      "============================= test session starts =============================\n",
      "platform win32 -- Python 3.8.8, pytest-6.2.4, py-1.10.0, pluggy-0.13.1\n",
      "Matplotlib: 3.3.4\n",
      "Freetype: 2.10.4\n",
      "rootdir: C:\\Users\\jaces\\Documents\\Data Science\\Python\\Cursos\\___Unit Testing for Data Science in Python\\univariate-linear-regression\n",
      "plugins: cov-2.12.1, mock-3.6.1, mpl-0.13\n",
      "collected 1 item\n",
      "\n",
      "data\\test_preprocessing_helpers_conditional_avoid_fail.py F              [100%]\n",
      "\n",
      "================================== FAILURES ===================================\n",
      "_____________________ TestConvertToInt.test_with_no_comma _____________________\n",
      "\n",
      "self = <test_preprocessing_helpers_conditional_avoid_fail.TestConvertToInt object at 0x000002AD7E66F610>\n",
      "\n",
      "    def test_with_no_comma(self):\n",
      "        \"\"\"Only runs on Python 2.7 or lower\"\"\"\n",
      "        test_argument = \"756\"\n",
      "        expected = 756\n",
      "        actual = convert_to_int(test_argument)\n",
      ">       message = unicode(\"Expected: 2081, Actual: {0}\".format(actual)) # Requires Python 2.7 or lower\n",
      "E       NameError: name 'unicode' is not defined\n",
      "\n",
      "data\\test_preprocessing_helpers_conditional_avoid_fail.py:11: NameError\n",
      "=========================== short test summary info ===========================\n",
      "FAILED data\\test_preprocessing_helpers_conditional_avoid_fail.py::TestConvertToInt::test_with_no_comma\n",
      "============================== 1 failed in 0.26s ==============================\n",
      "C:\\Users\\jaces\\Documents\\Data Science\\Python\\Cursos\\___Unit Testing for Data Science in Python\n"
     ]
    }
   ],
   "source": [
    "%cd univariate-linear-regression/src/test\n",
    "with open(\"data/test_preprocessing_helpers_conditional_avoid_fail.py\", \"w\") as text_file:\n",
    "    text_file.write(\"\"\"\n",
    "import pytest\n",
    "from data.preprocessing_helpers import convert_to_int\n",
    "\n",
    "class TestConvertToInt(object):\n",
    "    def test_with_no_comma(self):\n",
    "        \\\"\\\"\\\"Only runs on Python 2.7 or lower\\\"\\\"\\\"\n",
    "        test_argument = \"756\"\n",
    "        expected = 756\n",
    "        actual = convert_to_int(test_argument)\n",
    "        message = unicode(\"Expected: 2081, Actual: {0}\".format(actual)) # Requires Python 2.7 or lower\n",
    "        assert actual == expected, message\n",
    "    \"\"\")\n",
    "    \n",
    "!pytest data/test_preprocessing_helpers_conditional_avoid_fail.py\n",
    "%cd ../../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaces\\Documents\\Data Science\\Python\\Cursos\\___Unit Testing for Data Science in Python\\univariate-linear-regression\\src\\test\n",
      "============================= test session starts =============================\n",
      "platform win32 -- Python 3.8.8, pytest-6.2.4, py-1.10.0, pluggy-0.13.1\n",
      "Matplotlib: 3.3.4\n",
      "Freetype: 2.10.4\n",
      "rootdir: C:\\Users\\jaces\\Documents\\Data Science\\Python\\Cursos\\___Unit Testing for Data Science in Python\\univariate-linear-regression\n",
      "plugins: cov-2.12.1, mock-3.6.1, mpl-0.13\n",
      "collected 1 item\n",
      "\n",
      "data\\test_preprocessing_helpers_conditional_avoid_fail.py s              [100%]\n",
      "\n",
      "============================= 1 skipped in 0.19s ==============================\n",
      "C:\\Users\\jaces\\Documents\\Data Science\\Python\\Cursos\\___Unit Testing for Data Science in Python\n"
     ]
    }
   ],
   "source": [
    "%cd univariate-linear-regression/src/test\n",
    "with open(\"data/test_preprocessing_helpers_conditional_avoid_fail.py\", \"w\") as text_file:\n",
    "    text_file.write(\"\"\"\n",
    "import pytest\n",
    "import sys\n",
    "\n",
    "from data.preprocessing_helpers import convert_to_int\n",
    "\n",
    "class TestConvertToInt(object):\n",
    "    @pytest.mark.skipif(sys.version_info > (2, 7), reason=\"Requires Python 2.7 or lower.\")\n",
    "    def test_with_no_comma(self):\n",
    "        \\\"\\\"\\\"Only runs on Python 2.7 or lower\\\"\\\"\\\"\n",
    "        test_argument = \"756\"\n",
    "        expected = 756\n",
    "        actual = convert_to_int(test_argument)\n",
    "        message = unicode(\"Expected: 2081, Actual: {0}\".format(actual)) # Requires Python 2.7 or lower\n",
    "        assert actual == expected, message\n",
    "    \"\"\")\n",
    "    \n",
    "!pytest data/test_preprocessing_helpers_conditional_avoid_fail.py\n",
    "%cd ../../.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=blue>RUNNING ALL</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaces\\Documents\\Data Science\\Python\\Cursos\\___Unit Testing for Data Science in Python\\univariate-linear-regression\\src\\test\n",
      "============================= test session starts =============================\n",
      "platform win32 -- Python 3.8.8, pytest-6.2.4, py-1.10.0, pluggy-0.13.1\n",
      "Matplotlib: 3.3.4\n",
      "Freetype: 2.10.4\n",
      "rootdir: C:\\Users\\jaces\\Documents\\Data Science\\Python\\Cursos\\___Unit Testing for Data Science in Python\\univariate-linear-regression\n",
      "plugins: cov-2.12.1, mock-3.6.1, mpl-0.13\n",
      "collected 24 items\n",
      "\n",
      "data\\test_preprocessing_helpers.py .............                         [ 54%]\n",
      "data\\test_preprocessing_helpers_conditional_avoid_fail.py s              [ 58%]\n",
      "features\\test_as_numpy.py .                                              [ 62%]\n",
      "features\\test_as_numpy_avoid_fail.py xs                                  [ 70%]\n",
      "features\\test_as_numpy_avoid_fail2.py s                                  [ 75%]\n",
      "models\\test_train.py ...                                                 [ 87%]\n",
      "models\\test_train_avoid_fail.py xx                                       [ 95%]\n",
      "models\\test_train_not_implemented_avoid_fail.py x                        [100%]\n",
      "\n",
      "================== 17 passed, 3 skipped, 4 xfailed in 0.69s ===================\n",
      "C:\\Users\\jaces\\Documents\\Data Science\\Python\\Cursos\\___Unit Testing for Data Science in Python\n"
     ]
    }
   ],
   "source": [
    "%cd univariate-linear-regression/src/test\n",
    "!pytest \n",
    "%cd ../../.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=red>SHOWING REASON IN THE TEST RESULT REPORT</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaces\\Documents\\Data Science\\Python\\Cursos\\___Unit Testing for Data Science in Python\\univariate-linear-regression\\src\\test\n",
      "============================= test session starts =============================\n",
      "platform win32 -- Python 3.8.8, pytest-6.2.4, py-1.10.0, pluggy-0.13.1\n",
      "Matplotlib: 3.3.4\n",
      "Freetype: 2.10.4\n",
      "rootdir: C:\\Users\\jaces\\Documents\\Data Science\\Python\\Cursos\\___Unit Testing for Data Science in Python\\univariate-linear-regression\n",
      "plugins: cov-2.12.1, mock-3.6.1, mpl-0.13\n",
      "collected 24 items\n",
      "\n",
      "data\\test_preprocessing_helpers.py .............                         [ 54%]\n",
      "data\\test_preprocessing_helpers_conditional_avoid_fail.py s              [ 58%]\n",
      "features\\test_as_numpy.py .                                              [ 62%]\n",
      "features\\test_as_numpy_avoid_fail.py xs                                  [ 70%]\n",
      "features\\test_as_numpy_avoid_fail2.py s                                  [ 75%]\n",
      "models\\test_train.py ...                                                 [ 87%]\n",
      "models\\test_train_avoid_fail.py xx                                       [ 95%]\n",
      "models\\test_train_not_implemented_avoid_fail.py x                        [100%]\n",
      "\n",
      "=========================== short test summary info ===========================\n",
      "SKIPPED [1] data\\test_preprocessing_helpers_conditional_avoid_fail.py:8: Requires Python 2.7 or lower.\n",
      "SKIPPED [1] features\\test_as_numpy_avoid_fail.py:21: requires Python 2.7 or lower.\n",
      "SKIPPED [1] features\\test_as_numpy_avoid_fail2.py:25: Works only on Python 2.7 or lower.\n",
      "================== 17 passed, 3 skipped, 4 xfailed in 0.73s ===================\n",
      "C:\\Users\\jaces\\Documents\\Data Science\\Python\\Cursos\\___Unit Testing for Data Science in Python\n"
     ]
    }
   ],
   "source": [
    "# Showing reason for skipping\n",
    "%cd univariate-linear-regression/src/test\n",
    "!pytest -rs\n",
    "%cd ../../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaces\\Documents\\Data Science\\Python\\Cursos\\___Unit Testing for Data Science in Python\\univariate-linear-regression\\src\\test\n",
      "============================= test session starts =============================\n",
      "platform win32 -- Python 3.8.8, pytest-6.2.4, py-1.10.0, pluggy-0.13.1\n",
      "Matplotlib: 3.3.4\n",
      "Freetype: 2.10.4\n",
      "rootdir: C:\\Users\\jaces\\Documents\\Data Science\\Python\\Cursos\\___Unit Testing for Data Science in Python\\univariate-linear-regression\n",
      "plugins: cov-2.12.1, mock-3.6.1, mpl-0.13\n",
      "collected 24 items\n",
      "\n",
      "data\\test_preprocessing_helpers.py .............                         [ 54%]\n",
      "data\\test_preprocessing_helpers_conditional_avoid_fail.py s              [ 58%]\n",
      "features\\test_as_numpy.py .                                              [ 62%]\n",
      "features\\test_as_numpy_avoid_fail.py xs                                  [ 70%]\n",
      "features\\test_as_numpy_avoid_fail2.py s                                  [ 75%]\n",
      "models\\test_train.py ...                                                 [ 87%]\n",
      "models\\test_train_avoid_fail.py xx                                       [ 95%]\n",
      "models\\test_train_not_implemented_avoid_fail.py x                        [100%]\n",
      "\n",
      "=========================== short test summary info ===========================\n",
      "XFAIL features\\test_as_numpy_avoid_fail.py::TestGetPandasData::test_on_clean_file\n",
      "  Using TDD, as_numpy.get_pandas_data is not implemented.\n",
      "XFAIL models\\test_train_avoid_fail.py::TestModelTest::test_on_linear_data\n",
      "  Using TDD, model_test() has not yet been implemented\n",
      "XFAIL models\\test_train_avoid_fail.py::TestModelTest::test_on_one_dimensional_array\n",
      "  Using TDD, model_test() has not yet been implemented\n",
      "XFAIL models\\test_train_not_implemented_avoid_fail.py::TestTrainModelNotImplemented::test_on_linear_data\n",
      "  Using TDD, train.train_model_not_implemented() is not implemented.\n",
      "================== 17 passed, 3 skipped, 4 xfailed in 0.71s ===================\n",
      "C:\\Users\\jaces\\Documents\\Data Science\\Python\\Cursos\\___Unit Testing for Data Science in Python\n"
     ]
    }
   ],
   "source": [
    "# Showing reason for xfail\n",
    "%cd univariate-linear-regression/src/test\n",
    "!pytest -rx\n",
    "%cd ../../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaces\\Documents\\Data Science\\Python\\Cursos\\___Unit Testing for Data Science in Python\\univariate-linear-regression\\src\\test\n",
      "============================= test session starts =============================\n",
      "platform win32 -- Python 3.8.8, pytest-6.2.4, py-1.10.0, pluggy-0.13.1\n",
      "Matplotlib: 3.3.4\n",
      "Freetype: 2.10.4\n",
      "rootdir: C:\\Users\\jaces\\Documents\\Data Science\\Python\\Cursos\\___Unit Testing for Data Science in Python\\univariate-linear-regression\n",
      "plugins: cov-2.12.1, mock-3.6.1, mpl-0.13\n",
      "collected 24 items\n",
      "\n",
      "data\\test_preprocessing_helpers.py .............                         [ 54%]\n",
      "data\\test_preprocessing_helpers_conditional_avoid_fail.py s              [ 58%]\n",
      "features\\test_as_numpy.py .                                              [ 62%]\n",
      "features\\test_as_numpy_avoid_fail.py xs                                  [ 70%]\n",
      "features\\test_as_numpy_avoid_fail2.py s                                  [ 75%]\n",
      "models\\test_train.py ...                                                 [ 87%]\n",
      "models\\test_train_avoid_fail.py xx                                       [ 95%]\n",
      "models\\test_train_not_implemented_avoid_fail.py x                        [100%]\n",
      "\n",
      "=========================== short test summary info ===========================\n",
      "SKIPPED [1] data\\test_preprocessing_helpers_conditional_avoid_fail.py:8: Requires Python 2.7 or lower.\n",
      "SKIPPED [1] features\\test_as_numpy_avoid_fail.py:21: requires Python 2.7 or lower.\n",
      "SKIPPED [1] features\\test_as_numpy_avoid_fail2.py:25: Works only on Python 2.7 or lower.\n",
      "XFAIL features\\test_as_numpy_avoid_fail.py::TestGetPandasData::test_on_clean_file\n",
      "  Using TDD, as_numpy.get_pandas_data is not implemented.\n",
      "XFAIL models\\test_train_avoid_fail.py::TestModelTest::test_on_linear_data\n",
      "  Using TDD, model_test() has not yet been implemented\n",
      "XFAIL models\\test_train_avoid_fail.py::TestModelTest::test_on_one_dimensional_array\n",
      "  Using TDD, model_test() has not yet been implemented\n",
      "XFAIL models\\test_train_not_implemented_avoid_fail.py::TestTrainModelNotImplemented::test_on_linear_data\n",
      "  Using TDD, train.train_model_not_implemented() is not implemented.\n",
      "================== 17 passed, 3 skipped, 4 xfailed in 0.70s ===================\n",
      "C:\\Users\\jaces\\Documents\\Data Science\\Python\\Cursos\\___Unit Testing for Data Science in Python\n"
     ]
    }
   ],
   "source": [
    "# Showing reason for both skipped and xfail\n",
    "%cd univariate-linear-regression/src/test\n",
    "!pytest -rsx\n",
    "%cd ../../.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=blue>SKIPPING/XFAILING ENTIRE TEST CLASSES</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaces\\Documents\\Data Science\\Python\\Cursos\\___Unit Testing for Data Science in Python\\univariate-linear-regression\\src\\test\n",
      "============================= test session starts =============================\n",
      "platform win32 -- Python 3.8.8, pytest-6.2.4, py-1.10.0, pluggy-0.13.1\n",
      "Matplotlib: 3.3.4\n",
      "Freetype: 2.10.4\n",
      "rootdir: C:\\Users\\jaces\\Documents\\Data Science\\Python\\Cursos\\___Unit Testing for Data Science in Python\\univariate-linear-regression\n",
      "plugins: cov-2.12.1, mock-3.6.1, mpl-0.13\n",
      "collected 24 items\n",
      "\n",
      "data\\test_preprocessing_helpers.py .............                         [ 54%]\n",
      "data\\test_preprocessing_helpers_conditional_avoid_fail.py s              [ 58%]\n",
      "features\\test_as_numpy.py .                                              [ 62%]\n",
      "features\\test_as_numpy_avoid_fail.py xs                                  [ 70%]\n",
      "features\\test_as_numpy_avoid_fail2.py s                                  [ 75%]\n",
      "models\\test_train.py ...                                                 [ 87%]\n",
      "models\\test_train_avoid_fail.py xx                                       [ 95%]\n",
      "models\\test_train_not_implemented_avoid_fail.py x                        [100%]\n",
      "\n",
      "=========================== short test summary info ===========================\n",
      "XFAIL features\\test_as_numpy_avoid_fail.py::TestGetPandasData::test_on_clean_file\n",
      "  Using TDD, as_numpy.get_pandas_data is not implemented.\n",
      "XFAIL models\\test_train_avoid_fail.py::TestModelTest::test_on_linear_data\n",
      "  Using TDD, model_test() has not yet been implemented\n",
      "XFAIL models\\test_train_avoid_fail.py::TestModelTest::test_on_one_dimensional_array\n",
      "  Using TDD, model_test() has not yet been implemented\n",
      "XFAIL models\\test_train_not_implemented_avoid_fail.py::TestTrainModelNotImplemented::test_on_linear_data\n",
      "  Using TDD, train.train_model_not_implemented() is not implemented.\n",
      "SKIPPED [1] data\\test_preprocessing_helpers_conditional_avoid_fail.py:8: Requires Python 2.7 or lower.\n",
      "SKIPPED [1] features\\test_as_numpy_avoid_fail.py:21: requires Python 2.7 or lower.\n",
      "SKIPPED [1] features\\test_as_numpy_avoid_fail2.py:25: Works only on Python 2.7 or lower.\n",
      "================== 17 passed, 3 skipped, 4 xfailed in 0.70s ===================\n",
      "C:\\Users\\jaces\\Documents\\Data Science\\Python\\Cursos\\___Unit Testing for Data Science in Python\n"
     ]
    }
   ],
   "source": [
    "%cd univariate-linear-regression/src/test\n",
    "with open(\"features/test_as_numpy_avoid_fail.py\", \"w\") as text_file:\n",
    "    text_file.write(\"\"\"\n",
    "import pytest\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "from features import as_numpy\n",
    "\n",
    "@pytest.mark.xfail(reason=\"Using TDD, as_numpy.get_pandas_data is not implemented.\")\n",
    "class TestGetPandasData(object):\n",
    "    def test_on_clean_file(self):\n",
    "        expected = np.array([[2081.0, 314942.0],\n",
    "                             [1059.0, 186606.0],\n",
    "                             [1148.0, 206186.0]\n",
    "                            ])\n",
    "        actual = as_numpy.get_pandas_data(\"example_clean_data2.txt\", num_columns=2)\n",
    "        message = \"Expected return value: {0}, Actual return value: {1}\".format(expected, actual)\n",
    "        assert (actual == expected).all()\n",
    "        \n",
    "@pytest.mark.skipif(sys.version_info > (2, 7), reason=\"requires Python 2.7 or lower.\")\n",
    "class TestGetDataAsNumpyArray(object):\n",
    "    def test_on_clean_file(self):\n",
    "        expected = np.array([[2081.0, 314942.0],\n",
    "                             [1059.0, 186606.0],\n",
    "                             [1148.0, 206186.0]\n",
    "                            ])\n",
    "        actual = as_numpy.get_data_as_numpy_array(\"example_clean_data2.txt\", num_columns=2)\n",
    "        # Requires Python 2.7 or lower\n",
    "        message = unicode(\"Expected return value: {0}, Actual return value: {1}\".format(expected, actual)) \n",
    "        assert (actual == expected).all()\n",
    "    \"\"\")\n",
    "    \n",
    "!pytest -rxs\n",
    "%cd ../../.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=darkred>3.8 Mark a test class as expected to fail</font>\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "A new function model_test() is being developed and it returns the accuracy of a given linear regression model on a testing dataset. Test Driven Development (TDD) is being used to implement it. The procedure is: write tests first and then implement the function.\n",
    "\n",
    "A test class TestModelTest has been created within the test module models/test_train.py. In the test class, there are two unit tests called test_on_linear_data() and test_on_one_dimensional_array(). But the function model_test() has not been implemented yet.\n",
    "\n",
    "Throughout this exercise, pytest and numpy as np will be imported for you.\n",
    "\n",
    "**Results**\n",
    "\n",
    "- Run the tests in the test class TestModelTest in the IPython console. What is the outcome?\n",
    "    - The tests fail with IndexError because some arguments to format the variable message are missing.\n",
    "    - The tests pass.\n",
    "    - <font color=red>The tests fail with NameError since the function model_test() has not yet been defined.</font>\n",
    "    - The tests fail with AssertionError.\n",
    "\n",
    "- Mark the whole test class TestModelTest as \"expected to fail\".\n",
    "- Add the following reason for the expected failure: \"Using TDD, model_test() has not yet been implemented\".\n",
    "\n",
    "<font color=darkgreen>Awesome! The reason you provided for the expected failure is useful for your colleagues, who might be wondering why you marked this test as expected to fail.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaces\\Documents\\Data Science\\Python\\Cursos\\___Unit Testing for Data Science in Python\\univariate-linear-regression\\src\\test\n",
      "============================= test session starts =============================\n",
      "platform win32 -- Python 3.8.8, pytest-6.2.4, py-1.10.0, pluggy-0.13.1\n",
      "Matplotlib: 3.3.4\n",
      "Freetype: 2.10.4\n",
      "rootdir: C:\\Users\\jaces\\Documents\\Data Science\\Python\\Cursos\\___Unit Testing for Data Science in Python\\univariate-linear-regression\n",
      "plugins: cov-2.12.1, mock-3.6.1, mpl-0.13\n",
      "collected 2 items\n",
      "\n",
      "models\\test_train_avoid_fail.py xx                                       [100%]\n",
      "\n",
      "=========================== short test summary info ===========================\n",
      "XFAIL models\\test_train_avoid_fail.py::TestModelTest::test_on_linear_data\n",
      "  Using TDD, model_test() has not yet been implemented\n",
      "XFAIL models\\test_train_avoid_fail.py::TestModelTest::test_on_one_dimensional_array\n",
      "  Using TDD, model_test() has not yet been implemented\n",
      "============================= 2 xfailed in 0.29s ==============================\n",
      "C:\\Users\\jaces\\Documents\\Data Science\\Python\\Cursos\\___Unit Testing for Data Science in Python\n"
     ]
    }
   ],
   "source": [
    "%cd univariate-linear-regression/src/test\n",
    "with open(\"models/test_train_avoid_fail.py\", \"w\") as text_file:\n",
    "    text_file.write(\"\"\"\n",
    "import pytest\n",
    "import numpy as np\n",
    "\n",
    "@pytest.mark.xfail(reason=\"Using TDD, model_test() has not yet been implemented\")\n",
    "class TestModelTest(object):\n",
    "    def test_on_linear_data(self):\n",
    "        test_input = np.array([[1.0, 3.0], [2.0, 5.0], [3.0, 7.0]])\n",
    "        expected = 1.0\n",
    "        actual = model_test(test_input, 2.0, 1.0)\n",
    "        message = \"model_test({0}) should return {1}, but it actually returned {2}\".format(test_input, expected, actual)\n",
    "        assert actual == pytest.approx(expected), message\n",
    "        \n",
    "    def test_on_one_dimensional_array(self):\n",
    "        test_input = np.array([1.0, 2.0, 3.0, 4.0])\n",
    "        with pytest.raises(ValueError) as exc_info:\n",
    "            model_test(test_input, 1.0, 1.0)\n",
    "    \"\"\")\n",
    "    \n",
    "!pytest -rxs models/test_train_avoid_fail.py\n",
    "%cd ../../.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=darkred>3.9 Mark a test as conditionally skipped</font>\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "In Python 2, there was a built-in function called xrange(). In Python 3, xrange() was removed. Therefore, if any test uses xrange(), it's going to fail with a NameError in Python 3.\n",
    "\n",
    "Remember the function get_data_as_numpy_array()? You saw it in Chapter 2. It converted data in a preprocessed data file into a NumPy array.\n",
    "\n",
    "range() has been deliberately replaced with the obsolete xrange() in the function. Evil laughter! But no worries, it will be changed back after you're done with this exercise.\n",
    "\n",
    "You wrote a test called test_on_clean_file() for this function. This test currently resides in a test class TestGetDataAsNumpyArray inside the test module features/test_as_numpy.py.\n",
    "\n",
    "pytest, numpy as np and get_data_as_numpy_array() has been imported for you.\n",
    "\n",
    "**Results**\n",
    "- Run the tests in the test class TestGetDataAsNumpyArray in the IPython console. What is the outcome?\n",
    "    - <font color=red>The test test_on_clean_file() fails with a NameError because Python 3 does not recognize the xrange() function.</font>\n",
    "    - The test test_on_clean_file() passes.\n",
    "    - The test test_on_clean_file() fails with an AssertionError.\n",
    "\n",
    "- Import the sys module.\n",
    "- Mark the test test_on_clean_file() as skipped if the Python version is greater than 2.7.\n",
    "- Add the following reason for skipping the test: \"Works only on Python 2.7 or lower\".\n",
    "\n",
    "<font color=darkgreen>Great job! You can use any boolean expression as the first argument of pytest.mark.skipif. One other common situation is to skip tests that won't run on particular platforms like Windows, Linux or Mac using the sys.platform attribute.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaces\\Documents\\Data Science\\Python\\Cursos\\___Unit Testing for Data Science in Python\\univariate-linear-regression\\src\\test\n",
      "============================= test session starts =============================\n",
      "platform win32 -- Python 3.8.8, pytest-6.2.4, py-1.10.0, pluggy-0.13.1\n",
      "Matplotlib: 3.3.4\n",
      "Freetype: 2.10.4\n",
      "rootdir: C:\\Users\\jaces\\Documents\\Data Science\\Python\\Cursos\\___Unit Testing for Data Science in Python\\univariate-linear-regression\n",
      "plugins: cov-2.12.1, mock-3.6.1, mpl-0.13\n",
      "collected 1 item\n",
      "\n",
      "features\\test_as_numpy_avoid_fail2.py s                                  [100%]\n",
      "\n",
      "============================== warnings summary ===============================\n",
      "features\\test_as_numpy_avoid_fail2.py:13\n",
      "  C:\\Users\\jaces\\Documents\\Data Science\\Python\\Cursos\\___Unit Testing for Data Science in Python\\univariate-linear-regression\\src\\test\\features\\test_as_numpy_avoid_fail2.py:13: DeprecationWarning: invalid escape sequence \\ \n",
      "    row = np.array([rows[row_num].rstrip(\"\\ \").split(\"\\t\")], dtype=float)\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/warnings.html\n",
      "=========================== short test summary info ===========================\n",
      "SKIPPED [1] features\\test_as_numpy_avoid_fail2.py:25: Works only on Python 2.7 or lower.\n",
      "======================== 1 skipped, 1 warning in 0.22s ========================\n",
      "C:\\Users\\jaces\\Documents\\Data Science\\Python\\Cursos\\___Unit Testing for Data Science in Python\n"
     ]
    }
   ],
   "source": [
    "%cd univariate-linear-regression/src/test\n",
    "with open(\"features/test_as_numpy_avoid_fail2.py\", \"w\") as text_file:\n",
    "    text_file.write(\"\"\"\n",
    "import numpy as np\n",
    "import pytest\n",
    "import sys\n",
    "\n",
    "# importin a dummy function\n",
    "def get_data_as_numpy_array(clean_data_file_path, num_columns): \n",
    "    result = np.empty((0, num_columns)) \n",
    "    with open(clean_data_file_path, \"r\") as f: \n",
    "        rows = f.readlines() \n",
    "        for row_num in xrange(len(rows)): \n",
    "            try: \n",
    "                row = np.array([rows[row_num].rstrip(\"\\\\ \").split(\"\\\\t\")], dtype=float) \n",
    "            except ValueError: \n",
    "                raise ValueError(\"Line {0} of {1} is badly formatted\".format(row_num + 1, clean_data_file_path)) \n",
    "            else: \n",
    "                if row.shape != (1, num_columns): \n",
    "                    raise ValueError(\"Line {0} of {1} does not have {2} columns\".format(\n",
    "                        row_num + 1, clean_data_file_path, num_columns\n",
    "                    )) \n",
    "            result = np.append(result, row, axis=0) \n",
    "    return result \n",
    "\n",
    "class TestGetDataAsNumpyArray(object):\n",
    "    @pytest.mark.skipif(sys.version_info > (2, 7), reason=\"Works only on Python 2.7 or lower.\")\n",
    "    def test_on_clean_file(self):\n",
    "        expected = np.array([[2081.0, 314942.0],\n",
    "                             [1059.0, 186606.0],\n",
    "                             [1148.0, 206186.0]\n",
    "                             ]\n",
    "                            )\n",
    "        actual = get_data_as_numpy_array(\"example_clean_data.txt\", num_columns=2)\n",
    "        message = \"Expected return value: {0}, Actual return value: {1}\".format(expected, actual)\n",
    "        assert actual == pytest.approx(expected), message\n",
    "    \"\"\")\n",
    "    \n",
    "!pytest -rxs features/test_as_numpy_avoid_fail2.py::TestGetDataAsNumpyArray\n",
    "%cd ../../.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=darkred>3.10 Reasoning in the test result report</font>\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "In the last exercises, you marked the test class TestModelTest in the test module models/test_train.py as expected to fail. You also marked the test test_on_clean_file() in the test class TestGetDataAsNumpyArray belonging to the test module features/test_as_numpy.py as skipped if the Python version is greater than 2.7.\n",
    "\n",
    "In both cases, you provided a reason argument which detailed why they are expected to fail or skipped. In this exercise, your job is to make this reason show up in the test result report when you run all tests in the IPython console.\n",
    "\n",
    "Feel free to run the !pytest command with different options and flags in the IPython console while doing the exercise.\n",
    "\n",
    "**Results**\n",
    "- What is the command that would only show the reason for expected failures in the test result report?\n",
    "    - !pytest -r\n",
    "    - <font color=red>!pytest -rx</font>\n",
    "    - !pytest -x\n",
    "    - !pytest -rs\n",
    "\n",
    "- What is the command that would only show the reason for skipped tests in the test result report?\n",
    "    - !pytest -r\n",
    "    - !pytest -rx\n",
    "    - !pytest -x\n",
    "    - <font color=red>!pytest -rs</font>\n",
    "\n",
    "- What is the command that would show the reason for both skipped tests and tests that are expected to fail in the test result report?\n",
    "    - </font>!pytest -rsx.</font>\n",
    "    - !pytest -sx.\n",
    "    - !pytest -s -x\n",
    "\n",
    "<font color=darkgreen>Seems like you have become a pro at the pytest command line tool. Congratulations!</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaces\\Documents\\Data Science\\Python\\Cursos\\___Unit Testing for Data Science in Python\\univariate-linear-regression\\src\\test\n",
      "============================= test session starts =============================\n",
      "platform win32 -- Python 3.8.8, pytest-6.2.4, py-1.10.0, pluggy-0.13.1\n",
      "Matplotlib: 3.3.4\n",
      "Freetype: 2.10.4\n",
      "rootdir: C:\\Users\\jaces\\Documents\\Data Science\\Python\\Cursos\\___Unit Testing for Data Science in Python\\univariate-linear-regression\n",
      "plugins: cov-2.12.1, mock-3.6.1, mpl-0.13\n",
      "collected 24 items\n",
      "\n",
      "data\\test_preprocessing_helpers.py .............                         [ 54%]\n",
      "data\\test_preprocessing_helpers_conditional_avoid_fail.py s              [ 58%]\n",
      "features\\test_as_numpy.py .                                              [ 62%]\n",
      "features\\test_as_numpy_avoid_fail.py xs                                  [ 70%]\n",
      "features\\test_as_numpy_avoid_fail2.py s                                  [ 75%]\n",
      "models\\test_train.py ...                                                 [ 87%]\n",
      "models\\test_train_avoid_fail.py xx                                       [ 95%]\n",
      "models\\test_train_not_implemented_avoid_fail.py x                        [100%]\n",
      "\n",
      "=========================== short test summary info ===========================\n",
      "SKIPPED [1] data\\test_preprocessing_helpers_conditional_avoid_fail.py:8: Requires Python 2.7 or lower.\n",
      "SKIPPED [1] features\\test_as_numpy_avoid_fail.py:21: requires Python 2.7 or lower.\n",
      "SKIPPED [1] features\\test_as_numpy_avoid_fail2.py:25: Works only on Python 2.7 or lower.\n",
      "XFAIL features\\test_as_numpy_avoid_fail.py::TestGetPandasData::test_on_clean_file\n",
      "  Using TDD, as_numpy.get_pandas_data is not implemented.\n",
      "XFAIL models\\test_train_avoid_fail.py::TestModelTest::test_on_linear_data\n",
      "  Using TDD, model_test() has not yet been implemented\n",
      "XFAIL models\\test_train_avoid_fail.py::TestModelTest::test_on_one_dimensional_array\n",
      "  Using TDD, model_test() has not yet been implemented\n",
      "XFAIL models\\test_train_not_implemented_avoid_fail.py::TestTrainModelNotImplemented::test_on_linear_data\n",
      "  Using TDD, train.train_model_not_implemented() is not implemented.\n",
      "================== 17 passed, 3 skipped, 4 xfailed in 0.72s ===================\n",
      "C:\\Users\\jaces\\Documents\\Data Science\\Python\\Cursos\\___Unit Testing for Data Science in Python\n"
     ]
    }
   ],
   "source": [
    "%cd univariate-linear-regression/src/test\n",
    "!pytest -rsx.\n",
    "%cd ../../.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=darkred>3.11 Continuous integration and code coverage</font>\n",
    "\n",
    "1. Continuous integration and code coverage\n",
    ">In Chapter 1,\n",
    "\n",
    "2. Code coverage and build status badges\n",
    ">We saw how NumPy increases user trust\n",
    "\n",
    "3. Code coverage and build status badges\n",
    ">by adding code coverage\n",
    "\n",
    "4. Code coverage and build status badges\n",
    ">and build status badges. In this lesson, we will learn to implement these badges for our own GitHub projects.\n",
    "\n",
    "5. The build status badge\n",
    ">Let's start with the build status badge.\n",
    "\n",
    "6. The build status badge\n",
    ">This badge uses a Continuous Integration server, which runs all tests automatically whenever we push a commit to GitHub. It shows whether tests are currently passing or failing.\n",
    "\n",
    "7. Build passing = Stable project\n",
    ">To an end user, passing indicates a stable code base\n",
    "\n",
    "8. Build failing = Unstable project\n",
    ">while failing indicates instability.\n",
    "\n",
    "9. CI server\n",
    ">We will use Travis CI as our CI server.\n",
    "\n",
    "10. Step 1: Create a configuration file\n",
    ">To integrate with Travis CI, we have to create a settings file called .travis.yml at the root of our repository.\n",
    "\n",
    "11. Step 1: Create a configuration file\n",
    ">The file is arranged into sections. First, there's a language setting, and we set it to python. The python setting determines which Python version will be used to run the tests. We choose Python 3.6. The install setting is a list of commands to install our project and dependencies in the CI server. If we organized our tests in the recommended way, then we can use a local pip install using pip install -e dot. The script section lists the commands necessary to run the tests once everything is installed. We use pytest tests to run the test suite.\n",
    "\n",
    "12. Step 2: Push the file to GitHub\n",
    ">We push this settings file to GitHub.\n",
    "\n",
    "13. Step 3: Install the Travis CI app\n",
    ">Now we go to the GitHub profile page and click on MarketPlace.\n",
    "\n",
    "14. Step 3: Install the Travis CI app\n",
    ">We search for Travis CI, click on it\n",
    "\n",
    "15. Step 3: Install the Travis CI app\n",
    ">and install the app. It's free for public repositories.\n",
    "\n",
    "16. Step 3: Install the Travis CI app\n",
    ">We allow the app access to the necessary repositories or organizations. Here, we are only allowing it access to the public repository univariate-linear-regression, which holds the example code for this course.\n",
    "\n",
    "17. Step 3: Install the Travis CI app\n",
    ">We will be redirected to Travis CI, where we should login using our GitHub account. This will bring us to the Travis CI dashboard.\n",
    "\n",
    "18. Every commit leads to a build\n",
    ">That's all the setup we need! From now on, whenever we push a commit to the GitHub repo, we should see a build appearing in the Travis CI dashboard.\n",
    "\n",
    "19. Step 4: Showing the build status badge\n",
    ">When the build finishes, the badge appears here. We click on the badge,\n",
    "\n",
    "20. Step 4: Showing the build status badge\n",
    ">choose Markdown from the dropdown\n",
    "\n",
    "21. Step 4: Showing the build status badge\n",
    ">and paste the markdown code in the README file on GitHub. This adds the badge to the GitHub repo.\n",
    "\n",
    "22. Code coverage\n",
    ">We will add the code coverage badge next. The code coverage badge indicates the percentage of our application code that gets run when we run the test suite. High percentages indicate a well tested code base.\n",
    "\n",
    "23. Codecov\n",
    ">This badge comes from a service called Codecov that integrates seamlessly with GitHub and Travis CI.\n",
    "\n",
    "24. Step 1: Modify the Travis CI configuration file\n",
    ">First, we will modify the .travis.yml to enable code coverage reports.\n",
    "\n",
    "25. Step 1: Modify the Travis CI configuration file\n",
    ">In the install setting, pip install pytest-cov and codecov, as they are necessary to generate and upload coverage reports.\n",
    "\n",
    "26. Step 1: Modify the Travis CI configuration file\n",
    ">The usual pytest command to run the tests should be modified by adding a command line flag --cov which points to the application directory src. This new command will not only run the tests, but also produce a coverage report.\n",
    "\n",
    "27. Step 1: Modify the Travis CI configuration file\n",
    ">Finally, add a setting called after_success and add the command codecov. This makes Travis CI push the code coverage results to Codecov after every build.\n",
    "\n",
    "28. Step 2: Install Codecov\n",
    ">To enable Codecov for our repository, we install the Codecov app in the GitHub marketplace in the same way we installed Travis CI.\n",
    "\n",
    "29. Commits lead to coverage report at codecov.io\n",
    ">From now, when we push a new commit, the code coverage report should show up in Codecov, accessible at codecov.io, after Travis CI completes the build.\n",
    "\n",
    "30. Step 3: Showing the badge in GitHub\n",
    ">Go to the badge section in settings and paste the Markdown code to the GitHub README file.\n",
    "\n",
    "31. Step 3: Showing the badge in GitHub\n",
    ">And that adds the code coverage badge.\n",
    "\n",
    "32. Let's practice CI and code coverage!\n",
    ">Let's practice some of these concepts in the exercises. It is also recommended that you go through these steps for a GitHub repository that you own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=darkred>3.12 Build failing</font>\n",
    "\n",
    "**Instructions**\n",
    "In the GitHub repository of a Python package, you see the following badge:\n",
    "\n",
    "<img src='images/build_status_failing_no_whitespace.png' width=25%\\>\n",
    "          \n",
    "What can you, as a user, conclude from this badge?\n",
    "\n",
    "\n",
    "**Possible Answers**\n",
    "- There are no unit tests in the package.\n",
    "- The package has a code coverage less than 75%.\n",
    "- There's no .travis.yml configuration file at the root of the repository.\n",
    "- <font color=red>The package has bugs, which is either causing installation to error out or some of the unit tests in the test suite to fail.</font>\n",
    "\n",
    "**Results**\n",
    "\n",
    "<font color=darkgreen>That's correct! Since a build failing badge is indicative of bugs, the maintainer of any package should strive to keep this badge green (\"passing\").</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=darkred>3.13 What does code coverage mean?</font>\n",
    "\n",
    "**Instructions**\n",
    "In a Github repository of a Python package, you see the following badge\n",
    "\n",
    "<img src='images/code_coverage_badge.png' width=25%/>\n",
    "\n",
    "What does it mean?\n",
    "\n",
    "**Possible Answers**\n",
    "- <font color=red>The test suite tests about 85% of the application code.</font>\n",
    "- Unit tests make up 85% of the code base of the package.\n",
    "- Historically, the test suite has failed about 85% of the times it was ran.\n",
    "- The insurance pays 85% of any financial damages caused by malfunctioning of the package.\n",
    "\n",
    "\n",
    "**Results**\n",
    "\n",
    "<font color=darkgreen>You got that right! This brings us to the end of Chapter 3. Congratulations on coming this far! In the next Chapter, you are going to dive into advanced topics in unit testing and look at some data science specific unit testing tricks. See you there :-)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aditional material\n",
    "\n",
    "- Datacamp course: https://learn.datacamp.com/courses/unit-testing-for-data-science-in-python\n",
    "- Project https://github.com/gutfeeling/univariate-linear-regression"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
